[{"content":"  References   How To Run OpenVPN in a Docker Container on Ubuntu 14.04 Running Docker Containers with Systemd    Because we are installing our cluster bare metal on servers exposed on the Internet, we\u0026rsquo;ll need a way to secure all of our network traffic around the critical parts of kubernetes. To do so, we\u0026rsquo;ll use OpenVPN to create a virtual secured network where all of our nodes will work. Moreover, this network will also contains MetalLB services when  \u0026nbsp;configuring our bare metal load balancer.\nYou may need to edit your /etc/hosts files to associate vpn.{{cluster.baseHostName}} to your future OpenVPN server on each of the devices that will join the cluster (if vpn.{{cluster.baseHostName}} is not a real DNS name).\n1  echo \u0026#39;{{vpn.publicServerIp}}\tvpn.{{cluster.baseHostName}}\u0026#39; \u0026gt;\u0026gt; /etc/hosts    See the \u0026nbsp;docs of kylemanna/openvpn (our OpenVPN server).\nOpenVPN server initial setup On the OpenVPN server, create a volume for OpenVPN so that it can store files, and generate configuration.\n1 2 3 4 5 6 7 8  # Create the volume docker volume create --name {{vpn.volumeName}} # Init OpenVPN configuration \u0026amp; certificates docker run -v {{vpn.volumeName}}:/etc/openvpn --rm kylemanna/openvpn:2.4 ovpn_genconfig -Nd -u udp://vpn.{{cluster.baseHostName}}:1194 # Generate the EasyRSA PKI certificate authority. This will prompt a password, that you should keep safe. It will be used to generate new client certificates \u0026amp; configs docker run -v {{vpn.volumeName}}:/etc/openvpn --rm -it kylemanna/openvpn:2.4 ovpn_initpki # Start the server docker run -v {{vpn.volumeName}}:/etc/openvpn -it -p 1194:1194/udp --cap-add=NET_ADMIN kylemanna/openvpn:2.4    Note on the -Nd flags of the line 4: see \u0026nbsp;this RTFM page for split tunnel (partial traffic tunnel)\n Once the last command is executed, your OpenVPN server should start. If it started properly, just kill it. We will set it up as a systemd service for our host.\nMake a systemd service for OpenVPN through docker If you\u0026rsquo;re not using systemd, see how to use init.d, and skip this section. Install the \u0026nbsp;systemd/kubernetes-vpn.service template into /usr/lib/systemd/system, then enable this service. It will run our OpenVPN server container.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  [Unit] Description=OpenVPN server through Docker After=syslog.target network-online.target docker.service Wants=network-online.target Requires=docker.service Documentation=man:openvpn(8) Documentation=https://community.openvpn.net/openvpn/wiki/Openvpn24ManPage Documentation=https://community.openvpn.net/openvpn/wiki/HOWTO Documentation=https://github.com/kylemanna/docker-openvpn [Service] ExecStop=-/usr/bin/docker stop %n ExecStopPost=-/usr/bin/docker rm %n ExecStartPre=/usr/bin/docker pull kylemanna/openvpn:2.3 ExecStart=/usr/bin/docker run --name %n -v {{vpn.volumeName}}:/etc/openvpn --rm -p 1194:{{vpn.port}}/udp --cap-add=NET_ADMIN kylemanna/openvpn:2.3 TimeoutStartSec=30 TimeoutStopSec=15 Restart=always RestartSec=10s Type=simple [Install] WantedBy=multi-user.target   1 2 3 4 5  mv ./systemd/kubernetes-vpn.service /usr/lib/systemd/system # Reload available services to take into account our new `kubernetes-vpn` systemctl daemon-reload # Start \u0026amp; auto start it systemctl enable --now kubernetes-vpn.service     Have Fail2Ban installed ?    References   https://www.fail2ban.org/wiki/index.php/HOWTO_fail2ban_with_OpenVPN    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  cat \u0026lt;\u0026lt;EOF | tee /etc/fail2ban/filter.d/openvpn.local # Fail2Ban filter for selected OpenVPN rejections # # [Definition] # Example messages (other matched messages not seen in the testing server\u0026#39;s logs): # Fri Sep 23 11:55:36 2016 TLS Error: incoming packet authentication failed from [AF_INET]59.90.146.160:51223 # Thu Aug 25 09:36:02 2016 117.207.115.143:58922 TLS Error: TLS handshake failed failregex = ^ TLS Error: incoming packet authentication failed from \\[AF_INET\\]\u0026lt;HOST\u0026gt;:\\d+$ ^ \u0026lt;HOST\u0026gt;:\\d+ Connection reset, restarting ^ \u0026lt;HOST\u0026gt;:\\d+ TLS Auth Error ^ \u0026lt;HOST\u0026gt;:\\d+ TLS Error: TLS handshake failed$ ^ \u0026lt;HOST\u0026gt;:\\d+ VERIFY ERROR ignoreregex = EOF cat \u0026lt;\u0026lt;EOF | tee /etc/fail2ban/jail.d/openvpn.local # Fail2Ban configuration fragment for OpenVPN [openvpn] enabled = true port = 1194 protocol = udp filter = openvpn logpath = /var/log/openvpn.log maxretry = 3 EOF systemctl reload fail2ban      -- You can check our docker container with docker container inspect kubernetes-vpn.service \u0026amp; get our OpenVPN logs with journalctl -u kubernetes-vpn.service.\nNow, get the value of the variable ðŸ”– vpn.serverIp ({{vpn.serverIp}}) with this command:\n1 2 3 4 5  # Show interface informations docker exec -it kubernetes-vpn.service ip -4 addr show tun0 # Or, fancy buggy variant to show only interface IP docker exec -it kubernetes-vpn.service ip -4 addr show tun0 `# Get the \u0026#34;tun0\u0026#34; interface infos` \\ | grep -Po \u0026#39;inet \\K[0-9.]*\u0026#39;    See \u0026nbsp;Static IP Addresses documentation for docker-openvpn\n Setup clients This section is meant to be repeated for each of your cluster\u0026rsquo;s nodes. For every node, replace the ðŸ”– node.ip ({{node.ip}}) \u0026amp; ðŸ”– node.name ({{node.name}}) variables.\nImportant: ðŸ”– node.ip ({{node.ip}}) is the desired IP of your machine in your VPN. It must be on the same network than ðŸ”– vpn.serverIp ({{vpn.serverIp}}) (usually, 192.168.255.XXX) Generate credentials For each of our clients, we\u0026rsquo;ll need to generate credentials so that they can connect to the vpn server. Those clients may use static IPs. The master(s) must have a static IP since it must be reachable via a constant address for kubectl.\nOn your OpenVPN\u0026lsquo;server host:\n1 2 3 4 5 6  # Generate a client docker run -v {{vpn.volumeName}}:/etc/openvpn --rm -it kylemanna/openvpn:2.4 easyrsa build-client-full {{node.name}} nopass # Set its static IP echo \u0026#34;ifconfig-push {{node.ip}} {{vpn.serverIp}}\u0026#34; | docker run -v {{vpn.volumeName}}:/etc/openvpn -i --rm kylemanna/openvpn:2.4 tee /etc/openvpn/ccd/{{node.name}} # Get its config to your host docker run -v {{vpn.volumeName}}:/etc/openvpn --rm kylemanna/openvpn:2.4 ovpn_getclient {{node.name}} \u0026gt; {{node.name}}.ovpn   Move this {{node.name}}.ovpn file to the {{node.name}} node by a safe mean. Those files are super critical, so be very careful to not put it anywhere usafe.\nNext operations have to be run on clients.\nInstall OpenVPN client   References   https://www.vpsserver.com/community/tutorials/4035/install-openvpn-on-centos-8/    1 2 3  dnf install epel-release git dnf update dnf install openvpn   Install certificates Install the certificate previously copied with:\n1 2 3 4  # Install the OpenVPN configuration install -o root -m 400 {{node.name}}.ovpn /etc/openvpn/client/{{node.name}}.conf # Enable the OpenVPN client systemctl enable --now openvpn-client@{{node.name}}   Repeat those steps for each of our nodes, then make sure that you can reach each of your nodes from each other and you can still access the Internet.\n1 2 3 4 5  # Check internet connection by pinging Google ping -c 4 8.8.8.8 # Check in-VPN connection ping -c 4 {{vpn.serverIp}} # Eventually, check connection to other nodes   If you\u0026rsquo;re having troubles pinging 8.8.8.8 or another\u0026rsquo;s node IP, please refer to \u0026nbsp;this troubleshooting section\nYou should be good to go ðŸ”¥\nTroubleshoot   References   https://stackoverflow.com/a/63624477/4839162    No internet connection on nodes, or no connection between nodes   References   https://github.com/kylemanna/docker-openvpn#openvpn-details https://github.com/kylemanna/docker-openvpn/issues/381#issuecomment-386269991 https://github.com/kylemanna/docker-openvpn/issues/381#issuecomment-616009737    Check in case-by-case. I had to add a route push in my server configuration to make it work. See https://openvpn.net/community-resources/how-to/#expanding-the-scope-of-the-vpn-to-include-additional-machines-on-either-the-client-or-server-subnet\n1 2  docker exec -it kubernetes-vpn.service bash -c \u0026#34;echo \u0026#39;push \\\u0026#34;route 192.168.255.0 255.255.255.0\\\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/openvpn.conf\u0026#34; systemctl restart kubernetes-vpn.service   In another setup, I had to comment out push \u0026quot;block-outside-dns\u0026quot; from the server config file (see \u0026nbsp;this comment).\nUsefull commands demo Flush all routes 1 2  sudo iptables -t filter -F sudo iptables -t filter -X   Remove a client I bet there\u0026rsquo;s a better way to do this, but I noted this for myself.\n1 2 3 4 5  # Remove your node from this file vim /var/lib/containers/storage/volumes/{{vpn.volumeName}}/_data/pki/index.txt docker exec -it kubernetes-vpn.service rm /etc/openvpn/pki/issued/kube-master.crt docker exec -it kubernetes-vpn.service rm /etc/openvpn/pki/private/kube-master.key docker exec -it kubernetes-vpn.service rm /etc/openvpn/pki/reqs/kube-master.req   Regenerate client configs \u0026amp; copy them 1 2 3 4 5 6 7 8 9 10 11 12  OVPN_DATA=ovpn-data-cluster clients=kube-master-1 kube-worker-1 docker run -v {{vpn.volumeName}}:/etc/openvpn --rm kylemanna/openvpn:2.4 ovpn_genconfig -u udp://vpn.bar.com:1194 docker run -v {{vpn.volumeName}}:/etc/openvpn --rm -it kylemanna/openvpn:2.4 ovpn_initpki sudo systemctl restart kubernetes-vpn for client in $clients; do docker run -v {{vpn.volumeName}}:/etc/openvpn --rm -it kylemanna/openvpn:2.4 easyrsa build-client-full $client nopass docker run -v {{vpn.volumeName}}:/etc/openvpn --rm kylemanna/openvpn:2.4 ovpn_getclient $client \u0026gt; $client.ovpn done sudo install -o root -m 400 $(hostname).ovpn /etc/openvpn/client/$(hostname).conf sudo systemctl restart openvpn-client@$(hostname) scp kube-worker-1.ovpn gerkin@192.168.1.26:~   ","description":"","id":0,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Networking"],"title":"Setup the cluster's VPN","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/00-vpn/"},{"content":"  References   https://docs.kublr.com/articles/kubernetes-log-audit/ https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/ https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#log-collector-examples     Note : Even if this part is not required, you should not ignore it on dev environment and should really really REALLY not skip it for production. In fact, it can contain useful debug informations and security traces to see what is going on in your kubernetes cluster, and even on your whole server(s).\n This tutorial will guide you to setup audit log policy, catch logs with Fluentd, cast them to elasticsearch \u0026amp; show them using Kibana.\nFirst, choose an audit log dir name on the host ðŸ”– audit.sourceLogDir ({{audit.sourceLogDir}}). This is the directory where kubernetes will write its audit logs, and should be in /var/log. Then, choose an audit log file ðŸ”– audit.sourceLogFile ({{audit.sourceLogFile}}) in ðŸ”– audit.sourceLogDir ({{audit.sourceLogDir}}). The final audit logs path is then {{audit.sourceLogDir}}/{{audit.sourceLogFile}}\nFluentD will parse those audit logs, and split them by tags for easier sorting of logs. It will then write those zones in ðŸ”– audit.destLogDir ({{audit.destLogDir}})\nIn order to pipe audit log messages to Elasticsearch, we need to install fluentd on the kubernetes master host.\nInstall fluentd (on the kubernetes master host) https://docs.fluentd.org/installation/before-install\nInstall Chrony https://www.tecmint.com/install-ntp-in-rhel-8/\nStart by installing Chrony for accurate timestamps\n1 2  dnf install chrony systemctl enable --now chronyd   You should be good to go.\nConfigure other settings   References   https://superuser.com/questions/740000/modify-and-apply-limits-conf-without-reboot    Check the file descriptors limit for the root user (use sudo):\n1 2 3 4  ulimit -n # Â» 1024 ulimit -Hn # Â» 262144   If it is low (like 1024), you need to increase it, by opening your system\u0026rsquo;s limits. So, open your limits.conf file:\n1  vim /etc/security/limits.conf   Set the following configurations:\n1 2 3 4  root soft nofile 65536 root hard nofile 65536 * soft nofile 65536 * hard nofile 65536   Then reboot \u0026amp; recheck for the root user (use sudo).\n1 2  ulimit -n # should be 65536 ulimit -Hn # should be at least 65536   If you run this as your normal user, ulimit -n changes might not be changed. If the environment is expected to have a high load, follow \u0026nbsp;this section of the guide\nInstall FluentD \u0026amp; plugins https://docs.fluentd.org/installation/install-by-rpm\nAdd the td-agent repository \u0026amp; install it\n1 2  curl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent3.sh | sh systemctl enable --now td-agent.service   Check if it works by posting a sample log\n1 2  curl -X POST -d \u0026#39;json={\u0026#34;json\u0026#34;:\u0026#34;message\u0026#34;}\u0026#39; http://localhost:8888/debug.test cat /var/log/td-agent/td-agent.log # should end with our test message above   Install required plugins with the following command:\n1  td-agent-gem install fluent-plugin-forest fluent-plugin-rewrite-tag-filter   If having errors here, see the \u0026nbsp;Troubleshoot section at the end.\nConfigure Fluentd   References   https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#log-collector-examples    https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#log-collector-examples\nInstall the \u0026nbsp;td-agent/kube.conf template template into /etc/td-agent/, include it in your master configuration, and create the log dirs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  # From https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#log-collector-examples # fluentd conf runs in the same host with kube-apiserver \u0026lt;source\u0026gt; @type tail # audit log path of kube-apiserver path {{audit.sourceLogDir}}/{{audit.sourceLogFile}} pos_file {{audit.sourceLogDir}}/{{audit.sourceLogFile}}.pos format json time_key time time_format %Y-%m-%dT%H:%M:%S.%N%z tag audit \u0026lt;/source\u0026gt; \u0026lt;filter audit\u0026gt; #https://github.com/fluent/fluent-plugin-rewrite-tag-filter/issues/13 @type record_transformer enable_ruby \u0026lt;record\u0026gt; namespace ${record[\u0026#34;objectRef\u0026#34;].nil? ? \u0026#34;none\u0026#34;:(record[\u0026#34;objectRef\u0026#34;][\u0026#34;namespace\u0026#34;].nil? ? \u0026#34;none\u0026#34;:record[\u0026#34;objectRef\u0026#34;][\u0026#34;namespace\u0026#34;])} \u0026lt;/record\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;match audit\u0026gt; # route audit according to namespace element in context @type rewrite_tag_filter \u0026lt;rule\u0026gt; key namespace pattern /^(.+)/ tag ${tag}.$1 \u0026lt;/rule\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;filter audit.**\u0026gt; @type record_transformer remove_keys namespace \u0026lt;/filter\u0026gt; \u0026lt;match audit.**\u0026gt; @type forest subtype file remove_prefix audit \u0026lt;template\u0026gt; time_slice_format %Y%m%d%H compress gz path {{audit.destLogDir}}/audit-${tag}.*.log format json include_time_key true \u0026lt;/template\u0026gt; \u0026lt;/match\u0026gt;   1 2 3 4 5 6 7 8 9 10  mv ./td-agent/kube.conf /etc/td-agent/td-agent.conf # Include kubernetes configuration it in configuration echo \u0026#34;@include \u0026#39;./kube.conf\u0026#39;\u0026#34; \u0026gt;\u0026gt; /etc/td-agent/td-agent.conf # Create the log dir that will be mounted into the API server mkdir -p {{audit.destLogDir}} # If required, allow td-agent to read/write in it chown -R root:td-agent {{audit.destLogDir}} chmod -R g+w {{audit.destLogDir}} # Restart the agent systemctl restart td-agent.service   Setup the audit log   References   https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy https://medium.com/@noqcks/kubernetes-audit-logging-introduction-464a34a53f6c    See the \u0026nbsp;example audit log policy \u0026amp; the \u0026nbsp;template audit log file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71  # From https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/audit/audit-policy.yaml# See https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy for more infoapiVersion:audit.k8s.io/v1# This is required.kind:Policy# Don\u0026#39;t generate audit events for all requests in RequestReceived stage.omitStages:- \u0026#34;RequestReceived\u0026#34;rules:# Log pod changes at RequestResponse level- level:RequestResponseresources:- group:\u0026#34;\u0026#34;# Resource \u0026#34;pods\u0026#34; doesn\u0026#39;t match requests to any subresource of pods,# which is consistent with the RBAC policy.resources:[\u0026#34;pods\u0026#34;]# Log \u0026#34;pods/log\u0026#34;, \u0026#34;pods/status\u0026#34; at Metadata level- level:Metadataresources:- group:\u0026#34;\u0026#34;resources:[\u0026#34;pods/log\u0026#34;,\u0026#34;pods/status\u0026#34;]# Don\u0026#39;t log requests to a configmap called \u0026#34;controller-leader\u0026#34;- level:Noneresources:- group:\u0026#34;\u0026#34;resources:[\u0026#34;configmaps\u0026#34;]resourceNames:[\u0026#34;controller-leader\u0026#34;]# Don\u0026#39;t log watch requests by the \u0026#34;system:kube-proxy\u0026#34; on endpoints or services- level:Noneusers:[\u0026#34;system:kube-proxy\u0026#34;]verbs:[\u0026#34;watch\u0026#34;]resources:- group:\u0026#34;\u0026#34;# core API groupresources:[\u0026#34;endpoints\u0026#34;,\u0026#34;services\u0026#34;]# Don\u0026#39;t log authenticated requests to certain non-resource URL paths.- level:NoneuserGroups:[\u0026#34;system:authenticated\u0026#34;]nonResourceURLs:- \u0026#34;/api*\u0026#34;# Wildcard matching.- \u0026#34;/version\u0026#34;# Log the request body of configmap changes in kube-system.- level:Requestresources:- group:\u0026#34;\u0026#34;# core API groupresources:[\u0026#34;configmaps\u0026#34;]# This rule only applies to resources in the \u0026#34;kube-system\u0026#34; namespace.# The empty string \u0026#34;\u0026#34; can be used to select non-namespaced resources.namespaces:[\u0026#34;kube-system\u0026#34;]# Log configmap and secret changes in all other namespaces at the Metadata level.- level:Metadataresources:- group:\u0026#34;\u0026#34;# core API groupresources:[\u0026#34;secrets\u0026#34;,\u0026#34;configmaps\u0026#34;]# Log all other resources in core and extensions at the Request level.- level:Requestresources:- group:\u0026#34;\u0026#34;# core API group- group:\u0026#34;extensions\u0026#34;# Version of group should NOT be included.# A catch-all rule to log all other requests at the Metadata level.- level:Metadata# Long-running requests like watches that fall under this rule will not# generate an audit event in RequestReceived.omitStages:- \u0026#34;RequestReceived\u0026#34;   Move it in the /etc/kubernetes folder (because this is a kubernete\u0026rsquo;s configuration).\n1 2  mv ./kubernetes/audit-log-policy.yaml /etc/kubernetes/audit-log-policy.yaml chown root:root /etc/kubernetes/audit-log-policy.yaml   Troubleshoot Unable to download data from https://rubygems.org/ - timed out (https://api.rubygems.org/specs.4.8.gz) Rubygems repository seems to have issues with IPv6. Check with below commands:\n1 2  curl -v --head https://api.rubygems.org curl -6 -v --head https://api.rubygems.org   If the 1st command worked and the second hang (timeout), then you are having troubles with IPv6, and you need to temporarly disable it.\n1 2  sysctl -w net.ipv6.conf.default.disable_ipv6=1 sysctl -w net.ipv6.conf.all.disable_ipv6=1   After installing your plugin, re-enable IPv6\n1 2  sysctl -w net.ipv6.conf.default.disable_ipv6=0 sysctl -w net.ipv6.conf.all.disable_ipv6=0   ","description":"","id":1,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Monitoring","Security"],"title":"Setup the cluster's Audit Log","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/01-audit-log/"},{"content":"Create the cluster config file   References   How to set bind address by config file: https://stackoverflow.com/a/60391611    We are now going to configure the cluster. For the sake of traceability, this configuration won\u0026rsquo;t be done via CLI flags, but via \u0026nbsp;a configuration file. The path of the cluster config file will later be referenced as the ðŸ”– cluster.configFile ({{cluster.configFile}}), and should be inside /etc/kubernetes.\nFollowing \u0026nbsp;flannel requirements, you need to use --pod-network-cidr with address 10.244.0.0./16. This CLI option is equivalent to networking.podSubnet in our ðŸ”– cluster.configFile ({{cluster.configFile}}) file (see \u0026nbsp;this issue).\nThe variable ðŸ”– cluster.advertiseAddress ({{cluster.advertiseAddress}}) must be set to the network address of your master node through the VPN. You can get it like so:\n1  ip -4 a show tun0 | grep -Po \u0026#39;inet \\K[0-9.]*\u0026#39;   The variables ðŸ”– audit.sourceLogDir ({{audit.sourceLogDir}}) \u0026amp; ðŸ”– audit.sourceLogFile ({{audit.sourceLogFile}}) were set in  \u0026nbsp;Setup the cluster's Audit Log\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  apiVersion:kubeadm.k8s.io/v1beta2kind:InitConfigurationlocalAPIEndpoint:advertiseAddress:{{cluster.advertiseAddress}}---apiVersion:kubeadm.k8s.io/v1beta2kind:ClusterConfigurationclusterName:{{cluster.name}}networking:podSubnet:\u0026#34;10.244.0.0/16\u0026#34;apiServer:extraArgs:audit-policy-file:/etc/kubernetes/audit-log-policy.yamlaudit-log-path:{{audit.sourceLogDir}}/{{audit.sourceLogFile}}extraVolumes:- name:audit-policyhostPath:/etc/kubernetes/audit-log-policy.yamlmountPath:/etc/kubernetes/audit-log-policy.yaml# See apiServer.extraArgs.audit-policy-filereadOnly:true- name:audit-loghostPath:{{audit.sourceLogDir}}mountPath:{{audit.sourceLogDir}}pathType:DirectoryOrCreatereadOnly:false   1 2 3  mv ./kubernetes/cluster-config.yaml {{cluster.configFile}} chown root:root {{cluster.configFile}} chmod 600 {{cluster.configFile}}   Finally, init the cluster   References   kubeadm API resources Flannel kubernetes RTFM https://github.com/kubernetes/kubeadm/issues/203#issuecomment-335416377 https://coreos.com/os/docs/latest/using-systemd-drop-in-units.html Flannel yaml file customization â€“iface for vagrant Linux cluster    Pay attention to the feedbacks of the kubeadm command. It will show warnings about misconfigurations.\n1 2 3 4  # Init the cluster with our cluster config file kubeadm init --config {{cluster.configFile}} # Setup kubectl mkdir -p $HOME/.kube \u0026amp;\u0026amp; cp -i /etc/kubernetes/admin.conf $HOME/.kube/config \u0026amp;\u0026amp; chown $(id -u):$(id -g) $HOME/.kube/config   Now, the kubelet has been configured. Well, mainly. Because, as mentioned \u0026nbsp;here, it assumes that it should work through the default gateway (our public network), but that\u0026rsquo;s not what we want. So, we need to explicitly declare our node\u0026rsquo;s IP.\n1 2 3 4  sed -i.bak \u0026#34;s/KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=--node-ip=$(ip -4 a show tun0 | grep -Po \u0026#39;inet \\K[0-9.]*\u0026#39;)/\u0026#34; /etc/sysconfig/kubelet systemctl restart kubelet.service # Verify that the `--node-ip` flag is appended to the `/usr/bin/kubelet` process systemctl status kubelet.service   To communicate with each other, pods need a network layer. We\u0026rsquo;ll use flannel for this. Following its \u0026nbsp;installation instruction, you need to deploy \u0026nbsp;this file. But there\u0026rsquo;s a problem: as mentioned in the \u0026nbsp;configuration documentation, flannel use the default route (our public network) by default, and we still want to use the VPN fio this. So, I\u0026rsquo;ve just added a single line in the \u0026nbsp;kube-flannel file to specify our VPN interface (line 188, - --iface=tun0).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225  # From https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml---apiVersion:policy/v1beta1kind:PodSecurityPolicymetadata:name:psp.flannel.unprivilegedannotations:seccomp.security.alpha.kubernetes.io/allowedProfileNames:docker/defaultseccomp.security.alpha.kubernetes.io/defaultProfileName:docker/defaultapparmor.security.beta.kubernetes.io/allowedProfileNames:runtime/defaultapparmor.security.beta.kubernetes.io/defaultProfileName:runtime/defaultspec:privileged:falsevolumes:- configMap- secret- emptyDir- hostPathallowedHostPaths:- pathPrefix:\u0026#34;/etc/cni/net.d\u0026#34;- pathPrefix:\u0026#34;/etc/kube-flannel\u0026#34;- pathPrefix:\u0026#34;/run/flannel\u0026#34;readOnlyRootFilesystem:false# Users and groupsrunAsUser:rule:RunAsAnysupplementalGroups:rule:RunAsAnyfsGroup:rule:RunAsAny# Privilege EscalationallowPrivilegeEscalation:falsedefaultAllowPrivilegeEscalation:false# CapabilitiesallowedCapabilities:[\u0026#39;NET_ADMIN\u0026#39;,\u0026#39;NET_RAW\u0026#39;]defaultAddCapabilities:[]requiredDropCapabilities:[]# Host namespaceshostPID:falsehostIPC:falsehostNetwork:truehostPorts:- min:0max:65535# SELinuxseLinux:# SELinux is unused in CaaSPrule:\u0026#39;RunAsAny\u0026#39;---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:flannelrules:- apiGroups:[\u0026#39;extensions\u0026#39;]resources:[\u0026#39;podsecuritypolicies\u0026#39;]verbs:[\u0026#39;use\u0026#39;]resourceNames:[\u0026#39;psp.flannel.unprivileged\u0026#39;]- apiGroups:- \u0026#34;\u0026#34;resources:- podsverbs:- get- apiGroups:- \u0026#34;\u0026#34;resources:- nodesverbs:- list- watch- apiGroups:- \u0026#34;\u0026#34;resources:- nodes/statusverbs:- patch---kind:ClusterRoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:flannelroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:flannelsubjects:- kind:ServiceAccountname:flannelnamespace:kube-system---apiVersion:v1kind:ServiceAccountmetadata:name:flannelnamespace:kube-system---kind:ConfigMapapiVersion:v1metadata:name:kube-flannel-cfgnamespace:kube-systemlabels:tier:nodeapp:flanneldata:cni-conf.json:|{ \u0026#34;name\u0026#34;: \u0026#34;cbr0\u0026#34;, \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;flannel\u0026#34;, \u0026#34;delegate\u0026#34;: { \u0026#34;hairpinMode\u0026#34;: true, \u0026#34;isDefaultGateway\u0026#34;: true } }, { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;portMappings\u0026#34;: true } } ] }net-conf.json:|{ \u0026#34;Network\u0026#34;: \u0026#34;10.244.0.0/16\u0026#34;, \u0026#34;Backend\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;vxlan\u0026#34; } }---apiVersion:apps/v1kind:DaemonSetmetadata:name:kube-flannel-dsnamespace:kube-systemlabels:tier:nodeapp:flannelspec:selector:matchLabels:app:flanneltemplate:metadata:labels:tier:nodeapp:flannelspec:affinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:nodeSelectorTerms:- matchExpressions:- key:kubernetes.io/osoperator:Invalues:- linuxhostNetwork:truepriorityClassName:system-node-criticaltolerations:- operator:Existseffect:NoScheduleserviceAccountName:flannelinitContainers:- name:install-cniimage:quay.io/coreos/flannel:v0.13.1-rc1command:- cpargs:- -f- /etc/kube-flannel/cni-conf.json- /etc/cni/net.d/10-flannel.conflistvolumeMounts:- name:cnimountPath:/etc/cni/net.d- name:flannel-cfgmountPath:/etc/kube-flannel/containers:- name:kube-flannelimage:quay.io/coreos/flannel:v0.13.1-rc1command:- /opt/bin/flanneldargs:- --iface=tun0- --ip-masq- --kube-subnet-mgrresources:requests:cpu:\u0026#34;100m\u0026#34;memory:\u0026#34;50Mi\u0026#34;limits:cpu:\u0026#34;100m\u0026#34;memory:\u0026#34;50Mi\u0026#34;securityContext:privileged:falsecapabilities:add:[\u0026#34;NET_ADMIN\u0026#34;,\u0026#34;NET_RAW\u0026#34;]env:- name:POD_NAMEvalueFrom:fieldRef:fieldPath:metadata.name- name:POD_NAMESPACEvalueFrom:fieldRef:fieldPath:metadata.namespacevolumeMounts:- name:runmountPath:/run/flannel- name:flannel-cfgmountPath:/etc/kube-flannel/volumes:- name:runhostPath:path:/run/flannel- name:cnihostPath:path:/etc/cni/net.d- name:flannel-cfgconfigMap:name:kube-flannel-cfg   1 2 3 4  # If you want to run pods on the master (not recommended), run the following command: kubectl taint nodes $(hostname) node-role.kubernetes.io/master- # To undo, run the following kubectl taint nodes $(hostname) node-role.kubernetes.io/master:NoSchedule   Join workers At the end of the kubeadm init... command, a join command was issued if everything went OK. Execute this command on every workers you want in your cluster. The command is something like below:\n1 2  kubeadm join xxx.xxx.xxx.xxx:yyy --token foo.barqux123456 \\  --discovery-token-ca-cert-hash sha256:fed2136f5e41d654f6e6411d4f5e646512fd5   If lost, you can create a new one by executing following command on the control pane with:\n1  kubeadm token create --print-join-command   1 2 3 4  sed -i.bak \u0026#34;s/KUBELET_EXTRA_ARGS=/KUBELET_EXTRA_ARGS=--node-ip=$(ip -4 a show tun0 | grep -Po \u0026#39;inet \\K[0-9.]*\u0026#39;)/\u0026#34; /etc/sysconfig/kubelet systemctl restart kubelet.service # Verify that the --node-ip flag is appended to the /usr/bin/kubelet process systemctl status kubelet.service   You can check nodes by running following command from the control pane\n1 2 3  kubectl get nodes # Or watch kubectl get nodes -w   After some time, you should see the new node joining the cluster !\nYou may repeat this part of the process during the life of your cluster to add new nodes.\nInitialize metallb https://metallb.universe.tf/installation/\nCreate a metallb configmap, from the \u0026nbsp;kubernetes/metallb-configmap.yaml template. \u0026nbsp;See the docs for full reference on this config file \u0026amp; how to adapt it to your network configuration..\nThe ðŸ”– cluster.networkAddress ({{cluster.networkAddress}}) corresponds to the network part of your ðŸ”– cluster.advertiseAddress ({{cluster.advertiseAddress}}).\n1 2 3 4 5 6 7 8 9 10 11 12  apiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|address-pools: - name: default protocol: layer2 addresses: - {{cluster.networkAddress}}.100-{{cluster.networkAddress}}.250   1 2 3 4 5 6 7  # Deploy metallb kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/metallb.yaml # On first install only kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\u0026#34;$(openssl rand -base64 128)\u0026#34; # Create the configmap kubectl apply -f ./kubernetes/metallb-configmap.yaml   To check if everything works so far, start a test nginx instance:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  kubectl create namespace nginx-test kubectl --namespace nginx-test run nginx --image nginx # This may take some time to fetch the container kubectl --namespace nginx-test expose pod nginx --port 80 --type LoadBalancer nginx_ip=\u0026#34;$(kubectl --namespace nginx-test get svc nginx --output json | jq --raw-output \u0026#39;.status.loadBalancer.ingress[].ip\u0026#39;)\u0026#34; if [[ ! -z \u0026#34;$nginx_ip\u0026#34; ]]; then echo -e \u0026#34;$(tput setaf 2)Has public IP $nginx_ip. Testing connection. If nothing appears bellow, you might have a firewall configuration issue.$(tput sgr0)\u0026#34; if ! timeout 5 curl http://$nginx_ip ; then echo -e \u0026#34;$(tput setaf 1)nginx unreachable. You might have a firewall configuration issue.$(tput sgr0)\u0026#34; fi else echo \u0026#34;No public IP\u0026#34; fi unset nginx_ip   This should return Has public IP with an IP that should be reachable from the host \u0026amp; the HTML of the default nginx page. If not, then you might have additional configuration to do.\nCleanup the namespace afterwards\n1  kubectl delete namespace nginx-test    Hey, weâ€™ve done important things here ! Maybe itâ€™s time to commitâ€¦\n1 2 3 4  git add . git commit -m \u0026#34;Kickstart the cluster Following guide @ https://gerkindev.github.io/devblog/walkthroughs/kubernetes/02-cluster/\u0026#34;    Troubleshoot Kubelet is not running I had to reinstall kubelet to clear previous runs configurations.\n1  dnf reinstall -y kubelet kubeadm kubectl --disableexcludes=kubernetes   Nginx external ip is always pending   References   https://stackoverflow.com/a/60151612\n{{ /expand }}  Check that iptables is patched correctly.\n1 2 3 4 5 6  cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system update-alternatives --set iptables /usr/sbin/iptables-legacy   Check firewall, SELinux \u0026amp; swap\n1 2  getenforce cat /proc/swaps   Make sure your nodes are ready and that the networking plugin is correctly installed.\nCluster never starts Move or remove the existing kubeadm config file (if any) in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\nCheck firewall, getenforce \u0026amp; swap status.\nNetwork interfaces are not deleted after reseting kubeadm   References   https://blog.heptio.com/properly-resetting-your-kubeadm-bootstrapped-cluster-nodes-heptioprotip-473bd0b824aa Kubernetes cannot cleanup Flannel Kubernetes flannel pod getting the wrong network    1 2 3  iptables -F \u0026amp;\u0026amp; iptables -t nat -F \u0026amp;\u0026amp; iptables -t mangle -F \u0026amp;\u0026amp; iptables -X ip link delete cni0 ip link delete flannel.1   Usefull commands memo  Force reinit cluster:      ( sudo kubeadm reset -f \u0026amp;\u0026amp; sudo rm -rf /etc/cni/net.d || 1 ) \u0026amp;\u0026amp; sudo kubeadm init \u0026ndash;config cluster-config.yaml\n      ","description":"","id":2,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps"],"title":"Kickstart the cluster","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/02-cluster/"},{"content":"  References   https://github.com/traefik/traefik-helm-chart/pull/157/files    Start by creating traefik required resources. You can directly use resources from the \u0026nbsp;kubernetes/traefik templates: it does not contain variables. Those are taken from \u0026nbsp;traefik docs mixed up with \u0026nbsp;this PR for kubernetes 1.19 support and schemas.\n Please look forward for \u0026nbsp;this issue in traefik about official v1.19 support.\n Namespace Definitions Rbac IngressController Services  1 2 3 4 5  # File: ./kubernetes/traefik/01-Namespace.yamlapiVersion:v1kind:Namespacemetadata:name:traefik    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949  # File: ./kubernetes/traefik/02-CustomResourcesDefinitions.yaml# From https://doc.traefik.io/traefik/v2.4/user-guides/crd-acme/#ingressroute-definition# Applying schemas from https://github.com/traefik/traefik-helm-chart/pull/157/filesapiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:ingressroutes.traefik.containo.usspec:group:traefik.containo.usversions:- name:v1alpha1served:truestorage:trueschema:openAPIV3Schema:type:objectproperties:spec:type:objectproperties:routes:type:arrayitems:type:objectproperties:match:type:stringkind:type:stringpriority:type:integerservices:type:arrayitems:type:objectproperties:sticky:type:objectproperties:cookie:type:objectproperties:name:type:stringsecure:type:booleanhttpOnly:type:booleannamespace:type:stringkind:type:stringname:type:stringweight:type:integerresponseForwarding:type:objectproperties:flushInterval:type:stringpassHostHeader:type:booleanhealthCheck:type:objectproperties:path:type:stringhost:type:stringscheme:type:stringintervalSeconds:type:integertimeoutSeconds:type:integerheaders:type:objectstrategy:type:stringscheme:type:stringport:type:integermiddlewares:type:arrayitems:type:objectproperties:name:type:stringnamespace:type:stringrequired:- name- namespaceentryPoints:type:arrayitems:type:stringtls:type:objectproperties:secretName:type:stringoptions:type:objectproperties:name:type:stringnamespace:type:stringstore:type:objectproperties:name:type:stringnamespace:type:stringcertResolver:type:stringdomains:type:arrayitems:type:objectproperties:main:type:stringsans:type:arrayitems:type:stringnames:kind:IngressRouteplural:ingressroutessingular:ingressroutescope:Namespaced---apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:middlewares.traefik.containo.usspec:group:traefik.containo.usversions:- name:v1alpha1served:truestorage:trueschema:openAPIV3Schema:type:objectproperties:spec:type:objectproperties:addPrefix:type:objectproperties:prefix:type:stringstripPrefix:type:objectproperties:prefixes:type:arrayitems:type:stringforceSlash:type:booleanstripPrefixRegex:type:objectproperties:regex:type:arrayitems:type:stringreplacePath:type:objectproperties:path:type:stringreplacePathRegex:type:objectproperties:regex:type:stringreplacement:type:stringchain:type:objectproperties:middlewares:type:arrayitems:type:objectproperties:name:type:stringnamespace:type:stringrequired:- name- namespaceipWhiteList:type:objectproperties:sourceRange:type:arrayitems:type:stringipStrategy:type:objectproperties:depth:type:integerexcludedIPs:type:arrayitems:type:stringheaders:type:objectproperties:customRequestHeaders:type:objectcustomResponseHeaders:type:objectaccessControlAllowCredentials:type:booleanaccessControlAllowHeaders:type:arrayitems:type:stringaccessControlAllowMethods:type:arrayitems:type:stringaccessControlAllowOrigin:type:stringaccessControlAllowOriginList:type:arrayitems:type:stringaccessControlExposeHeaders:type:arrayitems:type:stringaccessControlMaxAge:type:integeraddVaryHeader:type:booleanallowedHosts:type:arrayitems:type:stringhostsProxyHeaders:type:arrayitems:type:stringsslRedirect:type:booleansslTemporaryRedirect:type:booleansslHost:type:stringsslProxyHeaders:type:objectsslForceHost:type:booleanstsSeconds:type:integerstsIncludeSubdomains:type:booleanstsPreload:type:booleanforceSTSheader:type:booleanframeDeny:type:booleancustomFrameOptionsValue:type:stringcontentTypeNosniff:type:booleanbrowserXssFilter:type:booleancustomBrowserXSSValue:type:stringcontentSecurityPolicy:type:stringpublicKey:type:stringreferrerPolicy:type:stringfeaturePolicy:type:stringisDevelopment:type:booleanerrors:type:objectproperties:status:type:arrayitems:type:stringservice:type:objectproperties:sticky:type:objectproperties:cookie:type:objectproperties:name:type:stringsecure:type:booleanhttpOnly:type:booleannamespace:type:stringkind:type:stringname:type:stringweight:type:integerresponseForwarding:type:objectproperties:flushInterval:type:stringpassHostHeader:type:booleanhealthCheck:type:objectproperties:path:type:stringhost:type:stringscheme:type:stringintervalSeconds:type:integertimeoutSeconds:type:integerheaders:type:objectstrategy:type:stringscheme:type:stringport:type:integerquery:type:stringrateLimit:type:objectproperties:average:type:integerburst:type:integersourceCriterion:type:objectproperties:ipStrategy:type:objectproperties:depth:type:integerexcludedIPs:type:arrayitems:type:stringrequestHeaderName:type:stringrequestHost:type:booleanredirectRegex:type:objectproperties:regex:type:stringreplacement:type:stringpermanent:type:booleanredirectScheme:type:objectproperties:scheme:type:stringport:type:stringpermanent:type:booleanbasicAuth:type:objectproperties:secret:type:stringrealm:type:stringremoveHeader:type:booleanheaderField:type:stringdigestAuth:type:objectproperties:secret:type:stringremoveHeader:type:booleanrealm:type:stringheaderField:type:stringforwardAuth:type:objectproperties:address:type:stringtrustForwardHeader:type:booleanauthResponseHeaders:type:arrayitems:type:stringtls:type:objectproperties:caSecret:type:stringcaOptional:type:booleancertSecret:type:stringinsecureSkipVerify:type:booleaninFlightReq:type:objectproperties:amount:type:integersourceCriterion:type:objectproperties:ipStrategy:type:objectproperties:depth:type:integerexcludedIPs:type:arrayitems:type:stringrequestHeaderName:type:stringrequestHost:type:booleanbuffering:type:objectproperties:maxRequestBodyBytes:type:integermemRequestBodyBytes:type:integermaxResponseBodyBytes:type:integermemResponseBodyBytes:type:integerretryExpression:type:stringcircuitBreaker:type:objectproperties:expression:type:stringcompress:type:objectproperties:excludedContentTypes:type:arrayitems:type:stringpassTLSClientCert:type:objectproperties:pem:type:booleaninfo:type:objectproperties:notAfter:type:booleannotBefore:type:booleansans:type:booleansubject:type:objectproperties:country:type:booleanprovince:type:booleanlocality:type:booleanorganization:type:booleancommonName:type:booleanserialNumber:type:booleandomainComponent:type:booleanissuer:type:objectproperties:country:type:booleanprovince:type:booleanlocality:type:booleanorganization:type:booleancommonName:type:booleanserialNumber:type:booleandomainComponent:type:booleanserialNumber:type:booleanretry:type:objectproperties:attempts:type:integercontentType:type:objectproperties:autoDetect:type:booleannames:kind:Middlewareplural:middlewaressingular:middlewarescope:Namespaced---apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:ingressroutetcps.traefik.containo.usspec:group:traefik.containo.usversions:- name:v1alpha1served:truestorage:trueschema:openAPIV3Schema:type:objectproperties:spec:type:objectproperties:routes:type:arrayitems:type:objectproperties:match:type:stringservices:type:arrayitems:type:objectproperties:name:type:stringnamespace:type:stringport:type:integerweight:type:integerterminationDelay:type:integerentryPoints:type:arrayitems:type:stringtls:type:objectproperties:secretName:type:stringpassthrough:type:booleanoptions:type:objectproperties:name:type:stringnamespace:type:stringstore:type:objectproperties:name:type:stringnamespace:type:stringcertResolver:type:stringdomains:type:arrayitems:type:objectproperties:main:type:stringsans:type:arrayitems:type:stringnames:kind:IngressRouteTCPplural:ingressroutetcpssingular:ingressroutetcpscope:Namespaced---apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:ingressrouteudps.traefik.containo.usspec:group:traefik.containo.usversions:- name:v1alpha1served:truestorage:truenames:kind:IngressRouteUDPplural:ingressrouteudpssingular:ingressrouteudpscope:Namespaced---apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:tlsoptions.traefik.containo.usspec:group:traefik.containo.usversions:- name:v1alpha1served:truestorage:trueschema:openAPIV3Schema:type:objectproperties:spec:type:objectproperties:minVersion:type:stringmaxVersion:type:stringcipherSuites:type:arrayitems:type:stringcurvePreferences:type:arrayitems:type:stringclientAuth:type:objectproperties:secretNames:type:arrayitems:type:stringclientAuthType:type:stringenum:- NoClientCert- RequestClientCert- VerifyClientCertIfGiven- RequireAndVerifyClientCertsniStrict:type:booleanpreferServerCipherSuites:type:booleannames:kind:TLSOptionplural:tlsoptionssingular:tlsoptionscope:Namespaced---apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:tlsstores.traefik.containo.usspec:group:traefik.containo.usversions:- name:v1alpha1served:truestorage:truenames:kind:TLSStoreplural:tlsstoressingular:tlsstorescope:Namespaced---apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:traefikservices.traefik.containo.usspec:group:traefik.containo.usversions:- name:v1alpha1served:truestorage:trueschema:openAPIV3Schema:type:objectproperties:spec:type:objectproperties:weighted:type:objectproperties:services:type:arrayitems:type:objectproperties:sticky:type:objectproperties:cookie:type:objectproperties:name:type:stringsecure:type:booleanhttpOnly:type:booleannamespace:type:stringkind:type:stringname:type:stringweight:type:integerresponseForwarding:type:objectproperties:flushInterval:type:stringpassHostHeader:type:booleanhealthCheck:type:objectproperties:path:type:stringhost:type:stringscheme:type:stringintervalSeconds:type:integertimeoutSeconds:type:integerheaders:type:objectstrategy:type:stringscheme:type:stringport:type:integersticky:type:objectproperties:cookie:type:objectproperties:name:type:stringsecure:type:booleanhttpOnly:type:booleanmirroring:type:objectproperties:weight:type:integerresponseForwarding:type:objectproperties:flushInterval:type:stringpassHostHeader:type:booleanhealthCheck:type:objectproperties:path:type:stringhost:type:stringscheme:type:stringintervalSeconds:type:integertimeoutSeconds:type:integerheaders:type:objectstrategy:type:stringscheme:type:stringport:type:integersticky:type:objectproperties:cookie:type:objectproperties:name:type:stringsecure:type:booleanhttpOnly:type:booleannamespace:type:stringkind:type:stringname:type:stringmirrors:type:arrayitems:type:objectproperties:name:type:stringkind:type:stringnamespace:type:stringsticky:type:objectproperties:cookie:type:objectproperties:name:type:stringsecure:type:booleanhttpOnly:type:booleanport:type:integerscheme:type:stringstrategy:type:stringhealthCheck:type:objectproperties:path:type:stringhost:type:stringscheme:type:stringintervalSeconds:type:integertimeoutSeconds:type:integerheaders:type:objectpassHostHeader:type:booleanresponseForwarding:type:objectproperties:flushInterval:type:stringweight:type:integerpercent:type:integernames:kind:TraefikServiceplural:traefikservicessingular:traefikservicescope:Namespaced---apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:serverstransports.traefik.containo.usspec:group:traefik.containo.usversions:- name:v1alpha1served:truestorage:truenames:kind:ServersTransportplural:serverstransportssingular:serverstransportscope:Namespaced    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74  # File: ./kubernetes/traefik/03-Rbac.yaml# Oddly, those definitions are in the CRD, but are not CRDs# https://doc.traefik.io/traefik/v2.4/user-guides/crd-acme/#ingressroute-definitionapiVersion:v1kind:ServiceAccountmetadata:name:traefiknamespace:traefik---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:traefikrules:- apiGroups:- \u0026#34;\u0026#34;resources:- services- endpoints- secretsverbs:- get- list- watch- apiGroups:- extensions- networking.k8s.ioresources:- ingresses- ingressclassesverbs:- get- list- watch- apiGroups:- extensionsresources:- ingresses/statusverbs:- update- apiGroups:- traefik.containo.usresources:- middlewares- ingressroutes- traefikservices- ingressroutetcps- ingressrouteudps- tlsoptions- tlsstores- serverstransportsverbs:- get- list- watch---kind:ClusterRoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:traefikroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:traefiksubjects:- kind:ServiceAccountname:traefiknamespace:traefik    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  # File: ./kubernetes/traefik/04-IngressController.yaml# https://doc.traefik.io/traefik/v2.4/user-guides/crd-acme/#deploymentskind:DeploymentapiVersion:apps/v1metadata:name:traefiknamespace:traefiklabels:app:traefikcomponent:ingress-controllerspec:replicas:1selector:matchLabels:app:traefikcomponent:ingress-controllertemplate:metadata:labels:app:traefikcomponent:ingress-controllerspec:serviceAccountName:traefikcontainers:- name:traefikimage:traefik:v2.4args:- --api=false- --api.dashboard=false- --accesslog- --entrypoints.web.Address=:8000- --entrypoints.websecure.Address=:4443- --providers.kubernetescrd- --certificatesresolvers.myresolver.acme.tlschallenge- --certificatesresolvers.myresolver.acme.email=FILL@ME.COM- --certificatesresolvers.myresolver.acme.storage=acme.json# Please note that this is the staging Let\u0026#39;s Encrypt server.# Once you get things working, you should remove that whole line altogether.- --certificatesresolvers.myresolver.acme.caserver=https://acme-staging-v02.api.letsencrypt.org/directoryports:- name:webcontainerPort:8000- name:websecurecontainerPort:4443- name:admincontainerPort:8080    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  # File: ./kubernetes/traefik/05-Services.yaml# https://doc.traefik.io/traefik/v2.4/user-guides/crd-acme/#servicesapiVersion:v1kind:Servicemetadata:name:traefiknamespace:traefikspec:ports:- protocol:TCPname:webport:8000- protocol:TCPname:adminport:8080- protocol:TCPname:websecureport:4443selector:app:traefikcomponent:ingress-controllertype:LoadBalancer      'use strict'; var containerId = JSON.parse(\"\\\"bef8510f5a480082\\\"\"); var containerElem = document.getElementById(containerId); var codetabLinks = null; var codetabContents = null; var ids = []; if (containerElem) { codetabLinks = containerElem.querySelectorAll('.codetab__link'); codetabContents = containerElem.querySelectorAll('.codetab__content'); } for (var i = 0; i 0) { codetabContents[0].style.display = 'block'; }  1 2 3 4 5  kubectl apply -f ./kubernetes/traefik/01-Namespace.yaml kubectl apply -f ./kubernetes/traefik/02-CustomResourcesDefinitions.yaml kubectl apply -f ./kubernetes/traefik/03-Rbac.yaml kubectl apply -f ./kubernetes/traefik/04-IngressController.yaml kubectl apply -f ./kubernetes/traefik/05-Services.yaml   You can now use the \u0026nbsp;custom resource kind IngressRoute to map routes using traefik.\nTo check if everything works so far, you can use a test nginx instance from \u0026nbsp;kubernetes/xx-WhoAmI.yaml. Once deployed, you should be able to display the nginx default page by reaching https://test.{{cluster.baseHostName}} from your host.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77  apiVersion:v1kind:Namespacemetadata:name:whoami---kind:DeploymentapiVersion:apps/v1metadata:name:whoaminamespace:whoamilabels:app:whoamispec:replicas:1selector:matchLabels:app:whoamitemplate:metadata:labels:app:whoamispec:containers:- name:whoamiimage:traefik/whoamiports:- name:webcontainerPort:80---apiVersion:v1kind:Servicemetadata:name:whoaminamespace:whoamispec:ports:- protocol:TCPname:webport:80selector:app:whoami---apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingress-tlsnamespace:whoami# Must be the same as the servicespec:entryPoints:- websecureroutes:- match:Host(`whoami.{{cluster.baseHostName}}`) \u0026amp;\u0026amp; PathPrefix(`/tls`)kind:Ruleservices:# Beware: the service MUST be in the same namespace than the IngressRoute.- name:whoamikind:Serviceport:80tls:certResolver:myresolver---apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingress-notlsnamespace:whoami# Must be the same as the servicespec:entryPoints:- webroutes:- match:Host(`whoami.{{cluster.baseHostName}}`) \u0026amp;\u0026amp; PathPrefix(`/notls`)kind:Ruleservices:# Beware: the service MUST be in the same namespace than the IngressRoute.- name:whoamikind:Serviceport:80   1 2  kubectl apply -f ./kubernetes/xx-WhoAmI.yaml curl -kH \u0026#39;Host: whoami.{{cluster.baseHostName}}\u0026#39; \u0026#34;https://$(kubectl --namespace traefik get svc traefik -o json | jq --raw-output \u0026#39;.status.loadBalancer.ingress[].ip\u0026#39;)\u0026#34;   1  kubectl --namespace whoami auth can-i get service --as=system:serviceaccount:traefik:traefik    Yes ! Our host can access traefik, that redirects the request to whoami ! Next step is to let the world access it.\nBut before that, cleanup your testing mess:\n1  kubectl delete -f ./kubernetes/xx-WhoAmI.yaml    Hey, weâ€™ve done important things here ! Maybe itâ€™s time to commitâ€¦\n1 2 3 4  git add . git commit -m \u0026#34;Setup the cluster\u0026#39;s internal router Following guide @ https://gerkindev.github.io/devblog/walkthroughs/kubernetes/03-router/\u0026#34;    ","description":"","id":3,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Networking","Web service"],"title":"Setup the cluster's internal router","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/03-router/"},{"content":"Now that you have a router installed, you have to pass requests on your server to it. This setup use a single entry point directly binding some ports on the host server.\n1. Make a static and previsible configuration As you may have noticed in the step  \u0026nbsp;Kickstart the cluster, the metallb configuration use only dynamic adresses. But for the reverse proxy to work, we\u0026rsquo;ll need to be sure that our traefik router has a constant IP in your VPN. For this, modify your metallb configuration using the new \u0026nbsp;kubernetes/metallb-configmap.yaml template. This new configuration declares a new address pool named frontend with a single IP in it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  apiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|address-pools: - name: default protocol: layer2 addresses: - {{cluster.networkAddress}}.100-{{cluster.networkAddress}}.250 - name: frontend protocol: layer2 addresses: - {{cluster.networkAddress}}.99-{{cluster.networkAddress}}.99   1 2  # Update the configuration kubectl apply -f ./kubernetes/metallb-configmap.yaml   2. Set the router\u0026rsquo;s IP   References   Requesting specific IPs from metallb    Once the configmap has been changed, force our traefik service to use this new address \u0026ldquo;pool\u0026rdquo;. This is done using the annotation metallb.universe.tf/address-pool. Use the new \u0026nbsp;ðŸ“‹ kubernetes/traefik/05-Services.yaml template, and check that its IP is correct.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  # File: ./kubernetes/traefik/05-Services.yaml# https://doc.traefik.io/traefik/v2.4/user-guides/crd-acme/#services# With custom address pool for metallbapiVersion:v1kind:Servicemetadata:name:traefiknamespace:traefikannotations:metallb.universe.tf/address-pool:frontendspec:ports:- protocol:TCPname:webport:8000- protocol:TCPname:adminport:8080- protocol:TCPname:websecureport:4443selector:app:traefikcomponent:ingress-controllertype:LoadBalancer   1 2 3 4  # Update the configuration kubectl apply -f ./kubernetes/traefik/05-Services.yaml # Check the IP. It should be the single one in the pool defined by `frontend` in the metallb configuration kubectl --namespace traefik get svc   3. Setup the bare metal proxy We\u0026rsquo;ll use nginx as our bare reverse proxy. It will simply redirect every requests on the specified ports to traefik, that was  \u0026nbsp;previously installed in kubernetes. In the case of an SSL connection, it won\u0026rsquo;t be unwrapped.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  # Install nginx dnf install nginx # Get our traefik entry point clusterEntry=\u0026#34;$(kubectl --namespace traefik get svc traefik -o json | jq --raw-output \u0026#39;.status.loadBalancer.ingress[].ip\u0026#39;)\u0026#34; # Omit IPv6 if local interface (VM, or anything not being a real interface) if [[ $pubAddrv6 == fe80:* ]]; then pubAddrv6=\u0026#34;::\u0026#34;; fi # Create the nginx stream configuration for kubernetes mkdir /etc/nginx/streams.d cat \u0026lt;\u0026lt;EOF | tee /etc/nginx/streams.d/kubernetes-proxy.conf stream { server { listen 443; listen [::]:443; proxy_pass $clusterEntry:4443; } server { listen 80; listen [::]:80; proxy_pass $clusterEntry:8000; } } EOF # Include it in the nginx config # Check that the base config does not already bind on port 80 echo \u0026#39;\\ninclude /etc/nginx/streams.d/*.conf;\\n\u0026#39; | tee -a /etc/nginx/nginx.conf # Start \u0026amp; auto-start nginx systemctl enable --now nginx.service   Now, you should be able to reach your traefik router by requesting directly your entry point server. Test this with the \u0026nbsp;kubernetes/xx-WhoAmI.yaml template.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77  apiVersion:v1kind:Namespacemetadata:name:whoami---kind:DeploymentapiVersion:apps/v1metadata:name:whoaminamespace:whoamilabels:app:whoamispec:replicas:1selector:matchLabels:app:whoamitemplate:metadata:labels:app:whoamispec:containers:- name:whoamiimage:traefik/whoamiports:- name:webcontainerPort:80---apiVersion:v1kind:Servicemetadata:name:whoaminamespace:whoamispec:ports:- protocol:TCPname:webport:80selector:app:whoami---apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingress-tlsnamespace:whoami# Must be the same as the servicespec:entryPoints:- websecureroutes:- match:Host(`whoami.{{cluster.baseHostName}}`) \u0026amp;\u0026amp; PathPrefix(`/tls`)kind:Ruleservices:# Beware: the service MUST be in the same namespace than the IngressRoute.- name:whoamikind:Serviceport:80tls:certResolver:myresolver---apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingress-notlsnamespace:whoami# Must be the same as the servicespec:entryPoints:- webroutes:- match:Host(`whoami.{{cluster.baseHostName}}`) \u0026amp;\u0026amp; PathPrefix(`/notls`)kind:Ruleservices:# Beware: the service MUST be in the same namespace than the IngressRoute.- name:whoamikind:Serviceport:80   1 2  # Deploy it kubectl apply -f ./kubernetes/xx-WhoAmI.yaml   Make sure that whoami.{{cluster.baseHostName}} correctly resolves to your entry-point server (either via real DNS records or editing /etc/hosts), then try to access to:\n https://whoami.{{cluster.baseHostName}}/tls http://whoami.{{cluster.baseHostName}}/notls  If this works, you\u0026rsquo;re good to go !\n1  kubectl delete -f ./kubernetes/xx-WhoAmI.yaml    Hey, weâ€™ve done important things here ! Maybe itâ€™s time to commitâ€¦\n1 2 3 4  git add . git commit -m \u0026#34;Make services reachable from the world Following guide @ https://gerkindev.github.io/devblog/walkthroughs/kubernetes/04-reverse-proxy/\u0026#34;    ","description":"","id":4,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Networking","Web service"],"title":"Make services reachable from the world","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/04-reverse-proxy/"},{"content":"  References   Persistent Volume Dynamic Volume Provisioning Storage Classes https://www.youtube.com/watch?v=0swOh5C3OVM    As you may know, docker (and thus, kubernetes) does not persist anything by default. That means that everytime you restart a pod (container), it is in the exact same state as it was at its first execution, except for the mount points. Those mount points are real hard drive directories injected into your pod. Some apps we\u0026rsquo;ll setup later will require to persist data, and, more generally, when you\u0026rsquo;ll run real applications on your own, they will probably use a database or something.\n Seeing how this part is highly tied with your specific setup, you should really do this part by yourself using the references above. But in case you want a basic thing working, I\u0026rsquo;ll guide you through the setup of \u0026nbsp;CephFS.\nâ— I repeat myself, but you should really do this part by yourself. Do not use those as-is if you are using your cluster to host real applications. But just like you, I\u0026rsquo;m doing tests here and I am tired of walking around the whole internet just to experiment a few things.\n Now that I\u0026rsquo;ve warned you enough (just look above, again), let\u0026rsquo;s declare a \u0026nbsp;persistent volume !\nSetup CephFS Check prerequisites   References   https://rook.io/docs/rook/v1.5/ceph-prerequisites.html   }}\nhttps://rook.io/docs/rook/v1.5/k8s-pre-reqs.html\nhttps://rook.io/docs/rook/v1.5/ceph-quickstart.html\n1  dnf install -y lvm2   1 2 3  modprobe rbd # Auto-load them at startup echo \u0026#34;rbd\u0026#34; \u0026gt; /etc/modules-load.d/cephfs.conf   Make sure you have at least one unused partition or filesystem.\n1  lsblk -f   https://rook.io/docs/rook/v1.5/ceph-quickstart.html#tldr\n1 2 3 4 5 6 7 8 9  mkdir -p ./kubernetes/rook/storageclass git clone --single-branch --branch v1.5.6 https://github.com/rook/rook.git ~/rook cp ~/rook/cluster/examples/kubernetes/ceph/{crds,common,operator,cluster}.yaml ./kubernetes/rook # The FileSystem configuration cp ~/rook/cluster/examples/kubernetes/ceph/filesystem.yaml ./kubernetes/rook/filesystem.yaml # Filesystem storage class base config. See https://rook.io/docs/rook/v1.5/ceph-filesystem.html cp ~/rook/cluster/examples/kubernetes/ceph/csi/cephfs/storageclass.yaml ./kubernetes/rook/storageclass/filesystem.yaml # Block storage class base config. See https://rook.io/docs/rook/v1.5/ceph-block.html cp ~/rook/cluster/examples/kubernetes/ceph/csi/rbd/storageclass.yaml ./kubernetes/rook/storageclass/block.yaml   Take some time to RTFM and configure operator, cluster, filesystem, storageclass/filesystem \u0026amp; storageclass/block.\nWant to store on your master node ? Update the discover tolerations to be scheduled on the master node by modifying ./kubernetes/rook/operator.yaml like below\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # ...kind:CephClustermetadata:name:rook-cephspec:placement:all:tolerations:- effect:NoSchedulekey:node-role.kubernetes.io/controlplaneoperator:Exists- effect:NoSchedulekey:node-role.kubernetes.io/masteroperator:Exists  1 2 3 4 5 6 7  kubectl apply -f ./kubernetes/rook/crds.yaml kubectl apply -f ./kubernetes/rook/common.yaml kubectl apply -f ./kubernetes/rook/operator.yaml kubectl apply -f ./kubernetes/rook/cluster.yaml kubectl apply -f ./kubernetes/rook/filesystem.yaml kubectl apply -f ./kubernetes/rook/storageclass/block.yaml kubectl apply -f ./kubernetes/rook/storageclass/filesystem.yaml   Configure the rook cluster Open your ./kubernetes/rook/cluster.yaml file for further customization.\n The default settings save rook data in /var/lib/rook. This can be changed by setting dataDirHostPath. If working with 1 or 2 workers, make sure that spec.mon.count is equal to 1 (only for testing purposes). It is highly advised to explicitly set spec.storage  1  kubectl apply -f ./kubernetes/rook/cluster.yaml   Create a test PVC 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98  apiVersion:v1kind:Namespacemetadata:name:persistent-nginx---apiVersion:v1kind:PersistentVolumeClaimmetadata:name:test-pv-claimnamespace:persistent-nginxspec:storageClassName:rook-cephfsvolumeMode:FilesystemaccessModes:- ReadWriteOnceresources:requests:storage:5Gi---kind:DeploymentapiVersion:apps/v1metadata:name:nginxnamespace:persistent-nginxlabels:app:nginxspec:replicas:1selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:volumes:- name:test-pv-storagepersistentVolumeClaim:claimName:test-pv-claimcontainers:- name:nginximage:nginxports:- name:webcontainerPort:80volumeMounts:- mountPath:\u0026#34;/usr/share/nginx/html\u0026#34;name:test-pv-storage---apiVersion:v1kind:Servicemetadata:name:nginxnamespace:persistent-nginxspec:ports:- protocol:TCPname:webport:80selector:app:nginx---apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingress-tlsnamespace:persistent-nginx# Must be the same as the servicespec:entryPoints:- websecureroutes:- match:Host(`persistent-nginx.scitizen.loc`)kind:Ruleservices:# Beware: the service MUST be in the same namespace than the IngressRoute.- name:nginxkind:Serviceport:80tls:certResolver:myresolver---apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingress-notlsnamespace:persistent-nginx# Must be the same as the servicespec:entryPoints:- webroutes:- match:Host(`persistent-nginx.scitizen.loc`)kind:Ruleservices:# Beware: the service MUST be in the same namespace than the IngressRoute.- name:nginxkind:Serviceport:80   1 2 3  kubectl apply -f ./kubernetes/rook/xx-PersistentNginx.yaml kubectl -n persistent-nginx get pvc -w kubectl -n persistent-nginx exec -it deploy/nginx -- bash -c \u0026#39;echo \u0026#34;Hello world !\u0026#34; | tee /usr/share/nginx/html/index.html\u0026#39;   Going to http://persistent-nginx.{{cluster.baseHostName}}/index.html should display Hello world !. And if you kill and restart your container, your data will be kept ;).\n1  kubectl -n persistent-nginx exec -it deploy/nginx -- /bin/sh -c \u0026#34;kill 1\u0026#34;   Dashboard If you\u0026rsquo;re planning to expose the dashboard from outside of the cluster, you have to disable spec.dashboard.ssl to false, since traefik will do the SSL encription. Then, deploy the routing:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  apiVersion:v1kind:Servicemetadata:name:rook-ceph-mgr-dashboardnamespace:rook-ceph# namespace:clusterlabels:app:rook-ceph-mgrrook_cluster:rook-ceph# namespace:clusterspec:ports:- name:dashboardport:8443protocol:TCPtargetPort:8443selector:app:rook-ceph-mgrrook_cluster:rook-ceph---apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingress-dashboard-tlsnamespace:rook-ceph# Must be the same as the servicespec:entryPoints:- websecureroutes:- match:Host(`ceph.{{cluster.baseHostName}}`)kind:Ruleservices:# Beware: the service MUST be in the same namespace than the IngressRoute.- name:rook-ceph-mgr-dashboardkind:Serviceport:8443tls:certResolver:myresolver   1  kubectl apply -f ./kubernetes/rook/dashboard-ingress.yaml   After this, you should be able to access to ceph dashboard via https://ceph.{{cluster.baseHostName}}\n The default username is admin. The default password is stored in a secret. To get it, run: 1  kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=\u0026#34;{[\u0026#39;data\u0026#39;][\u0026#39;password\u0026#39;]}\u0026#34; | base64 --decode \u0026amp;\u0026amp; echo     Having problems ? \u0026nbsp;RTFM\nIf you made errors and want to purge rook-ceph, remove following patterns on each nodes running cephfs (usually, all the worker nodes):\n1 2 3 4  rm -r /var/lib/rook rm -r /var/lib/kubelet/plugins/rook* rm -r /var/lib/kubelet/plugins_registry/rook* wipefs --all --force /dev/sdX # Each of the used disks   See https://github.com/rook/rook/issues/4553\n Hey, weâ€™ve done important things here ! Maybe itâ€™s time to commitâ€¦\n1 2 3 4  git add . git commit -m \u0026#34;Make things persistent Following guide @ https://gerkindev.github.io/devblog/walkthroughs/kubernetes/05-storage/\u0026#34;    ","description":"","id":5,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Storage"],"title":"Make things persistent","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/05-storage/"},{"content":"Well, things are getting real and are on the point to become quite complex. So we\u0026rsquo;ll setup (super unsafe) dashboards to see what is going on easily. After all, we have nothing critical for now, but we might get troubles soon. And, don\u0026rsquo;t worry, we\u0026rsquo;ll make it safe just after that.\n1. Traefik dashboard: monitoring routes The traefik dashboard will help us in the diagnostics of our ingress routes and traefik-related stuff. For this, we need to:\n update our ingress controller previously deployed to enable the dashboard and create routes to the dashboard.  Use the \u0026nbsp;kubernetes/traefik/04-IngressController.yaml and \u0026nbsp;kubernetes/traefik/06-IngressRoutes.yaml templates.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  kind:DeploymentapiVersion:apps/v1metadata:name:traefiknamespace:traefiklabels:app:traefikcomponent:ingress-controllerspec:replicas:1selector:matchLabels:app:traefikcomponent:ingress-controllertemplate:metadata:labels:app:traefikcomponent:ingress-controllerspec:serviceAccountName:traefikcontainers:- name:traefikimage:traefik:v2.4args:- --api=true- --api.dashboard=true- --accesslog- --entrypoints.web.Address=:8000- --entrypoints.websecure.Address=:4443- --providers.kubernetescrd- --certificatesresolvers.myresolver.acme.tlschallenge- --certificatesresolvers.myresolver.acme.email=FILL@ME.COM- --certificatesresolvers.myresolver.acme.storage=acme.json# Please note that this is the staging Let\u0026#39;s Encrypt server.# Once you get things working, you should remove that whole line altogether.- --certificatesresolvers.myresolver.acme.caserver=https://acme-staging-v02.api.letsencrypt.org/directoryports:- name:webcontainerPort:8000- name:websecurecontainerPort:4443- name:admincontainerPort:8080   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:dashboardnamespace:traefikspec:entryPoints:- websecureroutes:- match:Host(`traefik.{{cluster.baseHostName}}`) \u0026amp;\u0026amp; PathPrefix(`/dashboard`)kind:Ruleservices:- name:dashboard@internalkind:TraefikServicemiddlewares:- name:dashboard-stripprefixnamespace:traefiktls:certResolver:myresolver---apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:apinamespace:traefikspec:entryPoints:- websecureroutes:- match:Host(`traefik.{{cluster.baseHostName}}`) \u0026amp;\u0026amp; PathPrefix(`/api`)kind:Ruleservices:- name:api@internalkind:TraefikServicetls:certResolver:myresolver---apiVersion:traefik.containo.us/v1alpha1kind:Middlewaremetadata:name:dashboard-stripprefixnamespace:traefikspec:stripPrefix:prefixes:- /dashboard- /dashboard/   1 2  kubectl apply -f ./kubernetes/traefik/04-IngressController.yaml kubectl apply -f ./kubernetes/traefik/06-IngressRoutes.yaml   Now, you should be able to reach the dashboard via https://traefik.{{cluster.baseHostName}}/dashboard/.\n2. Kibana: harvest data from your cluster   References   https://kubernetes.io/docs/tasks/debug-application-cluster/logging-elasticsearch-kibana/ https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/#fluentd https://www.elastic.co/kibana https://www.elastic.co/elasticsearch/     \u0026nbsp;Kibana is a super versatile tool to visualize data stored in Elasticsearch.\n \u0026nbsp;Elasticsearch is a database particularly adapted for search engines, with \u0026nbsp;fulltext search and \u0026nbsp;scoring capabilities.\nTogether, they compose the perfect combo to ingest all our cluster\u0026rsquo;s logs, and do searches, visualizations, tracking, and everything you\u0026rsquo;ll need to understand what is going on in your cluster\u0026rsquo;s apps.\nElasticsearch may be quite resources-consuming, and your machines may not be optimized to make it run smoothly with the consequent flow of data its about to ingest. I strongly advise you to read some installation documentations to make things correctly.\nReading a guide don\u0026rsquo;t dispense you from RTFMing.\n 2.1. Pods logs We\u0026rsquo;ll start by getting our pods (container) logs. Deploy the following configuration files:\n 1 2 3 4  apiVersion:v1kind:Namespacemetadata:name:kibana   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  # See https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/#elastic---apiVersion:apps/v1kind:Deploymentmetadata:name:elasticsearchnamespace:kibanalabels:app:kibanacomponent:elasticsearchspec:selector:matchLabels:app:kibanacomponent:elasticsearchtemplate:metadata:labels:app:kibanacomponent:elasticsearchspec:containers:- name:elasticsearchimage:docker.elastic.co/elasticsearch/elasticsearch:7.10.2env:- name:discovery.typevalue:single-nodeports:- containerPort:9200name:httpprotocol:TCP---apiVersion:v1kind:Servicemetadata:name:elasticsearchnamespace:kibanalabels:app:kibanacomponent:elasticsearchspec:selector:app:kibanacomponent:elasticsearchports:- port:9200targetPort:9200protocol:TCPname:http   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  # See https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/#kibana---apiVersion:apps/v1kind:Deploymentmetadata:name:kibananamespace:kibanalabels:app:kibanacomponent:kibanaspec:selector:matchLabels:app:kibanacomponent:kibanatemplate:metadata:labels:app:kibanacomponent:kibanaspec:containers:- name:kibanaimage:docker.elastic.co/kibana/kibana:7.10.2env:- name:ELASTICSEARCH_URLvalue:http://elasticsearch.kibana.svc.cluster.local:9200- name:XPACK_SECURITY_ENABLEDvalue:\u0026#34;true\u0026#34;# - name: SERVER_NAME# value: kibana.{{cluster.baseHostName}}ports:- containerPort:5601name:httpprotocol:TCP---apiVersion:v1kind:Servicemetadata:name:kibananamespace:kibanalabels:app:kibanacomponent:kibanaspec:selector:app:kibanacomponent:kibanaports:- port:80targetPort:5601protocol:TCPname:http   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103  # See https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/#fluentd---apiVersion:v1kind:ServiceAccountmetadata:name:fluentdnamespace:kube-system---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:fluentdnamespace:kube-systemrules:- apiGroups:- \u0026#34;\u0026#34;resources:- pods- namespacesverbs:- get- list- watch---kind:ClusterRoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:fluentdroleRef:kind:ClusterRolename:fluentdapiGroup:rbac.authorization.k8s.iosubjects:- kind:ServiceAccountname:fluentdnamespace:kube-system---apiVersion:apps/v1kind:DaemonSetmetadata:name:fluentdnamespace:kube-systemlabels:app:kibanacomponent:fluentdspec:selector:matchLabels:app:kibanacomponent:fluentdtemplate:metadata:labels:app:kibanacomponent:fluentdspec:serviceAccountName:fluentdtolerations:- key:node-role.kubernetes.io/mastereffect:NoSchedulecontainers:- name:fluentdimage:fluent/fluentd-kubernetes-daemonset:v1.12-debian-elasticsearch7-1env:- name:FLUENT_ELASTICSEARCH_HOSTvalue:elasticsearch.kibana.svc.cluster.local- name:FLUENT_ELASTICSEARCH_PORTvalue:\u0026#34;9200\u0026#34;- name:FLUENT_ELASTICSEARCH_SCHEMEvalue:\u0026#34;http\u0026#34;- name:FLUENT_UIDvalue:\u0026#34;0\u0026#34;# See https://github.com/fluent/fluentd-kubernetes-daemonset#disable-systemd-input- name:FLUENTD_SYSTEMD_CONFvalue:disablevolumeMounts:- name:varlogmountPath:/var/log- name:auditlogmountPath:/var/log/kubernetes/kube-apiserver-audit.logreadOnly:true- name:varlibdockercontainersmountPath:/var/lib/docker/containersreadOnly:trueterminationGracePeriodSeconds:30volumes:- name:varloghostPath:path:/var/log- name:auditloghostPath:path:\u0026#34;{{audit.sourceLogDir}}/{{audit.sourceLogFile}}\u0026#34;- name:varlibdockercontainershostPath:path:/var/lib/docker/containers   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingress-securenamespace:kibanaspec:entryPoints:- websecureroutes:- match:Host(`kibana.{{cluster.baseHostName}}`)kind:Ruleservices:- name:kibanakind:Servicenamespace:kibanaport:80tls:certResolver:myresolver   1 2 3 4 5  kubectl apply -f ./kubernetes/kibana/01-Namespace.yaml kubectl apply -f ./kubernetes/kibana/11-Elasticsearch.yaml kubectl apply -f ./kubernetes/kibana/12-Kibana.yaml kubectl apply -f ./kubernetes/kibana/13-Fluentd.yaml kubectl apply -f ./kubernetes/kibana/21-Ingress.yaml   Once applied, you should be able to reach your kibana dashboard via https://kibana.{{cluster.baseHostName}}/. Be patient, it may take a bit of time to initialize ElasticSearch and Kibana itself. Once they started up, let\u0026rsquo;s configure those !\nGo to the \u0026nbsp;Kibana \u0026gt; Discover \u0026gt; Index patterns page. Kibana should ask to create indices. Index logs with pattern logstash*.\nThen, set up the time field as @timestamp.\nFinally, go back to the Discover page. You should get at least your pods logs!\nI strongly recommend you to inspect logs carefully, to clean up as many errors as possible. Yeah, you should have done that all the way long, but looking everywhere is painful, I know. So do that now while you don\u0026rsquo;t have a bunch of things to pollute your streams.\n2.2. Audit logs For a reason I can\u0026rsquo;t explain, the default settings for audit log parsing from fluentd are incorrect. Moreover, I find the \u0026ldquo;all settings in a single file\u0026rdquo; pattern awful. So we are going to reconfigure fluentd to parse correctly our logs. Use the \u0026nbsp;kubernetes/kibana/31-Fluentd.yaml \u0026amp; \u0026nbsp;kubernetes/kibana/32-FluentdConfigMap.yaml templates. The 1st one revrite some of the configuration of fluentd to use our custom configs.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  # See https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/#fluentd---apiVersion:apps/v1kind:DaemonSetmetadata:name:fluentdnamespace:kube-systemlabels:app:kibanacomponent:fluentdspec:selector:matchLabels:app:kibanacomponent:fluentdtemplate:metadata:labels:app:kibanacomponent:fluentdspec:serviceAccountName:fluentdtolerations:- key:node-role.kubernetes.io/mastereffect:NoSchedulecontainers:- name:fluentdimage:fluent/fluentd-kubernetes-daemonset:v1.12-debian-elasticsearch7-1env:- name:FLUENT_ELASTICSEARCH_HOSTvalue:elasticsearch.kibana.svc.cluster.local- name:FLUENT_ELASTICSEARCH_PORTvalue:\u0026#34;9200\u0026#34;- name:FLUENT_ELASTICSEARCH_SCHEMEvalue:\u0026#34;http\u0026#34;- name:FLUENT_UIDvalue:\u0026#34;0\u0026#34;# See https://github.com/fluent/fluentd-kubernetes-daemonset#disable-systemd-input- name:FLUENTD_SYSTEMD_CONFvalue:disablevolumeMounts:- name:fluentd-config-kubernetes-confmountPath:/fluentd/etc/kubernetes.confsubPath:kubernetes.conf- name:fluentd-config-conf-additionalmountPath:/fluentd/etc/conf.d/- name:varlogmountPath:/var/log- name:auditlogmountPath:/var/log/kubernetes- name:varlibdockercontainersmountPath:/var/lib/docker/containersreadOnly:trueterminationGracePeriodSeconds:30volumes:- name:fluentd-config-kubernetes-confconfigMap:name:fluentd-config-kubernetes-conf- name:fluentd-config-conf-additionalconfigMap:name:fluentd-config-conf-additional- name:varloghostPath:path:/var/log- name:auditloghostPath:path:/var/log/kubernetes- name:varlibdockercontainershostPath:path:/var/lib/docker/containers   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212  # Those configmaps are taken from https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/docker-image/v1.10/debian-elasticsearch6/conf/kubernetes.confapiVersion:v1data:kubernetes.conf:|-\u0026lt;label @FLUENT_LOG\u0026gt; \u0026lt;match fluent.**\u0026gt; @type null \u0026lt;/match\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;filter kubernetes.**\u0026gt; @type kubernetes_metadata @id filter_kube_metadata kubernetes_url \u0026#34;#{ENV[\u0026#39;FLUENT_FILTER_KUBERNETES_URL\u0026#39;] || \u0026#39;https://\u0026#39; + ENV.fetch(\u0026#39;KUBERNETES_SERVICE_HOST\u0026#39;) + \u0026#39;:\u0026#39; + ENV.fetch(\u0026#39;KUBERNETES_SERVICE_PORT\u0026#39;) + \u0026#39;/api\u0026#39;}\u0026#34; verify_ssl \u0026#34;#{ENV[\u0026#39;KUBERNETES_VERIFY_SSL\u0026#39;] || true}\u0026#34; ca_file \u0026#34;#{ENV[\u0026#39;KUBERNETES_CA_FILE\u0026#39;]}\u0026#34; skip_labels \u0026#34;#{ENV[\u0026#39;FLUENT_KUBERNETES_METADATA_SKIP_LABELS\u0026#39;] || \u0026#39;false\u0026#39;}\u0026#34; skip_container_metadata \u0026#34;#{ENV[\u0026#39;FLUENT_KUBERNETES_METADATA_SKIP_CONTAINER_METADATA\u0026#39;] || \u0026#39;false\u0026#39;}\u0026#34; skip_master_url \u0026#34;#{ENV[\u0026#39;FLUENT_KUBERNETES_METADATA_SKIP_MASTER_URL\u0026#39;] || \u0026#39;false\u0026#39;}\u0026#34; skip_namespace_metadata \u0026#34;#{ENV[\u0026#39;FLUENT_KUBERNETES_METADATA_SKIP_NAMESPACE_METADATA\u0026#39;] || \u0026#39;false\u0026#39;}\u0026#34; \u0026lt;/filter\u0026gt;kind:ConfigMapmetadata:name:fluentd-config-kubernetes-confnamespace:kube-system---apiVersion:v1data:container-logs.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_container_logs path /var/log/containers/*.log pos_file /var/log/fluentd-containers.log.pos tag \u0026#34;#{ENV[\u0026#39;FLUENT_CONTAINER_TAIL_TAG\u0026#39;] || \u0026#39;kubernetes.*\u0026#39;}\u0026#34; exclude_path \u0026#34;#{ENV[\u0026#39;FLUENT_CONTAINER_TAIL_EXCLUDE_PATH\u0026#39;] || use_default}\u0026#34; read_from_head true \u0026lt;parse\u0026gt; @type \u0026#34;#{ENV[\u0026#39;FLUENT_CONTAINER_TAIL_PARSER_TYPE\u0026#39;] || \u0026#39;json\u0026#39;}\u0026#34; time_format %Y-%m-%dT%H:%M:%S.%NZ \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;salt.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_minion path /var/log/salt/minion pos_file /var/log/fluentd-salt.pos tag salt \u0026lt;parse\u0026gt; @type regexp expression /^(?\u0026lt;time\u0026gt;[^ ]* [^ ,]*)[^\\[]*\\{{^\\}}*\\]\\[(?\u0026lt;severity\u0026gt;[^ \\]]*) *\\] (?\u0026lt;message\u0026gt;.*)$/ time_format %Y-%m-%d %H:%M:%S \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;startupscript.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_startupscript path /var/log/startupscript.log pos_file /var/log/fluentd-startupscript.log.pos tag startupscript \u0026lt;parse\u0026gt; @type syslog \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;docker.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_docker path /var/log/docker.log pos_file /var/log/fluentd-docker.log.pos tag docker \u0026lt;parse\u0026gt; @type regexp expression /^time=\u0026#34;(?\u0026lt;time\u0026gt;[^)]*)\u0026#34; level=(?\u0026lt;severity\u0026gt;[^ ]*) msg=\u0026#34;(?\u0026lt;message\u0026gt;[^\u0026#34;]*)\u0026#34;( err=\u0026#34;(?\u0026lt;error\u0026gt;[^\u0026#34;]*)\u0026#34;)?( statusCode=($\u0026lt;status_code\u0026gt;\\d+))?/ \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;etcd.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_etcd path /var/log/etcd.log pos_file /var/log/fluentd-etcd.log.pos tag etcd \u0026lt;parse\u0026gt; @type none \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;kubelet.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_kubelet multiline_flush_interval 5s path /var/log/kubelet.log pos_file /var/log/fluentd-kubelet.log.pos tag kubelet \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;kube-proxy.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_kube_proxy multiline_flush_interval 5s path /var/log/kube-proxy.log pos_file /var/log/fluentd-kube-proxy.log.pos tag kube-proxy \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;kube-apiserver.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_kube_apiserver multiline_flush_interval 5s path /var/log/kube-apiserver.log pos_file /var/log/fluentd-kube-apiserver.log.pos tag kube-apiserver \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;kube-controller-manager.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_kube_controller_manager multiline_flush_interval 5s path /var/log/kube-controller-manager.log pos_file /var/log/fluentd-kube-controller-manager.log.pos tag kube-controller-manager \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;kube-scheduler.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_kube_scheduler multiline_flush_interval 5s path /var/log/kube-scheduler.log pos_file /var/log/fluentd-kube-scheduler.log.pos tag kube-scheduler \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;rescheduler.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_rescheduler multiline_flush_interval 5s path /var/log/rescheduler.log pos_file /var/log/fluentd-rescheduler.log.pos tag rescheduler \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;glbc.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_glbc multiline_flush_interval 5s path /var/log/glbc.log pos_file /var/log/fluentd-glbc.log.pos tag glbc \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;autoscaler.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_cluster_autoscaler multiline_flush_interval 5s path /var/log/cluster-autoscaler.log pos_file /var/log/fluentd-cluster-autoscaler.log.pos tag cluster-autoscaler \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;audit-log.conf:|-# Example: # {\u0026#34;kind\u0026#34;:\u0026#34;Event\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;audit.k8s.io/v1\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;Metadata\u0026#34;,\u0026#34;auditID\u0026#34;:\u0026#34;xxxx\u0026#34;,\u0026#34;stage\u0026#34;:\u0026#34;ResponseComplete\u0026#34;,\u0026#34;requestURI\u0026#34;:\u0026#34;/apis/...?timeout=10s\u0026#34;,\u0026#34;verb\u0026#34;:\u0026#34;update\u0026#34;,\u0026#34;user\u0026#34;:{\u0026#34;username\u0026#34;:\u0026#34;system:kube-scheduler\u0026#34;,\u0026#34;groups\u0026#34;:[\u0026#34;system:authenticated\u0026#34;]},\u0026#34;sourceIPs\u0026#34;:[\u0026#34;xxx.xxx.xxx.xxx\u0026#34;],\u0026#34;userAgent\u0026#34;:\u0026#34;kube-scheduler/v1.19.3 (linux/amd64) kubernetes/1e11e4a/leader-election\u0026#34;,\u0026#34;objectRef\u0026#34;:{\u0026#34;resource\u0026#34;:\u0026#34;leases\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kube-system\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;kube-scheduler\u0026#34;,\u0026#34;uid\u0026#34;:\u0026#34;xxxx\u0026#34;,\u0026#34;apiGroup\u0026#34;:\u0026#34;coordination.k8s.io\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;resourceVersion\u0026#34;:\u0026#34;52124\u0026#34;},\u0026#34;responseStatus\u0026#34;:{\u0026#34;metadata\u0026#34;:{},\u0026#34;code\u0026#34;:200},\u0026#34;requestReceivedTimestamp\u0026#34;:\u0026#34;2020-10-29T16:26:44.967339Z\u0026#34;,\u0026#34;stageTimestamp\u0026#34;:\u0026#34;2020-10-29T16:26:44.968796Z\u0026#34;,\u0026#34;annotations\u0026#34;:{\u0026#34;authorization.k8s.io/decision\u0026#34;:\u0026#34;allow\u0026#34;,\u0026#34;authorization.k8s.io/reason\u0026#34;:\u0026#34;RBAC: allowed by ClusterRoleBinding \\\u0026#34;system:kube-scheduler\\\u0026#34; of ClusterRole \\\u0026#34;system:kube-scheduler\\\u0026#34; to User \\\u0026#34;system:kube-scheduler\\\u0026#34;\u0026#34;}} # {\u0026#34;kind\u0026#34;:\u0026#34;Event\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;audit.k8s.io/v1\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;Request\u0026#34;,\u0026#34;auditID\u0026#34;:\u0026#34;xxxx\u0026#34;,\u0026#34;stage\u0026#34;:\u0026#34;ResponseComplete\u0026#34;,\u0026#34;requestURI\u0026#34;:\u0026#34;/api/....?resourceVersion=0\\u0026timeout=10s\u0026#34;,\u0026#34;verb\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;user\u0026#34;:{\u0026#34;username\u0026#34;:\u0026#34;system:node:kube-slave-1\u0026#34;,\u0026#34;groups\u0026#34;:[\u0026#34;system:nodes\u0026#34;,\u0026#34;system:authenticated\u0026#34;]},\u0026#34;sourceIPs\u0026#34;:[\u0026#34;xxx.xxx.xxx.xxx\u0026#34;],\u0026#34;userAgent\u0026#34;:\u0026#34;kubelet/v1.19.3 (linux/amd64) kubernetes/1e11e4a\u0026#34;,\u0026#34;objectRef\u0026#34;:{\u0026#34;resource\u0026#34;:\u0026#34;nodes\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;kube-slave-1\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;},\u0026#34;responseStatus\u0026#34;:{\u0026#34;metadata\u0026#34;:{},\u0026#34;code\u0026#34;:200},\u0026#34;requestReceivedTimestamp\u0026#34;:\u0026#34;2020-10-29T16:26:45.099703Z\u0026#34;,\u0026#34;stageTimestamp\u0026#34;:\u0026#34;2020-10-29T16:26:45.100167Z\u0026#34;,\u0026#34;annotations\u0026#34;:{\u0026#34;authorization.k8s.io/decision\u0026#34;:\u0026#34;allow\u0026#34;,\u0026#34;authorization.k8s.io/reason\u0026#34;:\u0026#34;\u0026#34;}} \u0026lt;source\u0026gt; @type tail @id in_tail_kube_apiserver_audit multiline_flush_interval 5s path /var/log/kubernetes/kube-apiserver-audit.log pos_file /var/log/kube-apiserver-audit.log.pos tag audit \u0026lt;parse\u0026gt; @type json time_key requestReceivedTimestamp time_type string time_format %Y-%m-%dT%T.%L%Z \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;kind:ConfigMapmetadata:name:fluentd-config-conf-additionalnamespace:kube-system   1 2  kubectl apply -f ./kubernetes/kibana/31-Fluentd.yaml kubectl apply -f ./kubernetes/kibana/32-FluentdConfigMap.yaml   You should now be able to filter your logs by tag, and look at the audit logs.\nSo, in this setup, your audit logs are at 2 places: directly bare on your server, and in kibana. This redundancy is important IMO because whatever happens to your cluster (even a full flush of all your pods), you should be able to know who did what.\n2.3. Make things persistent Now that you have set up everything, you might have seen that everytime the ElasticSearch pod is restarted, the database is emptied. This is normal so far, because we don\u0026rsquo;t actually write any data on a persistent storage. For now ! But let\u0026rsquo;s solve that.\n3. Kube dashboard: Web UI to administrate the cluster   References   https://kubernetes.io/fr/docs/tasks/access-application-cluster/web-ui-dashboard/     1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318  # Copyright 2017 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;);# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.apiVersion:v1kind:Namespacemetadata:name:kubernetes-dashboard---apiVersion:v1kind:ServiceAccountmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboard---apiVersion:v1kind:Secretmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-certsnamespace:kubernetes-dashboardtype:Opaque---apiVersion:v1kind:Secretmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-csrfnamespace:kubernetes-dashboardtype:Opaquedata:csrf:\u0026#34;\u0026#34;---apiVersion:v1kind:Secretmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-key-holdernamespace:kubernetes-dashboardtype:Opaque---kind:ConfigMapapiVersion:v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-settingsnamespace:kubernetes-dashboard---kind:RoleapiVersion:rbac.authorization.k8s.io/v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardrules:# Allow Dashboard to get, update and delete Dashboard exclusive secrets.- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;secrets\u0026#34;]resourceNames:[\u0026#34;kubernetes-dashboard-key-holder\u0026#34;,\u0026#34;kubernetes-dashboard-certs\u0026#34;,\u0026#34;kubernetes-dashboard-csrf\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;delete\u0026#34;]# Allow Dashboard to get and update \u0026#39;kubernetes-dashboard-settings\u0026#39; config map.- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;configmaps\u0026#34;]resourceNames:[\u0026#34;kubernetes-dashboard-settings\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;update\u0026#34;]# Allow Dashboard to get metrics.- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;services\u0026#34;]resourceNames:[\u0026#34;heapster\u0026#34;,\u0026#34;dashboard-metrics-scraper\u0026#34;]verbs:[\u0026#34;proxy\u0026#34;]- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;services/proxy\u0026#34;]resourceNames:[\u0026#34;heapster\u0026#34;,\u0026#34;http:heapster:\u0026#34;,\u0026#34;https:heapster:\u0026#34;,\u0026#34;dashboard-metrics-scraper\u0026#34;,\u0026#34;http:dashboard-metrics-scraper\u0026#34;]verbs:[\u0026#34;get\u0026#34;]---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardrules:# Allow Metrics Scraper to get metrics from the Metrics server- apiGroups:[\u0026#34;metrics.k8s.io\u0026#34;]resources:[\u0026#34;pods\u0026#34;,\u0026#34;nodes\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;]---apiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardroleRef:apiGroup:rbac.authorization.k8s.iokind:Rolename:kubernetes-dashboardsubjects:- kind:ServiceAccountname:kubernetes-dashboardnamespace:kubernetes-dashboard---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:kubernetes-dashboardroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:viewsubjects:- kind:ServiceAccountname:kubernetes-dashboardnamespace:kubernetes-dashboard---kind:ServiceapiVersion:v1metadata:labels:app:kube-dashboardcomponent:kube-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardspec:ports:- port:80targetPort:9090selector:app:kube-dashboardcomponent:kube-dashboard---kind:DeploymentapiVersion:apps/v1metadata:labels:app:kube-dashboardcomponent:kube-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardspec:replicas:1revisionHistoryLimit:10selector:matchLabels:app:kube-dashboardcomponent:kube-dashboardtemplate:metadata:labels:app:kube-dashboardcomponent:kube-dashboardspec:containers:- name:kubernetes-dashboardimage:kubernetesui/dashboard:v2.0.1imagePullPolicy:Alwaysports:- containerPort:9090# containerPort: 8443protocol:TCPargs:- --insecure-port=9090# ADDED- --enable-insecure-login# ADDED# - --auto-generate-certificates- --namespace=kubernetes-dashboard- --authentication-mode=token# Uncomment the following line to manually specify Kubernetes API server Host# If not specified, Dashboard will attempt to auto discover the API server and connect# to it. Uncomment only if the default does not work.# - --apiserver-host=http://my-address:portvolumeMounts:- name:kubernetes-dashboard-certsmountPath:/certs# Create on-disk volume to store exec logs- mountPath:/tmpname:tmp-volumelivenessProbe:httpGet:# scheme: HTTPSpath:/# port: 8443port:9090# ADDEDinitialDelaySeconds:30timeoutSeconds:30securityContext:# allowPrivilegeEscalation: false allowPrivilegeEscalation:truereadOnlyRootFilesystem:truerunAsUser:1001runAsGroup:2001volumes:- name:kubernetes-dashboard-certssecret:secretName:kubernetes-dashboard-certs- name:tmp-volumeemptyDir:{}serviceAccountName:kubernetes-dashboardnodeSelector:\u0026#34;beta.kubernetes.io/os\u0026#34;: linux# Comment the following tolerations if Dashboard must not be deployed on mastertolerations:- key:node-role.kubernetes.io/mastereffect:NoSchedule---kind:ServiceapiVersion:v1metadata:labels:k8s-app:dashboard-metrics-scrapername:dashboard-metrics-scrapernamespace:kubernetes-dashboardspec:ports:- port:8000targetPort:8000selector:k8s-app:dashboard-metrics-scraper---kind:DeploymentapiVersion:apps/v1metadata:labels:k8s-app:dashboard-metrics-scrapername:dashboard-metrics-scrapernamespace:kubernetes-dashboardspec:replicas:1revisionHistoryLimit:10selector:matchLabels:k8s-app:dashboard-metrics-scrapertemplate:metadata:labels:k8s-app:dashboard-metrics-scraperannotations:seccomp.security.alpha.kubernetes.io/pod:\u0026#39;runtime/default\u0026#39;spec:containers:- name:dashboard-metrics-scraperimage:kubernetesui/metrics-scraper:v1.0.1ports:- containerPort:8000protocol:TCPlivenessProbe:httpGet:scheme:HTTPpath:/port:8000initialDelaySeconds:30timeoutSeconds:30volumeMounts:- mountPath:/tmpname:tmp-volumesecurityContext:allowPrivilegeEscalation:falsereadOnlyRootFilesystem:truerunAsUser:1001runAsGroup:2001serviceAccountName:kubernetes-dashboardnodeSelector:\u0026#34;beta.kubernetes.io/os\u0026#34;: linux# Comment the following tolerations if Dashboard must not be deployed on mastertolerations:- key:node-role.kubernetes.io/mastereffect:NoSchedulevolumes:- name:tmp-volumeemptyDir:{}   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingressroute-dashboardnamespace:kubernetes-dashboardspec:entryPoints:- websecureroutes:- match:Host(`kube-dashboard.bar.com`)kind:Ruleservices:- name:kubernetes-dashboardnamespace:kubernetes-dashboardkind:Serviceport:80tls:certResolver:myresolver   1 2  kubectl apply -f ./kubernetes/kube-dashboard/01-Dashboard.yaml kubectl apply -f ./kubernetes/kube-dashboard/02-IngressRoutes.yaml   Then, for debugging purpose, we\u0026rsquo;ll set up a test service account that can only view and list items in the dashboard. This service account will be named watchdog.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70  apiVersion:v1kind:ServiceAccountmetadata:labels:k8s-app:kubernetes-dashboardname:watchdognamespace:kubernetes-dashboard---kind:RoleapiVersion:rbac.authorization.k8s.io/v1metadata:labels:k8s-app:kubernetes-dashboardname:watchdognamespace:kubernetes-dashboardrules:- apiGroups:[\u0026#34;*\u0026#34;]resources:[\u0026#34;*\u0026#34;]resourceNames:[\u0026#34;*\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;]---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:labels:k8s-app:kubernetes-dashboardname:watchdogrules:# Allow Metrics Scraper to get metrics from the Metrics server- apiGroups:[\u0026#34;*\u0026#34;]resources:[\u0026#34;*\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;]---apiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:labels:k8s-app:kubernetes-dashboardname:watchdognamespace:kubernetes-dashboardroleRef:apiGroup:rbac.authorization.k8s.iokind:Rolename:watchdogsubjects:- kind:ServiceAccountname:watchdognamespace:kubernetes-dashboard---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:watchdogroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:watchdogsubjects:- kind:ServiceAccountname:watchdognamespace:kubernetes-dashboard   1 2 3 4 5 6  # Create the role, cluster role and service account using them kubectl apply -f ./kubernetes/kube-dashboard/03-ServiceAccount.yaml # Get the secret\u0026#39;s name secret_name=\u0026#34;$(kubectl get serviceaccount watchdog -n kubernetes-dashboard -o json | jq \u0026#39;.secrets[0].name\u0026#39; -r)\u0026#34; # Get the secret\u0026#39;s token echo $(kubectl get secret $secret_name -n kubernetes-dashboard -o json | jq \u0026#39;.data.token\u0026#39; -r | base64 --decode)   Now, navigate to https://kube-dashboard.{{cluster.baseHostName}} and log in using the token you got above.\nYou should be able to see all resources in your cluster.\nYes, this is super unsafe. That\u0026rsquo;s why we are going to add authentication right now, and why I told you not to make this publicly exposed for now.\n Hey, weâ€™ve done important things here ! Maybe itâ€™s time to commitâ€¦\n1 2 3 4  git add . git commit -m \u0026#34;Monitoring: See what is going on Following guide @ https://gerkindev.github.io/devblog/walkthroughs/kubernetes/06-monitoring/\u0026#34;    ","description":"","id":6,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Monitoring"],"title":"Monitoring: See what is going on","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/06-monitoring/"},{"content":"Here is a graph of the RBAC setup we are going to implement:\n1. Setup keycloak We\u0026rsquo;ll use keycloak to proxy our authentication for all monitors, using a single realm. You may use several realms in real-life situations. This is probably the tough part, and you may tweak heavily the following guide. Moreover, I may forgot to write some instructions, or somes are heavily linked to your very own setup.\n1.1. Install keycloak   References   https://kubernetes.github.io/ingress-nginx/examples/auth/oauth-external-auth/ https://itnext.io/protect-kubernetes-dashboard-with-openid-connect-104b9e75e39c https://geek-cookbook.funkypenguin.co.nz/ha-docker-swarm/traefik-forward-auth/keycloak/ https://medium.com/docker-hacks/how-to-apply-authentication-to-any-web-service-in-15-minutes-using-keycloak-and-keycloak-proxy-e4dd88bc1cd5    https://www.openshift.com/blog/adding-authentication-to-your-kubernetes-web-applications-with-keycloak\nStart by installing Keycloak via the \u0026nbsp;ðŸ“š Helm chart \u0026amp; expose it, using following templates:\n  \u0026nbsp;kubernetes/authentication/01-KeycloakChartValues.yaml  \u0026nbsp;kubernetes/authentication/02-PublicRoute.yaml   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  extraEnv:|- name: PROXY_ADDRESS_FORWARDING value: \u0026#34;true\u0026#34; - name: KEYCLOAK_USER value: {{keycloak.adminUser}} - name: KEYCLOAK_PASSWORD value: {{keycloak.adminPassword}}podLabels:app:keycloakcomponent:keycloakservice:labels:app:keycloakcomponent:keycloakhttpsPort:443# 8443 by default, but it should be reachable via the same URL from outside than inside, eg `https://keycloak.{{cluster.baseHostName}}`postgresql:enabled:false#storageClass: some-storage   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:public-routenamespace:keycloakspec:entryPoints:- websecureroutes:- match:Host(`keycloak.{{cluster.baseHostName}}`)kind:Ruleservices:- name:keycloak-httpkind:Servicenamespace:keycloakport:80tls:certResolver:myresolver   Note that the template \u0026nbsp;kubernetes/authentication/01-KeycloakChartValues.yaml contains the bare minimum settings for the keycloak chart with your route. You should tweak them to match your setup \u0026amp; security requirements. \u0026nbsp;RTFM the chart\n1 2 3 4 5 6  # Add the keycloak chart repository helm repo add codecentric https://codecentric.github.io/helm-charts # Install keycloak in its dedicated namespace helm install keycloak -f ./kubernetes/authentication/01-KeycloakChartValues.yaml codecentric/keycloak --version 9.9.2 --namespace keycloak --create-namespace # Initialize traefik routing to it. kubectl apply -f ./kubernetes/authentication/02-PublicRoute.yaml    You can remove this chart by running helm uninstall keycloak -n keycloak, or by deleting namespace keycloak altogether.\n This install should end by displaying either the default keycloak user\u0026rsquo;s password, or the command to get it.\n1.2. Alias hostname of keycloak from the cluster   References   https://stackoverflow.com/a/54692872     Note : This part is not required if you use a real host name registered with a A record. But if you use a fake hostname (which I still recommend at that point), DNS resolution of services trying to reach keycloak will fail, because your pods don\u0026rsquo;t know where is https://keycloak.{{cluster.baseHostName}}.\n We are going to alias the hostname keycloak.{{cluster.baseHostName}} so that it resolves to the internal name of our keycloak service, keycloak-http.keycloak.svc.cluster.local. For this, you need to edit CoreDNS configuration.\n1 2  # Open the CoreDNS config map kubectl edit configmap coredns -n kube-system   Add the following into your Corefile section:\n1  rewrite name keycloak.{{cluster.baseHostName}} keycloak-http.keycloak.svc.cluster.local    I added it just above the ready block (I don\u0026rsquo;t know if it really matters).\n Then, restart CoreDNS pods by deleting them.\n1  kubectl -n kube-system delete pods $(kubectl -n kube-system get pods -l k8s-app=kube-dns -o json | jq \u0026#39;.items[].metadata.name\u0026#39; -r)   This may take some time and crash kubectl connection to the API server. Don\u0026rsquo;t worry, it will reschedule our pods and restart everything after some time.\nSo, now, we can use keycloak.{{cluster.baseHostName}} as a domain name everywhere we want, and it will resolve to the cluster\u0026rsquo;s internal IP of keycloak\u0026rsquo;s service.\n1.3. Open keycloak admin panel Log in to your keycloak admin dashboard by reaching https://keycloak.{{cluster.baseHostName}}/auth/admin/.\n Username: ðŸ”– keycloak.adminUser ({{keycloak.adminUser}}) Password: ðŸ”– keycloak.adminPassword ({{keycloak.adminPassword}})  At your first login, change your password ! 1.4. Configure keycloak 1.4.1. Create groups To manage our groups, go to Master realm \u0026gt; Manage \u0026gt; Groups section \u0026gt; New button.\nCreate all the groups you need. For the setup I describe, I have to create groups watchdogs and admins.\n1.4.2. Create users To manage our users, go to Master realm \u0026gt; Manage \u0026gt; Users section \u0026gt; Add user button. Fill the form with at least a username. I usually check email verified just in case because I trust the emails I put in.\nIf you want to allow this user to log in via username/password, set its password in the Credentials tab. If the Temporary option is set, the user will have to change its password on first login.\nAssign your new user to the groups you want by going to the Groups tab.\nGo on and create all the users you need, and assign relevant groups to them.\n1.4.3. Setup clients \u0026amp; scopes  â„¹ï¸ Info: If you\u0026rsquo;re not familiar with oauth2, clients are roughly applications that are allowed to authenticate users in your authentication system (keycloak). Clients can ask for grants of scopes, that are user informations they want to access to.\n Back in the dashboards setup, we\u0026rsquo;ll protect our apps Test app, kibana, kube dashboard \u0026amp; traefik using \u0026nbsp;gogatekeeper/gatekeeper.\n1.4.3.1. Create scopes Since our apps Test app, kibana, kube dashboard \u0026amp; traefik will all rely on the same information (group), we can get our setup easier by adding a shared Client scope. Go to Master realm \u0026gt; Configure \u0026gt; Client Scopes section. Here, check if there is a client scope named groups (I didn\u0026rsquo;t, but just in case).\nIf not, create one named groups, then save it. If there is one, check if the following config matches or create a new one.\nAfter, go to the new Mappers tab \u0026amp; create a new mapper. This mapper will put the groups we set up earlier in our user\u0026rsquo;s token. Set its Mapper Type to Group Membership, \u0026amp; the Token Claim Name to groups. This is the name of the token\u0026rsquo;s field we\u0026rsquo;ll use later in our authentication proxy.\nOnce done, you may add it to the default client scopes. Go to Master realm \u0026gt; Configure \u0026gt; Client Scopes section \u0026gt; Default Client Scopes tab, \u0026amp; add our groups to the assigned column. You should probably not set it as Optional.\n1.4.3.2. Create client We are going to create our authentication for the app ðŸ”– nginxTest.clientId ({{nginxTest.clientId}}) (for instance, nginx-test), with url https://test.{{cluster.baseHostName}}). Fill the Client ID with {{nginxTest.clientId}}.\nSince this application will be logged in through a proxy (we\u0026rsquo;ll get back to that part next in the dashboard setups), set its Access Type to confidential.\nSet the valid redirect URIs to https://test.{{cluster.baseHostName}}/oauth/callback (the /oauth/callback part is required by the gatekeeper).\nThen, we need to add an audience. This field is required by gatekeeper to check our key. Go to the Mappers tab, \u0026amp; create a new mapper. Name it, for instance, audience, of Mapper Type Audience, \u0026amp; set the Included Client Audience to our ðŸ”– nginxTest.clientId ({{nginxTest.clientId}}). Save it.\nAfter this, check our client scopes by going to the Client Scopes tab. In the Setup sub-tab, make sure that our groups client scope is assigned. Then, you can test our setup ! Go to the Evaluate sub-tab, pick the User you want to check, click Evaluate then go to Generated Access Token sub-sub-tab. It should contain a key groups with all our user\u0026rsquo;s groups, prefixed with a /, and a key aud with at least our ðŸ”– {{nginxTest.clientId}}. If its okay, we are good to go !\nAnd, finally, go to the Credentials tab \u0026amp; get the secret. It will be used as ðŸ”– {{nginxTest.clientSecret}}.\nOther clients protected by our gatekeeper will be very similar.\n2. Setup our test application protected by authentication   References   https://www.openshift.com/blog/adding-authentication-to-your-kubernetes-web-applications-with-keycloak    We\u0026rsquo;ll start with the simplest of our cases: the nginx-test app. This will allow us to get used to keycloak for our authorization mechanism. We\u0026rsquo;ll proxy a simple nginx default instance behind our authentication proxy.\nLook at the \u0026nbsp;11-TestNginx.yaml template.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160  apiVersion:v1kind:Namespacemetadata:name:nginx-test# Nginx itself---kind:DeploymentapiVersion:apps/v1metadata:name:nginxnamespace:nginx-testlabels:app:nginx-testcomponent:nginxspec:replicas:1selector:matchLabels:app:nginx-testcomponent:nginxtemplate:metadata:labels:app:nginx-testcomponent:nginxspec:containers:- name:nginximage:nginx---apiVersion:v1kind:Servicemetadata:name:nginxnamespace:nginx-testlabels:app:nginx-testcomponent:nginxspec:ports:- protocol:TCPname:webport:80selector:app:nginx-testcomponent:nginx# Authentication proxy---apiVersion:apps/v1kind:Deploymentmetadata:name:gatekeepernamespace:nginx-testlabels:app:nginx-testcomponent:gatekeeperspec:replicas:1selector:matchLabels:app:nginx-testcomponent:gatekeepertemplate:metadata:labels:app:nginx-testcomponent:gatekeeperspec:containers:- name:keycloak-gatekeeperimage:\u0026#34;quay.io/gogatekeeper/gatekeeper:1.2.0\u0026#34;imagePullPolicy:IfNotPresentargs:- --listen=0.0.0.0:3000- --discovery-url=https://keycloak.{{cluster.baseHostName}}/auth/realms/master- --client-id={{nginxTest.clientId}}- --client-secret={{nginxTest.clientSecret}}- --upstream-url=http://nginx.nginx-test.svc.cluster.local:80# See https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/#namespaces-and-dns- --redirection-url=https://test.{{cluster.baseHostName}}/- --skip-openid-provider-tls-verify=true- --enable-default-deny=true- --enable-logging=true- --enable-refresh-tokens=true- --enable-session-cookies=true- --encryption-key={{random32charsString}}- --secure-cookie=false- --resources=uri=/*|groups=/watchdogsports:- name:httpcontainerPort:3000protocol:TCP---apiVersion:v1kind:Servicemetadata:name:gatekeepernamespace:nginx-testlabels:app:nginx-testcomponent:gatekeeperspec:ports:- port:80targetPort:httpprotocol:TCPname:httpselector:app:nginx-testcomponent:gatekeeper# ---# apiVersion: traefik.containo.us/v1alpha1# kind: Middleware# metadata:# name: gatekeeper# namespace: nginx-test# spec:# forwardAuth:# address: http://gatekeeper.nginx-test.svc.cluster.local:80# authResponseHeaders: # - \u0026#34;X-Forwarded-User\u0026#34;# trustForwardHeader: true---apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingressnamespace:nginx-testspec:entryPoints:- websecureroutes:- match:Host(`test.{{cluster.baseHostName}}`)kind:Ruleservices:- name:gatekeepernamespace:nginx-testkind:Serviceport:80# middlewares:# - name: gatekeeper# namespace: nginx-testtls:certResolver:myresolver   As you may notice in this template, the option --discovery-url=https://keycloak.{{cluster.baseHostName}}/auth/realms/master tells our authentication proxy where is our OAuth2 provider. If you are using an aliased hostname, this won\u0026rsquo;t resolve, as your server can\u0026rsquo;t resolve an IP for this hostname. That\u0026rsquo;s why we edited CoreDNS configuration earlier.\nSo let\u0026rsquo;s deploy that:\n1  kubectl apply -f 11-NginxTest.yaml    Gen random 32 chars str: $(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 32 | head -n 1)\n Now, you should be able to access https://test.{{cluster.baseHostName}}/, that will redirect you to https://keycloak.{{cluster.baseHostName}}/... for authentication. Once logged in, you\u0026rsquo;ll be redirected to https://test.{{cluster.baseHostName}}/oauth/callback?... that will log you in the proxy, check your infos, \u0026amp; allow you to pass through if you matches the criterions.\nTo log out from our authentication proxy, simply reach https://test.{{cluster.baseHostName}}/oauth/logout. You may need to refresh the page without cache on the base url again ( / ) in order to re-request to log in, otherwise the nginx page you\u0026rsquo;ll see would be cached.\nWell, now we know that our stuff work ! Let\u0026rsquo;s delete this test application.\n1  kubectl delete -f 11-NginxTest.yaml   3. Persist data from Keycloak   References   chown: changing ownership of â€˜/var/lib/postgresql/dataâ€™: Operation not permitted, when running in kubernetes with mounted \u0026ldquo;/var/lib/postgres/data\u0026rdquo; volume #361    You can use a persistent datastore by setting postgresql.enabled to true. Think about setting postgresql.storageClass.\n1  helm update keycloak -f ./kubernetes/authentication/01-KeycloakChartValues.yaml codecentric/keycloak --version 9.9.2 --namespace keycloak   Initialization of PostgreSQL may take some time. Don\u0026rsquo;t hesitate to look at the logs of both of PostgreSQL and Keycloak.\nOnce done, you will need to redo all the steps above, because you\u0026rsquo;re now using a brand new real database.\n Hey, weâ€™ve done important things here ! Maybe itâ€™s time to commitâ€¦\n1 2 3 4  git add . git commit -m \u0026#34;Setup cluster\u0026#39;s authentication Following guide @ https://gerkindev.github.io/devblog/walkthroughs/kubernetes/07-authentication/\u0026#34;    ","description":"","id":7,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Web service","Security","Authentication"],"title":"Setup cluster's authentication","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/07-authentication/"},{"content":"Create the realm and the client https://github.com/zufardhiyaulhaq/engineering-notes/blob/master/notes/kubernetes-keycloak-integration.md\nhttps://github.com/zufardhiyaulhaq/engineering-notes/blob/master/notes/kubernetes-keycloak-integration.md#integrating\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  REALM_URL=\u0026#34;https://keycloak.{{cluster.baseHostName}}/auth/realms/{{apiServer.realmName}}\u0026#34; # Log in TOKEN_RESPONSE=\u0026#34;$(curl \\  -d \u0026#34;grant_type=password\u0026#34; \\  -d \u0026#34;client_id={{apiServer.clientId}}\u0026#34; \\  -d \u0026#34;client_secret={{apiServer.clientSecret}}\u0026#34; \\  -d \u0026#34;username=admin-user\u0026#34; \\  -d \u0026#34;password=admin-user\u0026#34; \\  $REALM_URL/protocol/openid-connect/token)\u0026#34; # Extract the access token ACCESS_TOKEN=\u0026#34;$(echo \u0026#34;$TOKEN_RESPONSE\u0026#34; | jq \u0026#39;.access_token\u0026#39; -r)\u0026#34; # Check token curl \\  --user \u0026#34;{{apiServer.clientId}}:{{apiServer.clientSecret}}\u0026#34; \\  -d \u0026#34;token=$ACCESS_TOKEN\u0026#34; \\  $REALM_URL/protocol/openid-connect/token/introspect -k   Set up certificates https://medium.com/@zufardhiyaulhaq/kubernetes-authentication-with-keycloak-openid-connect-part-1-14f4e778b5e9#5252\nGenerate the certificates   References   https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes Provide subjectAltName to openssl directly on the command line    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  mkdir certs cd certs # CA part (Certificate Authority) # Generate the CA (Certificate Authority) private key openssl genrsa -out ca.key 2048 # Generate the CA (Certificate Authority) certificate openssl req -new -x509 \\  -subj \u0026#34;/C={{countryCodeIso3166_1_alpha_2}}/ST={{State}}/O={{companyName}}/CN={{cluster.baseHostName}}\u0026#34; \\  -addext \u0026#34;subjectAltName = DNS:{{cluster.baseHostName}}\u0026#34; \\  -key ca.key -out ca.crt # # Import the CA (Certificate Authority) in the truststore, so that certificates signed by our authority are considered as trusted # keytool -import -file ca.crt -keystore ca.truststore -keypass PASSWORD -storepass PASSWORD # Keycloak part # Generate the keycloak\u0026#39;s private key openssl genrsa -out keycloak.key 2048 # Generate the keycloak\u0026#39;s CSR (Certificate Signing Request) openssl req -new \\  -subj \u0026#34;/C={{countryCodeIso3166_1_alpha_2}}/ST={{State}}/O={{companyName}}/CN=kube-keycloak.{{cluster.baseHostName}}\u0026#34; \\  -addext \u0026#34;subjectAltName = DNS:kube-keycloak.{{cluster.baseHostName}}\u0026#34; \\  -key keycloak.key -out keycloak.csr # Sign the CSR using our custom CA openssl x509 -req \\  -days 3650 \\  -extfile \u0026lt;(printf \u0026#34;subjectAltName=DNS:kube-keycloak.{{cluster.baseHostName}}\u0026#34;) \\  -CA ca.crt -CAkey ca.key \\  -in keycloak.csr -out keycloak.crt   Finally, inspect your keycloak\u0026rsquo;s certificate.\n1  openssl x509 -noout -text -in keycloak.crt   If all worked well, the output should be like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  Certificate: Data: Version: 3 (0x2) Serial Number: # ... Signature Algorithm: sha256WithRSAEncryption Issuer: C = {{countryCodeIso3166_1_alpha_2}}, ST = {{State}}, O = {{companyName}}, CN = {{cluster.baseHostName}} Validity Not Before: Nov 18 20:29:01 2020 GMT Not After : Nov 16 20:29:01 2030 GMT Subject: C = {{countryCodeIso3166_1_alpha_2}}, ST = {{State}}, O = {{companyName}}, CN = kube-keycloak.{{cluster.baseHostName}} Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (2048 bit) Modulus: # ... Exponent: # ... X509v3 extensions: X509v3 Subject Alternative Name: DNS:kube-keycloak.{{cluster.baseHostName}} Signature Algorithm: sha256WithRSAEncryption # ...   The important part is that your certificate contains the correct X509v3 Subject Alternative Name field. If it is missing, Go will complain to you that the certificate use obsolete Common Name.\nGo deprecated use of Common Name by default since v1.15 via this commit.\n Pass certificates to keycloak The \u0026nbsp;keycloak docker container indicates that keycloak will use certificate and private keys from /etc/x509/https/tls.{crt,key}. So, we are going to pass those via a secret mounted at the desired directory.\nFirst, create the secret\n1 2  # Create our secret that will be mounted into our pod kubectl create secret generic certs -n keycloak --from-file keycloak.crt --from-file keycloak.key   Then, update your keycloak chart values to mount this new secret.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  extraEnv:|- name: PROXY_ADDRESS_FORWARDING value: \u0026#34;true\u0026#34; - name: KEYCLOAK_USER value: {{keycloak.adminUser}} - name: KEYCLOAK_PASSWORD value: {{keycloak.adminPassword}}podLabels:app:keycloakcomponent:keycloakservice:labels:app:keycloakcomponent:keycloakhttpsPort:443# 8443 by default, but it should be reachable via the same URL from outside than inside, eg `https://keycloak.{{cluster.baseHostName}}`ingress:labels:app:keycloakcomponent:keycloaktls:- hosts:- keycloak.{{cluster.baseHostName}}- kube-keycloak.{{cluster.baseHostName}}postgresql:postgresqlPassword:keycloakpostgresqlDatabase:keycloakenabled:truepersistence:existingClaim:postgresql-dataextraVolumes:|- name: certs secret: secretName: certs items: # Map keycloak.crt =\u0026gt; tls.crt - key: keycloak.crt path: tls.crt # Map keycloak.key =\u0026gt; tls.key - key: keycloak.key path: tls.keyextraVolumeMounts:|- name: certs mountPath: \u0026#34;/etc/x509/https\u0026#34; readOnly: true   Finally, update your chart.\n1 2  # Update our release to use the certificates helm upgrade -n keycloak -f ./kubernetes/authentication/01-KeycloakChartValues.yaml keycloak codecentric/keycloak   Enable alternative routing to keycloak 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  # apiVersion: traefik.containo.us/v1alpha1# kind: Middleware# metadata:# name: internal-whitelist# namespace: keycloak# spec:# ipWhiteList:# sourceRange:# - 127.0.0.1/32# - 192.168.255.0/24# ---apiVersion:traefik.containo.us/v1alpha1kind:IngressRouteTCPmetadata:name:internal-routenamespace:keycloakspec:entryPoints:- websecureroutes:- match:HostSNI(`kube-keycloak.bar.com`)# kind: Ruleservices:- name:keycloak-http# kind: Servicenamespace:keycloakport:443# middlewares:# - name: internal-whitelist# namespace: keycloaktls:passthrough:true   1 2  # Add a new route from \u0026#34;kube-keycloak.{{cluster.baseHostName}}\u0026#34; that delegates to the TLS connection using the certs declared above kubectl apply -f ./kubernetes/authentication/03-InternalRoute.yaml   Go to https://kube-keycloak.{{cluster.baseHostName}}. It should show you a security erro SEC_ERROR_UNKNOWN_ISSUER.\nDon\u0026rsquo;t worry, this is normal since keycloak\u0026rsquo;s certificate was signed by our custom Certificate Authority (CA). For curiosity, click on View Certificate.\nThe certificate correctly shows the Subject Alt Names extension, and is signed by our custom CA.\nA last verification step: ensure that requests are correctly trusted if using our custom CA.\n1 2 3  curl --cacert ca.crt https://kube-keycloak.{{cluster.baseHostName}}/ \u0026amp;\u0026amp; echo \u0026#39;Yay! Certificates correctly installed\u0026#39; || echo \u0026#39;Erf, something isn\\\u0026#39;t right\u0026#39;   If the command above works, our certificates are valid !\nEnable OIDC in the API server Place CA files in a safe place where kubernetes will be able to get it to check keycloak\u0026rsquo;s certificate.\n1 2 3 4 5 6 7 8 9 10  # Move and changes rights mkdir /etc/kubernetes/auth-cert chmod 755 /etc/kubernetes/auth-cert mv ./* /etc/kubernetes/auth-cert chmod 644 /etc/kubernetes/auth-cert/* chmod 600 /etc/kubernetes/auth-cert/*.key chown -R root:root /etc/kubernetes/auth-cert # Remove the dir cd ../ rm -r certs   https://stackoverflow.com/questions/50007654/how-does-kube-apiserver-restart-after-editing-etc-kubernetes-manifests-kube-api\n## What did not worked   ```sh  # Edit cluster config  vim /etc/kubernetes/cluster-config.yaml  ```   Under `apiServer.extraArgs`, add:  ```yaml  oidc-issuer-url: https://kube-keycloak.{{cluster.baseHostName}}/auth/realms/{{apiServer.realmName}}  oidc-client-id: {{apiServer.clientId}}  oidc-groups-claim: user_groups  oidc-username-claim: preferred_username  oidc-groups-prefix: \"oidc:\"  oidc-username-prefix: \"oidc:\"  oidc-ca-file: /etc/kubernetes/auth-cert/ca.crt  ```    Finally, restart kubelet to apply the configuration changes   ```sh  systemctl restart kubelet.service  kubeadm upgrade apply \"$(kubectl version -o json | jq '.serverVersion.gitVersion' -r | sed 's/^v//')\"  ``` -- Then\n1  vim /etc/kubernetes/manifests/kube-apiserver.yaml   Patch it to add following fields\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  #...spec:containers:- command:- kube-apiserver# ...- --oidc-issuer-url=https://kube-keycloak.{{cluster.baseHostName}}/auth/realms/{{apiServer.realmName}}- --oidc-client-id={{apiServer.clientId}}- --oidc-groups-claim=user_groups- --oidc-username-claim=preferred_username- \u0026#34;--oidc-groups-prefix=oidc:\u0026#34;- \u0026#34;--oidc-username-prefix=oidc:\u0026#34;- --oidc-ca-file=/etc/kubernetes/auth-cert/ca.crtvolumeMounts:# ...- mountPath:/etc/kubernetes/auth-certname:etc-kubernetes-auth-certreadOnly:true# ...volumes:# ...- hostPath:path:/etc/kubernetes/auth-certtype:DirectoryOrCreatename:etc-kubernetes-auth-cert# ...  The API server should restart automatically, because it is watching this manifest file.\nIf it does not, (or you want to restart it anyway because you changed the certificates), run:\n1  kubectl -n kube-system delete pod kube-apiserver-{{cluster.masterNode.1}}    Then, create the ClusterRoleBindings for the test groups:\n1  kubectl apply -f authentication/03-ClusterRoleBindings.yaml   Use kubelogin: https://github.com/int128/kubelogin\n1  kubectl krew install oidc-login   Then, configure it:\n1 2 3 4 5 6 7  kubectl oidc-login setup \\  --oidc-issuer-url=https://kube-keycloak.{{cluster.baseHostName}}/auth/realms/{{apiServer.realmName}} \\  --oidc-client-id={{apiServer.clientId}} \\  --oidc-client-secret={{apiServer.clientSecret}} \\  --certificate-authority=/etc/kubernetes/auth-cert/ca.crt # Add the parameter below if running from an environment where browser is unavailable. Don\u0026#39;t forget to add ` \\` above # --grant-type=authcode-keyboard   The command above will output you installation instruction. Don\u0026rsquo;t pay attention to the ## 3. cluster role setup part, we are getting to it, in a more generic way.\nAnd we already did the ## 4. API server setup above. Just run the step ## 5. to set credentials for our oidc user.\nFinally, create a new context for your user (and optionally switch to this context)\n1 2 3 4 5 6  # Create the context kubectl config set-context oidc@{{cluster.name}} --cluster=\u0026#34;{{cluster.name}}\u0026#34; --user=\u0026#34;oidc\u0026#34; # Switch to the context kubectl config use-context oidc@{{cluster.name}} # Go back to the admin context kubectl config use-context kubernetes-admin@{{cluster.name}}   1 2 3 4 5 6  # Get the current context kubectl config current-context # List contexts kubectl config get-contexts # Switch to other context kubectl config use-context {{contextName}}    Usefull commands 1  kubectl -n keycloak get secret certs -o json | jq \u0026#39;.data[\u0026#34;keycloak.crt\u0026#34;]\u0026#39; -r | base64 --decode | openssl x509 -noout -text    Hey, weâ€™ve done important things here ! Maybe itâ€™s time to commitâ€¦\n1 2 3 4  git add . git commit -m \u0026#34;Administrate the cluster with authentication Following guide @ https://gerkindev.github.io/devblog/walkthroughs/kubernetes/08-kubernetes-user-management/\u0026#34;    ","description":"","id":8,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Security","Authentication"],"title":"Administrate the cluster with authentication","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/08-kubernetes-user-management/"},{"content":"Now that we have our authentication service up and running, we can protect our dashboards installed in the step \u0026nbsp;06 - Monitoring: See what is going on using our Keycloak OpenID Connect provider. Here is a diagram on how authorization will be managed:\nTraefik dashboard Kibana Kube dashboard   References   https://itnext.io/protect-kubernetes-dashboard-with-openid-connect-104b9e75e39c    Again, we are going to set up a new instance of \u0026nbsp;louketo-proxy.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74  apiVersion:apps/v1kind:Deploymentmetadata:name:gatekeepernamespace:kubernetes-dashboardlabels:app:kubernetes-dashboardcomponent:gatekeeperspec:replicas:1selector:matchLabels:app:kubernetes-dashboardcomponent:gatekeepertemplate:metadata:labels:app:kubernetes-dashboardcomponent:gatekeeperspec:containers:- name:keycloak-gatekeeperimage:\u0026#34;quay.io/gogatekeeper/gatekeeper:1.2.0\u0026#34;imagePullPolicy:IfNotPresentargs:- --listen=0.0.0.0:3000- --discovery-url=https://kube-keycloak.{{cluster.baseHostName}}/auth/realms/{{apiServer.realm}}- --client-id={{apiServer.clientId}}- --client-secret={{apiServer.clientSecret}}- --upstream-url=http://kubernetes-dashboard.kubernetes-dashboard.svc.cluster.local:80- --redirection-url=https://kube-dashboard.bar.com/- --skip-openid-provider-tls-verify=true- --enable-default-deny=true- --enable-logging=true- --enable-refresh-tokens=true- --enable-session-cookies=true- --encryption-key={{random32charsString}}- --secure-cookie=true- --resources=uri=/*ports:- name:httpcontainerPort:3000protocol:TCPlivenessProbe:httpGet:path:/oauth/healthport:3000initialDelaySeconds:3timeoutSeconds:2readinessProbe:httpGet:path:/oauth/healthport:3000initialDelaySeconds:3timeoutSeconds:2---apiVersion:v1kind:Servicemetadata:name:gatekeepernamespace:kubernetes-dashboardlabels:app:kubernetes-dashboardcomponent:gatekeeperspec:ports:- port:80targetPort:httpprotocol:TCPname:httpselector:app:kubernetes-dashboardcomponent:gatekeeper   Finally, modify your ingress route\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingressroute-dashboardnamespace:kubernetes-dashboardspec:entryPoints:- websecureroutes:- match:Host(`kube-dashboard.{{cluster.baseHostName}}`)kind:Ruleservices:- name:gatekeepernamespace:kubernetes-dashboardkind:Serviceport:80tls:certResolver:myresolver    Hey, weâ€™ve done important things here ! Maybe itâ€™s time to commitâ€¦\n1 2 3 4  git add . git commit -m \u0026#34;Protect monitoring with authentication Following guide @ https://gerkindev.github.io/devblog/walkthroughs/kubernetes/09-safe-monitoring/\u0026#34;    ","description":"","id":9,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Web service","Security","Authentication"],"title":"Protect monitoring with authentication","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/09-safe-monitoring/"},{"content":"Your setup is running, everything runs smoothly, and suddenly, â€¼ï¸ nothing is responding: your cluster is overloaded.\nWell, I hope you\u0026rsquo;ll expand your cluster capacity before it happens. It\u0026rsquo;s always really bad and stressful to do maintenance because of downtime.\nHopefully, here comes the real huge advantage of kubernetes: it is meant to scale, up, and down. So, assuming you have followed the full guide so far, let\u0026rsquo;s review together how to add some juice to our cluster âš¡.\nJoin the cluster\u0026rsquo;s VPN In the step \u0026nbsp;00 Setup the cluster\u0026rsquo;s VPN, we have set up a VPN so that each of our nodes can communicate safely with each others, on their own virtual network across the internet. This comes with the great power of being able to have servers spread all around the globe.\nFrom the OpenVPN server node So, log in to the OpenVPN master server, and run the following to generate a configuration for your brand new machine:\n1 2 3 4 5 6  # Generate a client docker run -v {{vpn.volumeName}}:/etc/openvpn --rm -it kylemanna/openvpn:2.3 easyrsa build-client-full {{newNode.name}} nopass # Set its static IP echo \u0026#34;ifconfig-push {{newNode.vpnIp}} {{vpn.serverIp}}\u0026#34; | docker run -v {{vpn.volumeName}}:/etc/openvpn -i --rm kylemanna/openvpn:2.3 tee /etc/openvpn/ccd/{{newNode.name}} # Get its config to your host docker run -v {{vpn.volumeName}}:/etc/openvpn --rm kylemanna/openvpn:2.3 ovpn_getclient {{newNode.name}} \u0026gt; {{newNode.name}}.ovpn   Then, move {{newNode.name}}.ovpn to your new node by a safe mean.\nFrom the new node Install OpenVPN:\n1 2  dnf install epel-release dnf install openvpn   Add the OpenVPN server to your /etc/hosts file (if not a real DNS name).\n1  echo \u0026#39;{{vpn.publicServerIp}}\tvpn.{{cluster.baseHostName}}\u0026#39; \u0026gt;\u0026gt; /etc/hosts    Install the OpenVPN configuration you just copied\n1 2 3 4  # Install the OpenVPN configuration install -o root -m 400 {{newNode.name}}.ovpn /etc/openvpn/client/{{newNode.name}}.conf # Enable the OpenVPN client systemctl enable --now openvpn-client@{{newNode.name}}   Finally, check if everything works as expected and you can reach both internet and your neighbor nodes\n1 2 3 4  # Check internet connection ping -c 4 8.8.8.8 # Check in-VPN connection ping -c 4 192.168.255.1   Join the cluster Since I assume you\u0026rsquo;ve initialized your cluster a while ago, and your previous cluster\u0026rsquo;s join token is expired, we are going to create a new one and use it.\nIf you\u0026rsquo;ve just created your cluster, you can check out \u0026nbsp;02 - Kickstart the cluster\nFrom any account or node with cluster admin capabilities Create a new cluster token:\n1  kubeadm token create --print-join-command     Sample output  1  kubeadm join 192.168.255.10:6443 --token gmedpt.veqzvuhcazac26gf --discovery-token-ca-cert-hash sha256:cb316693e48403ff18f840d47930f6737744d2ead362838695df3a1e1400cec1      Copy the kubeadm join ... command outputted by the command above.\nFrom the new node Run the command copied above:\n1  kubeadm join ...:6443 --token ... --discovery-token-ca-cert-hash sha256:....   If everything worked correctly, you should have an output like below:\n  Sample output  1 2 3 4 5 6 7 8 9 10 11 12 13  [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... ^[[B This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run \u0026#39;kubectl get nodes\u0026#39; on the control-plane to see this node join the cluster.      ","description":"","id":10,"section":"walkthroughs","tags":["Kubernetes","Sysadmin"],"title":"Scaling up","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/10-scaling-up/"},{"content":"When configuring firewall rules to be as strict as required, you may have some troubles understanding why something you thought was allowed is, actually, not. And to track down this kind of issues, some logs might help. Here is a small copy-pasta to enable firewalld logging.\nEnable \u0026ldquo;Access Denied\u0026rdquo;   References   How to enable firewalld logging for denied packets on Linux    firewalld can log events to rsyslog, the events journal in most linux distribution. This can be enable either at runtime (that won\u0026rsquo;t persist across service restarts) or by configuration to keep it enabled for longer periods.\n1 2 3 4 5 6 7  # Either reconfigure the runtime (cleared on next reload) sudo firewall-cmd --set-log-denied=all # Or change a config file (persistent) sudo sed -i.bak -E \u0026#39;s/#?LogDenied=off/LogDenied=all/\u0026#39; /etc/firewalld/firewalld.conf \u0026amp;\u0026amp; \\ \tsudo firewall-cmd --reload # Reload the service to enable `LogDenied` option # Then, check sudo firewall-cmd --get-log-denied   Then, we\u0026rsquo;ll put rejection logs in ðŸ”– logFileName ({{logFileName}}).\n1 2 3 4 5 6 7  cat \u0026lt;\u0026lt;EOF | sudo tee /etc/rsyslog.d/{{logFileName}}.conf :msg,contains,\u0026#34;_DROP\u0026#34; /var/log/{{logFileName}}.log :msg,contains,\u0026#34;_REJECT\u0026#34; /var/log/{{logFileName}}.log \u0026amp; stop EOF sudo systemctl restart rsyslog.service sudo tail -f /var/log/{{logFileName}}.log   Yay ! Now, your can look at /var/log/{{logFileName}}.log to see denied messages info !\n","description":"Tracking down requests denied by firewalld is an important plus to be both strict and precise about what to allow. This small copy-pasta might help you.","id":11,"section":"blog","tags":["Firewall","Security","Sysadmin","Troubleshooting"],"title":"CentOS8 Firewalld Tips","uri":"https://gerkindev.github.io/devblog/blog/centos8-firewalld-tips/"},{"content":"Kubernetes is\u0026hellip;. Quite a thing, to say the least ! ðŸ˜… Even if their conceptors did a great job at making the kubectl cli as usable as possible, it can sometimes be a pain to be productive with it, read outputs, or do repetitive tasks. That\u0026rsquo;s why I wrote this small Quality of life improvements post: to regroup some install steps you might have missed, give you some useful 3rd party tools or maybe even give you tips a step ahead.\nkubectl auto-complete   References   ðŸ“š kubectl installation manual    Autocomplete is nice, and a real time saver. It avoids typos, and it\u0026rsquo;s quite satisfying to type a complete command in 4 keystrokes and a couple of tabs correctly placed. (even if I\u0026rsquo;m always unsure when relying on my browser\u0026rsquo;s autocomplete for https://analytics.google.com ðŸ˜‘).\nBut for this one, I can only say one thing, and you have no excuses:\n \u0026nbsp;\nSo, short stories short, and depending on your shell, type in:\nzsh bash  1 2 3 4 5 6  cat \u0026lt;\u0026lt;EOF | tee -a ~/.zshrc autoload -Uz compinit compinit source \u0026lt;(kubectl completion zsh) EOF source ~/.zshrc      All the (bad) flavours come from the natural world.\n 1 2 3 4 5 6 7 8 9 10 11  # Install the bash completion main script (assuming you\u0026#39;re on a RHEL/CentOS/Fedora) dnf install bash-completion # Reload env source ~/.bashrc # Check if bash_completion is properly imported, or add it to your bashrc if ! type _init_completion; then echo \u0026#39;source /usr/share/bash-completion/bash_completion\u0026#39; \u0026gt;\u0026gt; ~/.bashrc fi # Source the completion script echo \u0026#39;source \u0026lt;(kubectl completion bash)\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc       'use strict'; var containerId = JSON.parse(\"\\\"cae94f16814cbce8\\\"\"); var containerElem = document.getElementById(containerId); var tabLinks = null; var tabContents = null; var ids = []; if (containerElem) { tabLinks = containerElem.querySelectorAll('.tab__link'); tabContents = containerElem.querySelectorAll('.tab__content'); } for (var i = 0; i 0) { tabContents[0].style.display = 'block'; }  kubecolor: prettier kubectl commands outputs with colors   References   Add ANSI colors to kubectl describe and other outputs kubecolor    1 2 3  go get -u github.com/dty1er/kubecolor/cmd/kubecolor # Make sure kubecolor is found which kubecolor   If the command above did not worked, then you may have a problem with your $GOPATH or $GOHOME environment variables. If none are set, then the package was installed in ~/go/bin. Either fix your vars or add ~/go/bin to your $PATH.\n1 2 3 4  cat \u0026lt;\u0026lt;EOF | tee -a {{profileFile}} PATH=\u0026#34;\\$PATH:\\$HOME/go/bin\u0026#34; EOF source ~/.zshrc    Finally, you could either use kubecolor instead of kubectl, or alias kubectl as kubecolor with the following code sample:\n1 2 3 4 5 6 7 8 9 10 11  cat \u0026lt;\u0026lt;EOF | tee -a {{profileFile}} # Backup original \u0026#34;kubectl\u0026#34; command path. Supports subsequent imports of the file. export KUBECTL_ORIG_PATH=\u0026#34;\\${KUBECTL_ORIG_PATH:-\u0026#34;\\$(which kubectl)\u0026#34;}\u0026#34; # Alias the real \u0026#34;kubectl\u0026#34; as \u0026#34;kubectll\u0026#34; alias kubectll=\u0026#34;\\${KUBECTL_ORIG_PATH}\u0026#34; # Alias kubectl to use colors by default alias kubectl=\u0026#34;kubecolor\u0026#34; # Enable the autocompletion for the alias too (see auto-complete install above) complete -o default -F __start_kubectl kubecolor EOF source {{profileFile}}   I noticed some little things does not work well with kubecolor. That\u0026rsquo;s why the script above let you use the original kubectl command through kubectll. For instance, I noticed that some commands prompting user input (so using stdin), such as kubectl login, don\u0026rsquo;t work.\nSo, if you try a command that seems to not work as expected, or stay stuck, fall back to kubectll.\n helm: a kubernetes stack template repository  \u0026nbsp;Helm is a convinient way to use or share configurable kubernetes stacks. For example, it may allow to install easily a front-end, with its API and a database, in a single template, in which you can inject your specific configuration (PVC, ports, environment, etc\u0026hellip;).\nTo install helm, run the following command:\n1 2  # See https://helm.sh/docs/intro/install/ curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash   krew: a kubectl plugins manager  \u0026nbsp;krew is a nice small plugin manager for your kubectl command. At the time of writing, it has \u0026nbsp;129 plugins available, including some pretty convinient to restart pods, login using OpenId, check the state of your cluster, and more.\nTo install krew, run the following: (taken from \u0026nbsp;the docs)\n Think about replacing {{profileFile}} with your actual zsh or bash profile\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Install krew ( set -x; cd \u0026#34;$(mktemp -d)\u0026#34; \u0026amp;\u0026amp; curl -fsSLO \u0026#34;https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz\u0026#34; \u0026amp;\u0026amp; tar zxvf krew.tar.gz \u0026amp;\u0026amp; KREW=./krew-\u0026#34;$(uname | tr \u0026#39;[:upper:]\u0026#39; \u0026#39;[:lower:]\u0026#39;)_$(uname -m | sed -e \u0026#39;s/x86_64/amd64/\u0026#39; -e \u0026#39;s/arm.*$/arm/\u0026#39;)\u0026#34; \u0026amp;\u0026amp; \u0026#34;$KREW\u0026#34; install krew ) # Add it to your $PATH and reload config cat \u0026lt;\u0026lt;EOF | tee -a {{profileFile}} export PATH=\u0026#34;\\${KREW_ROOT:-\\$HOME/.krew}/bin:\\$PATH\u0026#34; EOF source {{profileFile}} # Check krew works kubectl krew   One ring to rule them all For this one, I plead guilty of not using it enough, but it contains a lot of useful knowledge and possible solutions of most of your problems.\nYou guessed it, I\u0026rsquo;m talking about documentation. (because it would be an insult to tell you that StackOverflow is a thing.)\nRead it carefully. Take time to understand it and its underlying concepts. Don\u0026rsquo;t use tools you don\u0026rsquo;t know how they work. Because when things breaks, your knowledge of what and how it broke will help you to solve the problem quickly and without damages. So, read the documentation of your containers, your helm charts, your kubernetes network layer, and, of course, kubernetes and docker themselves.\n","description":"","id":12,"section":"blog","tags":["Kubernetes"],"title":"Quality Of Life improvements to kubernetes","uri":"https://gerkindev.github.io/devblog/blog/kubernetes-qol/"},{"content":"Hello world ! ","description":"","id":13,"section":"","tags":null,"title":"About","uri":"https://gerkindev.github.io/devblog/about/"}]