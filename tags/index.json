[{"content":"  References   How To Run OpenVPN in a Docker Container on Ubuntu 14.04 Running Docker Containers with Systemd    Because we are installing our cluster bare metal on servers exposed on the Internet, we\u0026rsquo;ll need a way to secure all of our network traffic around the critical parts of kubernetes. To do so, we\u0026rsquo;ll use OpenVPN to create a virtual secured network where all of our nodes will work. Moreover, this network will also contains MetalLB services when   configuring our bare metal load balancer  .\nYou may need to edit your /etc/hosts files to associate vpn.{{cluster.baseHostName}} to your future OpenVPN server on each of the devices that will join the cluster (if vpn.{{cluster.baseHostName}} is not a real DNS name).\n1  echo \u0026#39;{{vpn.publicServerIp}}\tvpn.{{cluster.baseHostName}}\u0026#39; \u0026gt;\u0026gt; /etc/hosts    See the  docs of kylemanna/openvpn  (our OpenVPN server).\nOpenVPN server initial setup On the OpenVPN server, create a volume for OpenVPN so that it can store files, and generate configuration.\n1 2 3 4 5 6 7 8  # Create the volume docker volume create --name {{vpn.volumeName}} # Init OpenVPN configuration \u0026amp; certificates docker run -v {{vpn.volumeName}}:/etc/openvpn --rm kylemanna/openvpn:2.3 ovpn_genconfig -Nd -u udp://vpn.{{cluster.baseHostName}}:1194 # Generate the EasyRSA PKI certificate authority. This will prompt a password, that you should keep safe. It will be used to generate new client certificates \u0026amp; configs docker run -v {{vpn.volumeName}}:/etc/openvpn --rm -it kylemanna/openvpn:2.3 ovpn_initpki # Start the server docker run -v {{vpn.volumeName}}:/etc/openvpn -d -p 1194:1194/udp --cap-add=NET_ADMIN kylemanna/openvpn:2.3    Note on the -Nd flags of the line 4: see  this RTFM page  for split tunnel (partial traffic tunnel)\n Once the last command is executed, your OpenVPN server should start. If it started properly, just kill it. We will set it up as a systemd service for our host.\nMake a systemd service for OpenVPN through docker If you\u0026rsquo;re not using systemd, see how to use init.d, and skip this section. Install the  📋 systemd/kubernetes-vpn.service  template into /usr/lib/systemd/system, then enable this service. It will run our OpenVPN server container.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  [Unit] Description=OpenVPN server through Docker After=syslog.target network-online.target docker.service Wants=network-online.target Requires=docker.service Documentation=man:openvpn(8) Documentation=https://community.openvpn.net/openvpn/wiki/Openvpn24ManPage Documentation=https://community.openvpn.net/openvpn/wiki/HOWTO Documentation=https://github.com/kylemanna/docker-openvpn [Service] ExecStop=-/usr/bin/docker stop %n ExecStopPost=-/usr/bin/docker rm %n ExecStartPre=/usr/bin/docker pull kylemanna/openvpn:2.3 ExecStart=/usr/bin/docker run --name %n -v {{vpn.volumeName}}:/etc/openvpn --rm -p 1194:{{vpn.port}}/udp --cap-add=NET_ADMIN kylemanna/openvpn:2.3 TimeoutStartSec=30 TimeoutStopSec=15 Restart=always RestartSec=10s Type=simple [Install] WantedBy=multi-user.target    1 2 3 4 5  mv ./systemd/kubernetes-vpn.service /usr/lib/systemd/system # Reload available services to take into account our new `kubernetes-vpn` systemctl daemon-reload # Start \u0026amp; auto start it systemctl enable --now kubernetes-vpn.service   You can check our docker container with docker container inspect kubernetes-vpn.service \u0026amp; get our OpenVPN logs with journalctl -u kubernetes-vpn.service.\nNow, get the value of the variable 🔖 vpn.serverIp ({{vpn.serverIp}}) with this command:\n1 2 3 4 5 6  # Show interface informations docker exec -it kubernetes-vpn.service ifconfig tun0 # Or, fancy buggy variant to show only interface IP docker exec -it kubernetes-vpn.service ifconfig tun0 `# Get the \u0026#34;tun0\u0026#34; interface infos` \\ | grep \u0026#39;inet addr\u0026#39; `# Get only IPv4 related line` \\ | cut -d: -f2 | awk \u0026#39;{print $1}\u0026#39; `# Get only the IP`    See  Static IP Addresses documentation for docker-openvpn \n Setup clients This section is meant to be repeated for each of your cluster\u0026rsquo;s nodes. For every node, replace the 🔖 node.ip ({{node.ip}}) \u0026amp; 🔖 node.name ({{node.name}}) variables.\nImportant: node.ip ({{node.ip}}) must be on the same network than vpn.serverIp ({{vpn.serverIp}}) (usually, 192.168.255.XXX) Generate credentials For each of our clients, we\u0026rsquo;ll need to generate credentials so that they can connect to the vpn server. Those clients may use static IPs. The master(s) must have a static IP since it must be reachable via a constant address for kubectl.\nOn your OpenVPN\u0026lsquo;server host:\n1 2 3 4 5 6  # Generate a client docker run -v {{vpn.volumeName}}:/etc/openvpn --rm -it kylemanna/openvpn:2.3 easyrsa build-client-full {{node.name}} nopass # Set its static IP echo \u0026#34;ifconfig-push {{node.ip}} {{vpn.serverIp}}\u0026#34; | docker run -v {{vpn.volumeName}}:/etc/openvpn -i --rm kylemanna/openvpn:2.3 tee /etc/openvpn/ccd/{{node.name}} # Get its config to your host docker run -v {{vpn.volumeName}}:/etc/openvpn --rm kylemanna/openvpn:2.3 ovpn_getclient {{node.name}} \u0026gt; {{node.name}}.ovpn   Move this {{node.name}}.ovpn file to the {{node.name}} node by a safe mean. Those files are super critical, so be very careful to not put it anywhere usafe.\nNext operations have to be run on clients.\nInstall OpenVPN client   References   https://www.vpsserver.com/community/tutorials/4035/install-openvpn-on-centos-8/    1 2 3  dnf install epel-release git dnf update dnf install openvpn   Install certificates Install the certificate previously copied with:\n1 2 3 4  # Install the OpenVPN configuration install -o root -m 400 {{node.name}}.ovpn /etc/openvpn/client/{{node.name}}.conf # Enable the OpenVPN client systemctl enable --now openvpn-client@{{node.name}}   Repeat those steps for each of our nodes, then make sure that you can reach each of your nodes from each other and you can still access the Internet.\n1 2 3 4 5  # Check internet connection by pinging Google ping -c 4 8.8.8.8 # Check in-VPN connection ping -c 4 {{vpn.serverIp}} # Eventually, check connection to other nodes   If you\u0026rsquo;re having troubles pinging 8.8.8.8 or another\u0026rsquo;s node IP, please refer to  this troubleshooting section \nYou should be good to go 🔥\nTroubleshoot   References   https://stackoverflow.com/a/63624477/4839162    No internet connection on nodes, or no connection between nodes   References   https://github.com/kylemanna/docker-openvpn#openvpn-details https://github.com/kylemanna/docker-openvpn/issues/381#issuecomment-386269991 https://github.com/kylemanna/docker-openvpn/issues/381#issuecomment-616009737    Check in case-by-case. I had to add a route push in my server configuration to make it work. See https://openvpn.net/community-resources/how-to/#expanding-the-scope-of-the-vpn-to-include-additional-machines-on-either-the-client-or-server-subnet\n1 2  docker exec -it kubernetes-vpn.service bash -c \u0026#34;echo \u0026#39;push \\\u0026#34;route 192.168.255.0 255.255.255.0\\\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/openvpn/openvpn.conf\u0026#34; systemctl restart kubernetes-vpn.service   In another setup, I had to comment out push \u0026quot;block-outside-dns\u0026quot; from the server config file (see  this comment ).\nUsefull commands demo Flush all routes 1 2  sudo iptables -t filter -F sudo iptables -t filter -X   Remove a client I bet there\u0026rsquo;s a better way to do this, but I noted this for myself.\n1 2 3 4 5  # Remove your node from this file vim /var/lib/containers/storage/volumes/{{vpn.volumeName}}/_data/pki/index.txt docker exec -it kubernetes-vpn.service rm /etc/openvpn/pki/issued/kube-master.crt docker exec -it kubernetes-vpn.service rm /etc/openvpn/pki/private/kube-master.key docker exec -it kubernetes-vpn.service rm /etc/openvpn/pki/reqs/kube-master.req   Regenerate client configs \u0026amp; copy them 1 2 3 4 5 6 7 8 9 10 11 12  OVPN_DATA=ovpn-data-cluster clients=kube-master-1 kube-worker-1 docker run -v {{vpn.volumeName}}:/etc/openvpn --rm kylemanna/openvpn:2.3 ovpn_genconfig -u udp://vpn.bar.com:1194 docker run -v {{vpn.volumeName}}:/etc/openvpn --rm -it kylemanna/openvpn:2.3 ovpn_initpki sudo systemctl restart kubernetes-vpn for client in $clients; do docker run -v {{vpn.volumeName}}:/etc/openvpn --rm -it kylemanna/openvpn:2.3 easyrsa build-client-full $client nopass docker run -v {{vpn.volumeName}}:/etc/openvpn --rm kylemanna/openvpn:2.3 ovpn_getclient $client \u0026gt; $client.ovpn done sudo install -o root -m 400 $(hostname).ovpn /etc/openvpn/client/$(hostname).conf sudo systemctl restart openvpn-client@$(hostname) scp kube-worker-1.ovpn gerkin@192.168.1.26:~   ","description":"","id":0,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Networking"],"title":"Setup the cluster's VPN","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/00-vpn/"},{"content":"  References   https://docs.kublr.com/articles/kubernetes-log-audit/ https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/ https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#log-collector-examples     Note : Even if this part is not required, you should not ignore it on dev environment and should really really REALLY not skip it for production. In fact, it can contain useful debug informations and security traces to see what is going on in your kubernetes cluster, and even on your whole server(s).\n This tutorial will guide you to setup audit log policy, catch logs with Fluentd, cast them to elasticsearch \u0026amp; show them using Kibana.\nFirst, choose an audit log dir name on the host 🔖 audit.sourceLogDir ({{audit.sourceLogDir}}). This is the directory where kubernetes will write its audit logs, and should be in /var/log. Then, choose an audit log file 🔖 audit.sourceLogFile ({{audit.sourceLogFile}}) in 🔖 audit.sourceLogDir ({{audit.sourceLogDir}}). The final audit logs path is then {{audit.sourceLogDir}}/{{audit.sourceLogFile}}\nFluentD will parse those audit logs, and split them by tags for easier sorting of logs. It will then write those zones in 🔖 audit.destLogDir ({{audit.destLogDir}})\nIn order to pipe audit log messages to Elasticsearch, we need to install fluentd on the kubernetes master host.\nInstall fluentd (on the kubernetes master host) https://docs.fluentd.org/installation/before-install\nInstall Chrony https://www.tecmint.com/install-ntp-in-rhel-8/\nStart by installing Chrony for accurate timestamps\n1 2  dnf install chrony systemctl enable --now chronyd   You should be good to go.\nConfigure other settings   References   https://superuser.com/questions/740000/modify-and-apply-limits-conf-without-reboot    Check the file descriptors limit:\n1 2 3 4  ulimit -n # » 1024 ulimit -Hn # » 262144   If it is low (like 1024), you need to increase it, by opening your system\u0026rsquo;s limits. So, open your limits.conf file:\n1  vim /etc/security/limits.conf   Set the following configurations:\nroot soft nofile 65536 root hard nofile 65536 * soft nofile 65536 * hard nofile 65536 Then reboot \u0026amp; recheck.\n1 2  ulimit -n # should be 65536 ulimit -Hn # should be at least 65536   If the environment is expected to have a high load, follow  this section of the guide \nInstall FluentD \u0026amp; plugins https://docs.fluentd.org/installation/install-by-rpm\nAdd the td-agent repository \u0026amp; install it\n1 2  curl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent3.sh | sh systemctl enable --now td-agent.service   Check if it works by posting a sample log\n1 2  curl -X POST -d \u0026#39;json={\u0026#34;json\u0026#34;:\u0026#34;message\u0026#34;}\u0026#39; http://localhost:8888/debug.test cat /var/log/td-agent/td-agent.log # should end with our test message above   Install required plugins with the following command:\n1  td-agent-gem install fluent-plugin-forest fluent-plugin-rewrite-tag-filter   If having errors here, see the Troubleshoot section at the end.\nConfigure Fluentd   References   https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#log-collector-examples    https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#log-collector-examples\nInstall the  📋 td-agent/kube.conf  template template into /etc/td-agent/, include it in your master configuration, and create the log dirs.\n# From https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#log-collector-examples # fluentd conf runs in the same host with kube-apiserver \u0026lt;source\u0026gt; @type tail # audit log path of kube-apiserver path {{audit.sourceLogDir}}/{{audit.sourceLogFile}} pos_file {{audit.sourceLogDir}}/{{audit.sourceLogFile}}.pos format json time_key time time_format %Y-%m-%dT%H:%M:%S.%N%z tag audit \u0026lt;/source\u0026gt; \u0026lt;filter audit\u0026gt; #https://github.com/fluent/fluent-plugin-rewrite-tag-filter/issues/13 @type record_transformer enable_ruby \u0026lt;record\u0026gt; namespace ${record[\u0026#34;objectRef\u0026#34;].nil? ? \u0026#34;none\u0026#34;:(record[\u0026#34;objectRef\u0026#34;][\u0026#34;namespace\u0026#34;].nil? ? \u0026#34;none\u0026#34;:record[\u0026#34;objectRef\u0026#34;][\u0026#34;namespace\u0026#34;])} \u0026lt;/record\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;match audit\u0026gt; # route audit according to namespace element in context @type rewrite_tag_filter \u0026lt;rule\u0026gt; key namespace pattern /^(.+)/ tag ${tag}.$1 \u0026lt;/rule\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;filter audit.**\u0026gt; @type record_transformer remove_keys namespace \u0026lt;/filter\u0026gt; \u0026lt;match audit.**\u0026gt; @type forest subtype file remove_prefix audit \u0026lt;template\u0026gt; time_slice_format %Y%m%d%H compress gz path {{audit.destLogDir}}/audit-${tag}.*.log format json include_time_key true \u0026lt;/template\u0026gt; \u0026lt;/match\u0026gt;  1 2 3 4 5 6 7 8 9 10  mv ./td-agent/kube.conf /etc/td-agent/td-agent.conf # Include kubernetes configuration it in configuration echo \u0026#34;@include \u0026#39;./kube.conf\u0026#39;\u0026#34; \u0026gt;\u0026gt; /etc/td-agent/td-agent.conf # Create the log dir that will be mounted into the API server mkdir -p {{audit.destLogDir}} # If required, allow td-agent to read/write in it chown -R root:td-agent {{audit.destLogDir}} chmod -R g+w {{audit.destLogDir}} # Restart the agent systemctl restart td-agent.service   Setup the audit log   References   https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy https://medium.com/@noqcks/kubernetes-audit-logging-introduction-464a34a53f6c    See the  example audit log policy  \u0026amp; the  template audit log file .\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71  # From https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/audit/audit-policy.yaml# See https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy for more infoapiVersion:audit.k8s.io/v1# This is required.kind:Policy# Don\u0026#39;t generate audit events for all requests in RequestReceived stage.omitStages:- \u0026#34;RequestReceived\u0026#34;rules:# Log pod changes at RequestResponse level- level:RequestResponseresources:- group:\u0026#34;\u0026#34;# Resource \u0026#34;pods\u0026#34; doesn\u0026#39;t match requests to any subresource of pods,# which is consistent with the RBAC policy.resources:[\u0026#34;pods\u0026#34;]# Log \u0026#34;pods/log\u0026#34;, \u0026#34;pods/status\u0026#34; at Metadata level- level:Metadataresources:- group:\u0026#34;\u0026#34;resources:[\u0026#34;pods/log\u0026#34;,\u0026#34;pods/status\u0026#34;]# Don\u0026#39;t log requests to a configmap called \u0026#34;controller-leader\u0026#34;- level:Noneresources:- group:\u0026#34;\u0026#34;resources:[\u0026#34;configmaps\u0026#34;]resourceNames:[\u0026#34;controller-leader\u0026#34;]# Don\u0026#39;t log watch requests by the \u0026#34;system:kube-proxy\u0026#34; on endpoints or services- level:Noneusers:[\u0026#34;system:kube-proxy\u0026#34;]verbs:[\u0026#34;watch\u0026#34;]resources:- group:\u0026#34;\u0026#34;# core API groupresources:[\u0026#34;endpoints\u0026#34;,\u0026#34;services\u0026#34;]# Don\u0026#39;t log authenticated requests to certain non-resource URL paths.- level:NoneuserGroups:[\u0026#34;system:authenticated\u0026#34;]nonResourceURLs:- \u0026#34;/api*\u0026#34;# Wildcard matching.- \u0026#34;/version\u0026#34;# Log the request body of configmap changes in kube-system.- level:Requestresources:- group:\u0026#34;\u0026#34;# core API groupresources:[\u0026#34;configmaps\u0026#34;]# This rule only applies to resources in the \u0026#34;kube-system\u0026#34; namespace.# The empty string \u0026#34;\u0026#34; can be used to select non-namespaced resources.namespaces:[\u0026#34;kube-system\u0026#34;]# Log configmap and secret changes in all other namespaces at the Metadata level.- level:Metadataresources:- group:\u0026#34;\u0026#34;# core API groupresources:[\u0026#34;secrets\u0026#34;,\u0026#34;configmaps\u0026#34;]# Log all other resources in core and extensions at the Request level.- level:Requestresources:- group:\u0026#34;\u0026#34;# core API group- group:\u0026#34;extensions\u0026#34;# Version of group should NOT be included.# A catch-all rule to log all other requests at the Metadata level.- level:Metadata# Long-running requests like watches that fall under this rule will not# generate an audit event in RequestReceived.omitStages:- \u0026#34;RequestReceived\u0026#34;    Move it in the /etc/kubernetes folder (because this is a kubernete\u0026rsquo;s configuration).\n1 2  mv ./kubernetes/audit-log-policy.yaml /etc/kubernetes/audit-log-policy.yaml chown root:root /etc/kubernetes/audit-log-policy.yaml   Troubleshoot Unable to download data from https://rubygems.org/ - timed out (https://api.rubygems.org/specs.4.8.gz) Rubygems repository seems to have issues with IPv6. Check with below commands:\n1 2  curl -v --head https://api.rubygems.org curl -6 -v --head https://api.rubygems.org   If the 1st command worked and the second hang (timeout), then you are having troubles with IPv6, and you need to temporarly disable it.\n1 2  sysctl -w net.ipv6.conf.default.disable_ipv6=1 sysctl -w net.ipv6.conf.all.disable_ipv6=1   After installing your plugin, re-enable IPv6\n1 2  sysctl -w net.ipv6.conf.default.disable_ipv6=0 sysctl -w net.ipv6.conf.all.disable_ipv6=0   ","description":"","id":1,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Monitoring","Security"],"title":"Setup the cluster's Audit Log","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/01-audit-log/"},{"content":"Configure network requirements Required by  flannel config .\n1 2 3 4 5 6  if [ \u0026#34;$(cat /proc/sys/net/bridge/bridge-nf-call-iptables)\u0026#34; = \u0026#34;1\u0026#34; ]; then echo \u0026#34;OK\u0026#34; else echo \u0026#34;Setting bridged mode\u0026#34; sysctl net.bridge.bridge-nf-call-iptables=1 fi   Create the cluster config file   References   How to set bind address by config file: https://stackoverflow.com/a/60391611    We are now going to configure the cluster. For the sake of traceability, this configuration won\u0026rsquo;t be done via CLI flags, but via  a configuration file . The path of the cluster config file will later be referenced as the 🔖 cluster.configFile ({{cluster.configFile}}), and should be inside /etc/kubernetes.\nFollowing  flannel requirements , you need to use --pod-network-cidr with address 10.244.0.0./16. This CLI option is equivalent to networking.podSubnet in our 🔖 cluster.configFile ({{cluster.configFile}}) file (see  this issue ).\nThe variable 🔖 cluster.advertiseAddress ({{cluster.advertiseAddress}}) must be set to the network address of your master node through the VPN. You can get it like so:\n1  ip -4 addr show tun0 | grep -oP \u0026#39;(?\u0026lt;=inet\\s)\\d+(\\.\\d+){3}\u0026#39;   The variables 🔖 audit.sourceLogDir ({{audit.sourceLogDir}}) \u0026amp; 🔖 audit.sourceLogFile ({{audit.sourceLogFile}}) were set in   Setup the cluster's Audit Log  \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  apiVersion:kubeadm.k8s.io/v1beta2kind:InitConfigurationlocalAPIEndpoint:advertiseAddress:{{cluster.advertiseAddress}}---apiVersion:kubeadm.k8s.io/v1beta2kind:ClusterConfigurationclusterName:{{cluster.name}}networking:podSubnet:\u0026#34;10.244.0.0/16\u0026#34;apiServer:extraArgs:audit-policy-file:/etc/kubernetes/audit-log-policy.yamlaudit-log-path:{{audit.sourceLogDir}}/{{audit.sourceLogFile}}extraVolumes:- name:audit-policyhostPath:/etc/kubernetes/audit-log-policy.yamlmountPath:/etc/kubernetes/audit-log-policy.yaml# See apiServer.extraArgs.audit-policy-filereadOnly:true- name:audit-loghostPath:{{audit.sourceLogDir}}mountPath:{{audit.sourceLogDir}}pathType:DirectoryOrCreatereadOnly:false    1 2 3  sudo mv ./kubernetes/cluster-config.yaml {{cluster.configFile}} sudo chown root:root {{cluster.configFile}} sudo chmod 600 {{cluster.configFile}}   Finally, init the cluster   References   kubeadm API resources Flannel kubernetes RTFM    Pay attention to the feedbacks of the kubeadm command. It will show warnings about misconfigurations.\n1 2 3 4 5 6 7 8 9 10 11  # Init the cluster with our cluster config file kubeadm init --config {{cluster.configFile}} # Setup kubectl mkdir -p $HOME/.kube \u0026amp;\u0026amp; cp -i /etc/kubernetes/admin.conf $HOME/.kube/config \u0026amp;\u0026amp; chown $(id -u):$(id -g) $HOME/.kube/config # Deploy flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml # If you want to run pods on the master (not recommended), run the following command: kubectl taint nodes $(hostname) node-role.kubernetes.io/master- # To undo, run the following kubectl taint nodes $(hostname) node-role.kubernetes.io/master:NoSchedule   Join workers At the end of the kubeadm init... command, a join command was issued if everything went OK. Execute this command on every workers you want in your cluster. The command is something like below:\n1 2  kubeadm join xxx.xxx.xxx.xxx:yyy --token foo.barqux123456 \\  --discovery-token-ca-cert-hash sha256:fed2136f5e41d654f6e6411d4f5e646512fd5   If lost, you can create a new one by executing following command on the control pane with:\n1  kubeadm token create --print-join-command   You can check nodes by running following command from the control pane\n1  kubectl get nodes   You may repeat this part of the process during the life of your cluster to add new nodes.\nInitialize metallb https://metallb.universe.tf/installation/\nCreate a metallb configmap, from the  kubernetes/metallb-configmap.yaml  template.  See the docs  for full reference on this config file \u0026amp; how to adapt it to your network configuration..\nThe 🔖 cluster.networkAddress ({{cluster.networkAddress}}) corresponds to the network part of your 🔖 cluster.advertiseAddress ({{cluster.advertiseAddress}}).\n1 2 3 4 5 6 7 8 9 10 11 12  apiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|address-pools: - name: default protocol: layer2 addresses: - {{cluster.networkAddress}}.100-{{cluster.networkAddress}}.250    1 2 3 4 5 6 7  # Deploy metallb kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/metallb.yaml # On first install only kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\u0026#34;$(openssl rand -base64 128)\u0026#34; # Create the configmap kubectl apply -f ./kubernetes/metallb-configmap.yaml   To check if everything works so far, start a test nginx instance:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  kubectl create namespace nginx-test kubectl --namespace nginx-test run nginx --image nginx # This may take some time to fetch the container kubectl --namespace nginx-test expose pod nginx --port 80 --type LoadBalancer nginx_ip=\u0026#34;$(kubectl --namespace nginx-test get svc nginx --output json | jq --raw-output \u0026#39;.status.loadBalancer.ingress[].ip\u0026#39;)\u0026#34; if [[ ! -z \u0026#34;$nginx_ip\u0026#34; ]]; then echo -e \u0026#34;$(tput setaf 2)Has public IP $nginx_ip. Testing connection. If nothing appears bellow, you might have a firewall configuration issue.$(tput sgr0)\u0026#34; if ! timeout 5 curl http://$nginx_ip ; then echo -e \u0026#34;$(tput setaf 1)NGinx unreachable. You might have a firewall configuration issue.$(tput sgr0)\u0026#34; fi else echo \u0026#34;No public IP\u0026#34; fi unset nginx_ip   This should return Has public IP with an IP that should be reachable from the host \u0026amp; the HTML of the default nginx page.\nCleanup the namespace afterwards\n1  kubectl delete namespace nginx-test   Hey, we\u0026rsquo;ve done important things here ! Maybe it\u0026rsquo;s time to commit\u0026hellip;\n1 2  git add . git commit -m \u0026#34;Kickstart the cluster\u0026#34;     Troubleshoot Nginx external ip is always pending   References   https://stackoverflow.com/a/60151612\n{{ /expand }}  Check that iptables is patched correctly.\n1 2 3 4 5 6  cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system update-alternatives --set iptables /usr/sbin/iptables-legacy   Check firewall, SELinux \u0026amp; swap\n1 2  getenforce cat /proc/swaps   Make sure your nodes are ready and that the networking plugin is correctly installed.\nCluster never starts Move or remove the existing kubeadm config file (if any) in /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\nCheck firewall, getenforce \u0026amp; swap status.\nNetwork interfaces are not deleted after reseting kubeadm   References   https://blog.heptio.com/properly-resetting-your-kubeadm-bootstrapped-cluster-nodes-heptioprotip-473bd0b824aa https://stackoverflow.com/a/46438072    1 2 3  iptables -F \u0026amp;\u0026amp; iptables -t nat -F \u0026amp;\u0026amp; iptables -t mangle -F \u0026amp;\u0026amp; iptables -X ip link delete cni0 ip link delete flannel.1   Usefull commands memo  Force reinit cluster:      ( sudo kubeadm reset -f \u0026amp;\u0026amp; sudo rm -rf /etc/cni/net.d || 1 ) \u0026amp;\u0026amp; sudo kubeadm init \u0026ndash;config cluster-config.yaml\n   ","description":"","id":2,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps"],"title":"Kickstart the cluster","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/02-cluster/"},{"content":"  References   https://github.com/traefik/traefik-helm-chart/pull/157/files    Start by creating traefik required resources. You can directly use resources from the  📋 kubernetes/traefik  templates: it does not contain variables. Those are taken from  📚 traefik docs  mixed up with  this PR  for kubernetes 1.19 support and schemas.\n Please look forward for  this issue in traefik  about official v1.19 support.\n Namespace Definitions Rbac IngressController Services  1 2 3 4  apiVersion:v1kind:Namespacemetadata:name:traefik     1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891  apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:ingressroutes.traefik.containo.usspec:group:traefik.containo.usversions:- name:v1alpha1served:truestorage:trueschema:openAPIV3Schema:type:objectproperties:spec:type:objectproperties:routes:type:arrayitems:type:objectproperties:match:type:stringkind:type:stringpriority:type:integerservices:type:arrayitems:type:objectproperties:sticky:type:objectproperties:cookie:type:objectproperties:name:type:stringsecure:type:booleanhttpOnly:type:booleannamespace:type:stringkind:type:stringname:type:stringweight:type:integerresponseForwarding:type:objectproperties:flushInterval:type:stringpassHostHeader:type:booleanhealthCheck:type:objectproperties:path:type:stringhost:type:stringscheme:type:stringintervalSeconds:type:integertimeoutSeconds:type:integerheaders:type:objectstrategy:type:stringscheme:type:stringport:type:integermiddlewares:type:arrayitems:type:objectproperties:name:type:stringnamespace:type:stringrequired:- name- namespaceentryPoints:type:arrayitems:type:stringtls:type:objectproperties:secretName:type:stringoptions:type:objectproperties:name:type:stringnamespace:type:stringstore:type:objectproperties:name:type:stringnamespace:type:stringcertResolver:type:stringdomains:type:arrayitems:type:objectproperties:main:type:stringsans:type:arrayitems:type:stringnames:kind:IngressRouteplural:ingressroutessingular:ingressroutescope:Namespaced---apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:middlewares.traefik.containo.usspec:group:traefik.containo.usversions:- name:v1alpha1served:truestorage:trueschema:openAPIV3Schema:type:objectproperties:spec:type:objectproperties:addPrefix:type:objectproperties:prefix:type:stringstripPrefix:type:objectproperties:prefixes:type:arrayitems:type:stringforceSlash:type:booleanstripPrefixRegex:type:objectproperties:regex:type:arrayitems:type:stringreplacePath:type:objectproperties:path:type:stringreplacePathRegex:type:objectproperties:regex:type:stringreplacement:type:stringchain:type:objectproperties:middlewares:type:arrayitems:type:objectproperties:name:type:stringnamespace:type:stringrequired:- name- namespaceipWhiteList:type:objectproperties:sourceRange:type:arrayitems:type:stringipStrategy:type:objectproperties:depth:type:integerexcludedIPs:type:arrayitems:type:stringheaders:type:objectproperties:customRequestHeaders:type:objectcustomResponseHeaders:type:objectaccessControlAllowCredentials:type:booleanaccessControlAllowHeaders:type:arrayitems:type:stringaccessControlAllowMethods:type:arrayitems:type:stringaccessControlAllowOrigin:type:stringaccessControlAllowOriginList:type:arrayitems:type:stringaccessControlExposeHeaders:type:arrayitems:type:stringaccessControlMaxAge:type:integeraddVaryHeader:type:booleanallowedHosts:type:arrayitems:type:stringhostsProxyHeaders:type:arrayitems:type:stringsslRedirect:type:booleansslTemporaryRedirect:type:booleansslHost:type:stringsslProxyHeaders:type:objectsslForceHost:type:booleanstsSeconds:type:integerstsIncludeSubdomains:type:booleanstsPreload:type:booleanforceSTSheader:type:booleanframeDeny:type:booleancustomFrameOptionsValue:type:stringcontentTypeNosniff:type:booleanbrowserXssFilter:type:booleancustomBrowserXSSValue:type:stringcontentSecurityPolicy:type:stringpublicKey:type:stringreferrerPolicy:type:stringfeaturePolicy:type:stringisDevelopment:type:booleanerrors:type:objectproperties:status:type:arrayitems:type:stringservice:type:objectproperties:sticky:type:objectproperties:cookie:type:objectproperties:name:type:stringsecure:type:booleanhttpOnly:type:booleannamespace:type:stringkind:type:stringname:type:stringweight:type:integerresponseForwarding:type:objectproperties:flushInterval:type:stringpassHostHeader:type:booleanhealthCheck:type:objectproperties:path:type:stringhost:type:stringscheme:type:stringintervalSeconds:type:integertimeoutSeconds:type:integerheaders:type:objectstrategy:type:stringscheme:type:stringport:type:integerquery:type:stringrateLimit:type:objectproperties:average:type:integerburst:type:integersourceCriterion:type:objectproperties:ipStrategy:type:objectproperties:depth:type:integerexcludedIPs:type:arrayitems:type:stringrequestHeaderName:type:stringrequestHost:type:booleanredirectRegex:type:objectproperties:regex:type:stringreplacement:type:stringpermanent:type:booleanredirectScheme:type:objectproperties:scheme:type:stringport:type:stringpermanent:type:booleanbasicAuth:type:objectproperties:secret:type:stringrealm:type:stringremoveHeader:type:booleanheaderField:type:stringdigestAuth:type:objectproperties:secret:type:stringremoveHeader:type:booleanrealm:type:stringheaderField:type:stringforwardAuth:type:objectproperties:address:type:stringtrustForwardHeader:type:booleanauthResponseHeaders:type:arrayitems:type:stringtls:type:objectproperties:caSecret:type:stringcaOptional:type:booleancertSecret:type:stringinsecureSkipVerify:type:booleaninFlightReq:type:objectproperties:amount:type:integersourceCriterion:type:objectproperties:ipStrategy:type:objectproperties:depth:type:integerexcludedIPs:type:arrayitems:type:stringrequestHeaderName:type:stringrequestHost:type:booleanbuffering:type:objectproperties:maxRequestBodyBytes:type:integermemRequestBodyBytes:type:integermaxResponseBodyBytes:type:integermemResponseBodyBytes:type:integerretryExpression:type:stringcircuitBreaker:type:objectproperties:expression:type:stringcompress:type:objectproperties:excludedContentTypes:type:arrayitems:type:stringpassTLSClientCert:type:objectproperties:pem:type:booleaninfo:type:objectproperties:notAfter:type:booleannotBefore:type:booleansans:type:booleansubject:type:objectproperties:country:type:booleanprovince:type:booleanlocality:type:booleanorganization:type:booleancommonName:type:booleanserialNumber:type:booleandomainComponent:type:booleanissuer:type:objectproperties:country:type:booleanprovince:type:booleanlocality:type:booleanorganization:type:booleancommonName:type:booleanserialNumber:type:booleandomainComponent:type:booleanserialNumber:type:booleanretry:type:objectproperties:attempts:type:integercontentType:type:objectproperties:autoDetect:type:booleannames:kind:Middlewareplural:middlewaressingular:middlewarescope:Namespaced---apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:ingressroutetcps.traefik.containo.usspec:group:traefik.containo.usversions:- name:v1alpha1served:truestorage:trueschema:openAPIV3Schema:type:objectproperties:spec:type:objectproperties:routes:type:arrayitems:type:objectproperties:match:type:stringservices:type:arrayitems:type:objectproperties:name:type:stringnamespace:type:stringport:type:integerweight:type:integerterminationDelay:type:integerentryPoints:type:arrayitems:type:stringtls:type:objectproperties:secretName:type:stringpassthrough:type:booleanoptions:type:objectproperties:name:type:stringnamespace:type:stringstore:type:objectproperties:name:type:stringnamespace:type:stringcertResolver:type:stringdomains:type:arrayitems:type:objectproperties:main:type:stringsans:type:arrayitems:type:stringnames:kind:IngressRouteTCPplural:ingressroutetcpssingular:ingressroutetcpscope:Namespaced---apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:tlsoptions.traefik.containo.usspec:group:traefik.containo.usversions:- name:v1alpha1served:truestorage:trueschema:openAPIV3Schema:type:objectproperties:spec:type:objectproperties:minVersion:type:stringmaxVersion:type:stringcipherSuites:type:arrayitems:type:stringcurvePreferences:type:arrayitems:type:stringclientAuth:type:objectproperties:secretNames:type:arrayitems:type:stringclientAuthType:type:stringenum:- NoClientCert- RequestClientCert- VerifyClientCertIfGiven- RequireAndVerifyClientCertsniStrict:type:booleanpreferServerCipherSuites:type:booleannames:kind:TLSOptionplural:tlsoptionssingular:tlsoptionscope:Namespaced---apiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:name:traefikservices.traefik.containo.usspec:group:traefik.containo.usversions:- name:v1alpha1served:truestorage:trueschema:openAPIV3Schema:type:objectproperties:spec:type:objectproperties:weighted:type:objectproperties:services:type:arrayitems:type:objectproperties:sticky:type:objectproperties:cookie:type:objectproperties:name:type:stringsecure:type:booleanhttpOnly:type:booleannamespace:type:stringkind:type:stringname:type:stringweight:type:integerresponseForwarding:type:objectproperties:flushInterval:type:stringpassHostHeader:type:booleanhealthCheck:type:objectproperties:path:type:stringhost:type:stringscheme:type:stringintervalSeconds:type:integertimeoutSeconds:type:integerheaders:type:objectstrategy:type:stringscheme:type:stringport:type:integersticky:type:objectproperties:cookie:type:objectproperties:name:type:stringsecure:type:booleanhttpOnly:type:booleanmirroring:type:objectproperties:weight:type:integerresponseForwarding:type:objectproperties:flushInterval:type:stringpassHostHeader:type:booleanhealthCheck:type:objectproperties:path:type:stringhost:type:stringscheme:type:stringintervalSeconds:type:integertimeoutSeconds:type:integerheaders:type:objectstrategy:type:stringscheme:type:stringport:type:integersticky:type:objectproperties:cookie:type:objectproperties:name:type:stringsecure:type:booleanhttpOnly:type:booleannamespace:type:stringkind:type:stringname:type:stringmirrors:type:arrayitems:type:objectproperties:name:type:stringkind:type:stringnamespace:type:stringsticky:type:objectproperties:cookie:type:objectproperties:name:type:stringsecure:type:booleanhttpOnly:type:booleanport:type:integerscheme:type:stringstrategy:type:stringhealthCheck:type:objectproperties:path:type:stringhost:type:stringscheme:type:stringintervalSeconds:type:integertimeoutSeconds:type:integerheaders:type:objectpassHostHeader:type:booleanresponseForwarding:type:objectproperties:flushInterval:type:stringweight:type:integerpercent:type:integernames:kind:TraefikServiceplural:traefikservicessingular:traefikservicescope:Namespaced     1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67  apiVersion:v1kind:ServiceAccountmetadata:name:traefiknamespace:traefik---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:name:traefiknamespace:traefikrules:- apiGroups:- \u0026#34;\u0026#34;resources:- services- endpoints- secretsverbs:- get- list- watch- apiGroups:- extensionsresources:- ingressesverbs:- get- list- watch- apiGroups:- extensionsresources:- ingresses/statusverbs:- update- apiGroups:- traefik.containo.usresources:- middlewares- ingressroutes- traefikservices- ingressroutetcps- tlsoptionsverbs:- get- list- watch---kind:ClusterRoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:traefiknamespace:traefikroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:traefiksubjects:- kind:ServiceAccountname:traefiknamespace:traefik     1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  kind:DeploymentapiVersion:apps/v1metadata:name:traefiknamespace:traefiklabels:app:traefikcomponent:ingress-controllerspec:replicas:1selector:matchLabels:app:traefikcomponent:ingress-controllertemplate:metadata:labels:app:traefikcomponent:ingress-controllerspec:serviceAccountName:traefikcontainers:- name:traefikimage:traefik:v2.1args:- --api=false- --api.dashboard=false- --accesslog- --entrypoints.web.Address=:80- --entrypoints.websecure.Address=:443- --providers.kubernetescrd- --certificatesresolvers.myresolver.acme.tlschallenge- --certificatesresolvers.myresolver.acme.email=FILL@ME.COM- --certificatesresolvers.myresolver.acme.storage=acme.json# Please note that this is the staging Let\u0026#39;s Encrypt server.# Once you get things working, you should remove that whole line altogether.- --certificatesresolvers.myresolver.acme.caserver=https://acme-staging-v02.api.letsencrypt.org/directoryports:- name:webcontainerPort:80- name:websecurecontainerPort:443     1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  apiVersion:v1kind:Servicemetadata:name:traefiknamespace:traefiklabels:app:traefikcomponent:ingress-controllerspec:ports:- protocol:TCPname:webport:80- protocol:TCPname:websecureport:443selector:app:traefikcomponent:ingress-controllertype:LoadBalancer       'use strict'; var containerId = JSON.parse(\"\\\"85b806420fac3760\\\"\"); var containerElem = document.getElementById(containerId); var codetabLinks = null; var codetabContents = null; var ids = []; if (containerElem) { codetabLinks = containerElem.querySelectorAll('.codetab__link'); codetabContents = containerElem.querySelectorAll('.codetab__content'); } for (var i = 0; i 0) { codetabContents[0].style.display = 'block'; }  1 2 3 4 5  kubectl apply -f ./kubernetes/traefik/0-1-Namespace.yaml kubectl apply -f ./kubernetes/traefik/0-2-Definitions.yaml kubectl apply -f ./kubernetes/traefik/0-3-Rbac.yaml kubectl apply -f ./kubernetes/traefik/0-4-IngressController.yaml kubectl apply -f ./kubernetes/traefik/0-5-Services.yaml   You can now use the  📚 custom resource kind IngressRoute  to map routes using traefik.\nTo check if everything works so far, you can use a test nginx instance from  📋 kubernetes/01-TestNginx.yaml . Once deployed, you should be able to display the nginx default page by reaching https://test.{{cluster.baseHostName}} from your host.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  apiVersion:v1kind:Namespacemetadata:name:nginx-test---kind:DeploymentapiVersion:apps/v1metadata:name:nginxnamespace:nginx-testlabels:app:nginx-testcomponent:nginxspec:replicas:1selector:matchLabels:app:nginx-testcomponent:nginxtemplate:metadata:labels:app:nginx-testcomponent:nginxspec:containers:- name:nginximage:nginx---apiVersion:v1kind:Servicemetadata:name:nginxnamespace:nginx-testspec:ports:- protocol:TCPname:webport:80selector:app:nginx-testcomponent:nginx---apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingressnamespace:nginx-testspec:entryPoints:- websecureroutes:- match:Host(`test.{{cluster.baseHostName}}`)kind:Ruleservices:- name:nginxkind:Servicenamespace:nginx-testport:80tls:certResolver:myresolver    1 2  kubectl apply -f ./kubernetes/xx-TestNginx.yaml curl -kH \u0026#39;Host: test.{{cluster.baseHostName}}\u0026#39; \u0026#34;https://$(kubectl get svc traefik -n traefik -o json | jq --raw-output \u0026#39;.status.loadBalancer.ingress[].ip\u0026#39;)\u0026#34;   Yes ! Our host can access traefik, that redirects the request to nginx ! Now, let the world access it.\nBut before that, cleanup your testing mess:\n1  kubectl delete -f xx-TestNginx.yaml    Hey, we’ve done important things here ! Maybe it’s time to commit…\n1 2  git add . git commit -m \u0026#34;Setup the cluster\u0026#39;s internal router\\n\\nFollowing guide @ https://gerkindev.github.io/devblog/walkthroughs/kubernetes/03-router/\u0026#34;    ","description":"","id":3,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Networking","Web service"],"title":"Setup the cluster's internal router","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/03-router/"},{"content":"  References   📖 persistent volume https://www.youtube.com/watch?v=0swOh5C3OVM    As you may know, docker (and thus, kubernetes) does not persist anything by default. That means that everytime you restart a pod (container), it is in the exact same state as it was at its first execution, except for the mount points. Those mount points are real hard drive directories injected into your pod. Some apps we\u0026rsquo;ll setup later will require to persist data, and, more generally, when you\u0026rsquo;ll run real applications on your own, they will probably use a database or something.\n Seeing how this part is highly tied with your specific setup, you should really do this part by yourself using the references above. But in case you want a basic thing working, I\u0026rsquo;ll guide you through the setup of  CephFS .\n❗ I repeat myself, but you should really do this part by yourself. Do not use those as-is if you are using your cluster to host real applications. But just like you, I\u0026rsquo;m doing tests here and I am tired of walking around the whole internet just to experiment a few things.\n Now that I\u0026rsquo;ve warned you enough (just look above, again), let\u0026rsquo;s declare a  📖 persistent volume  !\nThe persistent volume we are about to initiate here is a NFS (Network FileSystem), that will be hosted on our master. You are totally free to host it on either one of your slave or another device outside your cluster.\nhttps://rook.io/docs/rook/v1.4/ceph-prerequisites.html\nhttps://rook.io/docs/rook/v1.4/k8s-pre-reqs.html\nhttps://rook.io/docs/rook/v1.4/ceph-quickstart.html\n1 2 3  modprobe rbd # Auto-load them at startup echo \u0026#34;rbd\u0026#34; \u0026gt; /etc/modules-load.d/cephfs.conf   https://rook.io/docs/rook/v1.5/ceph-quickstart.html#tldr\nI use version 1.5-beta.0 because it is the oldest supporting kubernetes v1.19.\n1 2 3 4 5 6  git clone --single-branch --branch v1.5.0-beta.0 https://github.com/rook/rook.git cd rook/cluster/examples/kubernetes/ceph kubectl create -f common.yaml kubectl create -f operator.yaml kubectl create -f cluster.yaml kubectl apply -f dashboard-external-http.yaml   If you made errors and want to purge rook-ceph, remove following patterns on each nodes running cephfs (usually, all the worker nodes):\n1 2 3 4  rm -r /var/lib/rook rm -r /var/lib/kubelet/plugins/rook* rm -r /var/lib/kubelet/plugins_registry/rook* wipefs /dev/sdX # Each of the used disks   See https://github.com/rook/rook/issues/4553\nDeclare the storage class and the provisionner   References   📖 Persistent Volume 📖 Dynamic Volume Provisioning 📖 Storage Classes     Hey, we’ve done important things here ! Maybe it’s time to commit…\n1 2  git add . git commit -m \u0026#34;Make things persistent\\n\\nFollowing guide @ https://gerkindev.github.io/devblog/walkthroughs/kubernetes/05-storage/\u0026#34;    ","description":"","id":4,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Storage"],"title":"Make things persistent","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/05-storage/"},{"content":"Now that you have a router installed, you have to pass requests on your server to it. This setup use a single entry point directly binding some ports on the host server.\n1. Make a static and previsible configuration As you may have noticed in the step  02. Init the cluster , the metallb configuration use only dynamic adresses. But for the reverse proxy to work, we\u0026rsquo;ll need to be sure that our traefik router has a constant IP in your VPN. For this, modify your metallb configuration using the new  📋 kubernetes/metallb-configmap.yaml  template. This new configuration declares a new address pool named frontend with a single IP in it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  apiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|address-pools: - name: default protocol: layer2 addresses: - {{vpn.netBase}}.100-{{vpn.netBase}}.250 - name: frontend protocol: layer2 addresses: - {{vpn.netBase}}.99-{{vpn.netBase}}.99    1 2  # Update the configuration kubectl apply -f ./kubernetes/1-0-metallb-configmap.yaml   2. Set the router\u0026rsquo;s IP   References   Requesting specific IPs from metallb    Once the configmap has been changed, force our traefik service to use this new address \u0026ldquo;pool\u0026rdquo;. This is done using the annotation metallb.universe.tf/address-pool. Use the new  📋 kubernetes/traefik/22-Services.yaml  template, and check that its IP is correct.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  apiVersion:v1kind:Servicemetadata:name:traefiknamespace:traefikannotations:metallb.universe.tf/address-pool:frontendlabels:app:traefikcomponent:ingress-controllerspec:ports:- protocol:TCPname:webport:80- protocol:TCPname:websecureport:443selector:app:traefikcomponent:ingress-controllertype:LoadBalancer    1 2 3 4  # Update the configuration kubectl apply -f ./kubernetes/traefik/2-1-Services.yaml # Check the IP. It should be the single one in the pool defined by `frontend` in the metallb configuration kubectl get svc -n traefik   3. Setup the bare metal proxy We\u0026rsquo;ll use nginx as our bare reverse proxy. It will simply redirect every requests on the specified ports to traefik, that was  previously installed in kubernetes . In the case of an SSL connection, it won\u0026rsquo;t be unwrapped.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  # Install nginx dnf install nginx ports=(80 443) # Fill it beforehand # Get our traefik entry point clusterEntry=\u0026#34;$(kubectl get svc traefik -n traefik -o json | jq --raw-output \u0026#39;.status.loadBalancer.ingress[].ip\u0026#39;)\u0026#34; # Create the nginx stream configuration for kubernetes mkdir /etc/nginx/streams.d portsStr=\u0026#39;\u0026#39; for port in $ports; do portsStr=\u0026#34;$portsStrlisten $port; listen [::]:$port;\u0026#34; done cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/nginx/streams.d/kubernetes-proxy.conf stream { server { $portsStr proxy_pass $clusterEntry:\\$server_port; } } EOF # Include it in the nginx config echo \u0026#39;\\ninclude /etc/nginx/streams.d/*.conf;\\n\u0026#39; \u0026gt;\u0026gt; /etc/nginx/nginx.conf # Start \u0026amp; auto-start nginx systemctl enable --now nginx.service   Now, you should be able to reach your traefik router by requesting directly your entry point server. Test this with the  📋 kubernetes/01-TestNginx.yaml  template.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62  apiVersion:v1kind:Namespacemetadata:name:nginx-test---kind:DeploymentapiVersion:apps/v1metadata:name:nginxnamespace:nginx-testlabels:app:nginx-testcomponent:nginxspec:replicas:1selector:matchLabels:app:nginx-testcomponent:nginxtemplate:metadata:labels:app:nginx-testcomponent:nginxspec:containers:- name:nginximage:nginx---apiVersion:v1kind:Servicemetadata:name:nginxnamespace:nginx-testspec:ports:- protocol:TCPname:webport:80selector:app:nginx-testcomponent:nginx---apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingressnamespace:nginx-testspec:entryPoints:- websecureroutes:- match:Host(`test.{{cluster.baseHostName}}`)kind:Ruleservices:- name:nginxkind:Servicenamespace:nginx-testport:80tls:certResolver:myresolver    1 2 3 4 5 6  # Deploy it kubectl apply -f ./kubernetes/01-TestNginx.yaml ## Here, go to your browser and check that you can in fact reach your test nginx instance ## via `test.{{cluster.baseHostName}}`. # Then cleanup your own shit. kubectl delete -f ./kubernetes/01-TestNginx.yaml    Hey, we’ve done important things here ! Maybe it’s time to commit…\n1 2  git add . git commit -m \u0026#34;Make services reachable from the world\\n\\nFollowing guide @ https://gerkindev.github.io/devblog/walkthroughs/kubernetes/04-reverse-proxy/\u0026#34;    ","description":"","id":5,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Networking","Web service"],"title":"Make services reachable from the world","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/04-reverse-proxy/"},{"content":"Well, things are getting real and are on the point to become quite complex. So we\u0026rsquo;ll setup (super unsafe) dashboards to see what is going on easily. After all, we have nothing critical for now, but we might get troubles soon. And, don\u0026rsquo;t worry, we\u0026rsquo;ll make it safe just after that.\n1. Traefik dashboard: monitoring routes The traefik dashboard will help us in the diagnostics of our ingress routes and traefik-related stuff. For this, we need to:\n update our ingress controller previously deployed to enable the dashboard and create routes to the dashboard.  Use the  📋 kubernetes/traefik/21-IngressController.yaml  and  📋 kubernetes/traefik/23-IngressRoutes.yaml  templates.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  kind:DeploymentapiVersion:apps/v1metadata:name:traefiknamespace:traefiklabels:app:traefikcomponent:ingress-controllerspec:replicas:1selector:matchLabels:app:traefikcomponent:ingress-controllertemplate:metadata:labels:app:traefikcomponent:ingress-controllerspec:serviceAccountName:traefikcontainers:- name:traefikimage:traefik:v2.1args:- --api=true- --api.dashboard=true- --accesslog- --entrypoints.web.Address=:80- --entrypoints.websecure.Address=:443- --providers.kubernetescrd- --certificatesresolvers.myresolver.acme.tlschallenge- --certificatesresolvers.myresolver.acme.email=FILL@ME.COM- --certificatesresolvers.myresolver.acme.storage=acme.json# Please note that this is the staging Let\u0026#39;s Encrypt server.# Once you get things working, you should remove that whole line altogether.- --certificatesresolvers.myresolver.acme.caserver=https://acme-staging-v02.api.letsencrypt.org/directoryports:- name:webcontainerPort:80- name:websecurecontainerPort:443    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:dashboardnamespace:traefikspec:entryPoints:- websecureroutes:- match:Host(`traefik.{{cluster.baseHostName}}`) \u0026amp;\u0026amp; PathPrefix(`/dashboard`)kind:Ruleservices:- name:dashboard@internalkind:TraefikServicemiddlewares:- name:dashboard-stripprefixnamespace:traefiktls:certResolver:myresolver---apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:apinamespace:traefikspec:entryPoints:- websecureroutes:- match:Host(`traefik.{{cluster.baseHostName}}`) \u0026amp;\u0026amp; PathPrefix(`/api`)kind:Ruleservices:- name:api@internalkind:TraefikServicetls:certResolver:myresolver---apiVersion:traefik.containo.us/v1alpha1kind:Middlewaremetadata:name:dashboard-stripprefixnamespace:traefikspec:stripPrefix:prefixes:- /dashboard- /dashboard/    1 2  kubectl apply -f ./kubernetes/traefik/21-IngressController.yaml kubectl apply -f ./kubernetes/traefik/23-IngressRoutes.yaml   Now, you should be able to reach the dashboard via  https://traefik.{{cluster.baseHostName}}/dashboard/ .\n2. Kibana: harvest data from your cluster   References   https://kubernetes.io/docs/tasks/debug-application-cluster/logging-elasticsearch-kibana/ https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/#fluentd https://www.elastic.co/kibana https://www.elastic.co/elasticsearch/      Kibana  is a super versatile tool to visualize data stored in Elasticsearch.\n  Elasticsearch  is a database particularly adapted for search engines, with  fulltext search  and  scoring  capabilities.\nTogether, they compose the perfect combo to ingest all our cluster\u0026rsquo;s logs, and do searches, visualizations, tracking, and everything you\u0026rsquo;ll need to understand what is going on in your cluster\u0026rsquo;s apps.\nElasticsearch may be quite resources-consuming, and your machines may not be optimized to make it run smoothly with the consequent flow of data its about to ingest. I strongly advise you to read some installation documentations to make things correctly.\nReading a guide don\u0026rsquo;t dispense you from RTFMing.\n 2.1. Pods logs We\u0026rsquo;ll start by getting our pods (container) logs. Deploy the following configuration files:\n 1 2 3 4  apiVersion:v1kind:Namespacemetadata:name:kibana    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  # See https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/#elastic---apiVersion:apps/v1kind:Deploymentmetadata:name:elasticsearchnamespace:kibanalabels:app:kibanacomponent:elasticsearchspec:selector:matchLabels:app:kibanacomponent:elasticsearchtemplate:metadata:labels:app:kibanacomponent:elasticsearchspec:containers:- name:elasticsearchimage:docker.elastic.co/elasticsearch/elasticsearch:6.5.4env:- name:discovery.typevalue:single-nodeports:- containerPort:9200name:httpprotocol:TCP---apiVersion:v1kind:Servicemetadata:name:elasticsearchnamespace:kibanalabels:app:kibanacomponent:elasticsearchspec:selector:app:kibanacomponent:elasticsearchports:- port:9200targetPort:9200protocol:TCPname:http    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55  # See https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/#kibana---apiVersion:apps/v1kind:Deploymentmetadata:name:kibananamespace:kibanalabels:app:kibanacomponent:kibanaspec:selector:matchLabels:app:kibanacomponent:kibanatemplate:metadata:labels:app:kibanacomponent:kibanaspec:containers:- name:kibanaimage:docker.elastic.co/kibana/kibana:6.5.4env:- name:ELASTICSEARCH_URLvalue:http://elasticsearch.kibana.svc.cluster.local:9200- name:XPACK_SECURITY_ENABLEDvalue:\u0026#34;true\u0026#34;ports:- containerPort:5601name:httpprotocol:TCP---apiVersion:v1kind:Servicemetadata:name:kibananamespace:kibanalabels:app:kibanacomponent:kibanaspec:selector:app:kibanacomponent:kibanaports:- port:80targetPort:5601protocol:TCPname:http    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97  # See https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/#fluentd---apiVersion:v1kind:ServiceAccountmetadata:name:fluentdnamespace:kube-system---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:fluentdnamespace:kube-systemrules:- apiGroups:- \u0026#34;\u0026#34;resources:- pods- namespacesverbs:- get- list- watch---kind:ClusterRoleBindingapiVersion:rbac.authorization.k8s.io/v1metadata:name:fluentdroleRef:kind:ClusterRolename:fluentdapiGroup:rbac.authorization.k8s.iosubjects:- kind:ServiceAccountname:fluentdnamespace:kube-system---apiVersion:apps/v1kind:DaemonSetmetadata:name:fluentdnamespace:kube-systemlabels:app:kibanacomponent:fluentdspec:selector:matchLabels:app:kibanacomponent:fluentdtemplate:metadata:labels:app:kibanacomponent:fluentdspec:serviceAccountName:fluentdtolerations:- key:node-role.kubernetes.io/mastereffect:NoSchedulecontainers:- name:fluentdimage:fluent/fluentd-kubernetes-daemonset:v1.10.4-debian-elasticsearch6-1.0env:- name:FLUENT_ELASTICSEARCH_HOSTvalue:elasticsearch.kibana.svc.cluster.local- name:FLUENT_ELASTICSEARCH_PORTvalue:\u0026#34;9200\u0026#34;- name:FLUENT_ELASTICSEARCH_SCHEMEvalue:\u0026#34;http\u0026#34;- name:FLUENT_UIDvalue:\u0026#34;0\u0026#34;# See https://github.com/fluent/fluentd-kubernetes-daemonset#disable-systemd-input- name:FLUENTD_SYSTEMD_CONFvalue:disablevolumeMounts:- name:varlogmountPath:/var/log- name:varlibdockercontainersmountPath:/var/lib/docker/containersreadOnly:trueterminationGracePeriodSeconds:30volumes:- name:varloghostPath:path:/var/log- name:varlibdockercontainershostPath:path:/var/lib/docker/containers    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingress-securenamespace:kibanaspec:entryPoints:- websecureroutes:- match:Host(`kibana.{{cluster.baseHostName}}`)kind:Ruleservices:- name:kibanakind:Servicenamespace:kibanaport:80tls:certResolver:myresolver    1 2 3 4 5  kubectl apply -f ./kubernetes/kibana/01-Namespace.yaml kubectl apply -f ./kubernetes/kibana/11-Elasticsearch.yaml kubectl apply -f ./kubernetes/kibana/12-Kibana.yaml kubectl apply -f ./kubernetes/kibana/13-Fluentd.yaml kubectl apply -f ./kubernetes/kibana/21-Ingress.yaml   Once applied, you should be able to reach your kibana dashboard via https://kibana.{{cluster.baseHostName}}/. Be patient, it may take a bit of time to initialize ElasticSearch and Kibana itself. Once they started up, let\u0026rsquo;s configure those !\nGo to the Discover page. Kibana should ask to create indices. Index logs with pattern logstash*.\nThen, set up the time field as @timestamp.\nFinally, go back to the Discover page. You should get at least your pods logs!\nI strongly recommend you to inspect logs carefully, to clean up as many errors as possible. Yeah, you should have done that all the way long, but looking everywhere is painful, I know. So do that now while you don\u0026rsquo;t have a bunch of things to pollute your streams.\n2.2. Audit logs For a reason I can\u0026rsquo;t explain, the default settings for audit log parsing from fluentd are incorrect. Moreover, I find the \u0026ldquo;all settings in a single file\u0026rdquo; pattern awful. So we are going to reconfigure fluentd to parse correctly our logs. Use the  📋 kubernetes/kibana/31-Fluentd.yaml  \u0026amp;  📋 kubernetes/kibana/32-FluentdConfigMap.yaml  templates. The 1st one revrite some of the configuration of fluentd to use our custom configs.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  # See https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/#fluentd---apiVersion:apps/v1kind:DaemonSetmetadata:name:fluentdnamespace:kube-systemlabels:app:kibanacomponent:fluentdspec:selector:matchLabels:app:kibanacomponent:fluentdtemplate:metadata:labels:app:kibanacomponent:fluentdspec:serviceAccountName:fluentdtolerations:- key:node-role.kubernetes.io/mastereffect:NoSchedulecontainers:- name:fluentdimage:fluent/fluentd-kubernetes-daemonset:v1.10.4-debian-elasticsearch6-1.0env:- name:FLUENT_ELASTICSEARCH_HOSTvalue:elasticsearch.kibana.svc.cluster.local- name:FLUENT_ELASTICSEARCH_PORTvalue:\u0026#34;9200\u0026#34;- name:FLUENT_ELASTICSEARCH_SCHEMEvalue:\u0026#34;http\u0026#34;- name:FLUENT_UIDvalue:\u0026#34;0\u0026#34;# See https://github.com/fluent/fluentd-kubernetes-daemonset#disable-systemd-input- name:FLUENTD_SYSTEMD_CONFvalue:disablevolumeMounts:- name:fluentd-config-kubernetes-confmountPath:/fluentd/etc/kubernetes.confsubPath:kubernetes.conf- name:fluentd-config-conf-additionalmountPath:/fluentd/etc/conf.d/- name:varlogmountPath:/var/log- name:auditlogmountPath:/var/log/kubernetes- name:varlibdockercontainersmountPath:/var/lib/docker/containersreadOnly:trueterminationGracePeriodSeconds:30volumes:- name:fluentd-config-kubernetes-confconfigMap:name:fluentd-config-kubernetes-conf- name:fluentd-config-conf-additionalconfigMap:name:fluentd-config-conf-additional- name:varloghostPath:path:/var/log- name:auditloghostPath:path:/var/log/kubernetes- name:varlibdockercontainershostPath:path:/var/lib/docker/containers    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212  # Those configmaps are taken from https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/docker-image/v1.10/debian-elasticsearch6/conf/kubernetes.confapiVersion:v1data:kubernetes.conf:|-\u0026lt;label @FLUENT_LOG\u0026gt; \u0026lt;match fluent.**\u0026gt; @type null \u0026lt;/match\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;filter kubernetes.**\u0026gt; @type kubernetes_metadata @id filter_kube_metadata kubernetes_url \u0026#34;#{ENV[\u0026#39;FLUENT_FILTER_KUBERNETES_URL\u0026#39;] || \u0026#39;https://\u0026#39; + ENV.fetch(\u0026#39;KUBERNETES_SERVICE_HOST\u0026#39;) + \u0026#39;:\u0026#39; + ENV.fetch(\u0026#39;KUBERNETES_SERVICE_PORT\u0026#39;) + \u0026#39;/api\u0026#39;}\u0026#34; verify_ssl \u0026#34;#{ENV[\u0026#39;KUBERNETES_VERIFY_SSL\u0026#39;] || true}\u0026#34; ca_file \u0026#34;#{ENV[\u0026#39;KUBERNETES_CA_FILE\u0026#39;]}\u0026#34; skip_labels \u0026#34;#{ENV[\u0026#39;FLUENT_KUBERNETES_METADATA_SKIP_LABELS\u0026#39;] || \u0026#39;false\u0026#39;}\u0026#34; skip_container_metadata \u0026#34;#{ENV[\u0026#39;FLUENT_KUBERNETES_METADATA_SKIP_CONTAINER_METADATA\u0026#39;] || \u0026#39;false\u0026#39;}\u0026#34; skip_master_url \u0026#34;#{ENV[\u0026#39;FLUENT_KUBERNETES_METADATA_SKIP_MASTER_URL\u0026#39;] || \u0026#39;false\u0026#39;}\u0026#34; skip_namespace_metadata \u0026#34;#{ENV[\u0026#39;FLUENT_KUBERNETES_METADATA_SKIP_NAMESPACE_METADATA\u0026#39;] || \u0026#39;false\u0026#39;}\u0026#34; \u0026lt;/filter\u0026gt;kind:ConfigMapmetadata:name:fluentd-config-kubernetes-confnamespace:kube-system---apiVersion:v1data:container-logs.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_container_logs path /var/log/containers/*.log pos_file /var/log/fluentd-containers.log.pos tag \u0026#34;#{ENV[\u0026#39;FLUENT_CONTAINER_TAIL_TAG\u0026#39;] || \u0026#39;kubernetes.*\u0026#39;}\u0026#34; exclude_path \u0026#34;#{ENV[\u0026#39;FLUENT_CONTAINER_TAIL_EXCLUDE_PATH\u0026#39;] || use_default}\u0026#34; read_from_head true \u0026lt;parse\u0026gt; @type \u0026#34;#{ENV[\u0026#39;FLUENT_CONTAINER_TAIL_PARSER_TYPE\u0026#39;] || \u0026#39;json\u0026#39;}\u0026#34; time_format %Y-%m-%dT%H:%M:%S.%NZ \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;salt.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_minion path /var/log/salt/minion pos_file /var/log/fluentd-salt.pos tag salt \u0026lt;parse\u0026gt; @type regexp expression /^(?\u0026lt;time\u0026gt;[^ ]* [^ ,]*)[^\\[]*\\{{^\\}}*\\]\\[(?\u0026lt;severity\u0026gt;[^ \\]]*) *\\] (?\u0026lt;message\u0026gt;.*)$/ time_format %Y-%m-%d %H:%M:%S \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;startupscript.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_startupscript path /var/log/startupscript.log pos_file /var/log/fluentd-startupscript.log.pos tag startupscript \u0026lt;parse\u0026gt; @type syslog \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;docker.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_docker path /var/log/docker.log pos_file /var/log/fluentd-docker.log.pos tag docker \u0026lt;parse\u0026gt; @type regexp expression /^time=\u0026#34;(?\u0026lt;time\u0026gt;[^)]*)\u0026#34; level=(?\u0026lt;severity\u0026gt;[^ ]*) msg=\u0026#34;(?\u0026lt;message\u0026gt;[^\u0026#34;]*)\u0026#34;( err=\u0026#34;(?\u0026lt;error\u0026gt;[^\u0026#34;]*)\u0026#34;)?( statusCode=($\u0026lt;status_code\u0026gt;\\d+))?/ \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;etcd.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_etcd path /var/log/etcd.log pos_file /var/log/fluentd-etcd.log.pos tag etcd \u0026lt;parse\u0026gt; @type none \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;kubelet.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_kubelet multiline_flush_interval 5s path /var/log/kubelet.log pos_file /var/log/fluentd-kubelet.log.pos tag kubelet \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;kube-proxy.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_kube_proxy multiline_flush_interval 5s path /var/log/kube-proxy.log pos_file /var/log/fluentd-kube-proxy.log.pos tag kube-proxy \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;kube-apiserver.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_kube_apiserver multiline_flush_interval 5s path /var/log/kube-apiserver.log pos_file /var/log/fluentd-kube-apiserver.log.pos tag kube-apiserver \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;kube-controller-manager.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_kube_controller_manager multiline_flush_interval 5s path /var/log/kube-controller-manager.log pos_file /var/log/fluentd-kube-controller-manager.log.pos tag kube-controller-manager \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;kube-scheduler.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_kube_scheduler multiline_flush_interval 5s path /var/log/kube-scheduler.log pos_file /var/log/fluentd-kube-scheduler.log.pos tag kube-scheduler \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;rescheduler.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_rescheduler multiline_flush_interval 5s path /var/log/rescheduler.log pos_file /var/log/fluentd-rescheduler.log.pos tag rescheduler \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;glbc.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_glbc multiline_flush_interval 5s path /var/log/glbc.log pos_file /var/log/fluentd-glbc.log.pos tag glbc \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;autoscaler.conf:|-\u0026lt;source\u0026gt; @type tail @id in_tail_cluster_autoscaler multiline_flush_interval 5s path /var/log/cluster-autoscaler.log pos_file /var/log/fluentd-cluster-autoscaler.log.pos tag cluster-autoscaler \u0026lt;parse\u0026gt; @type kubernetes \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;audit-log.conf:|-# Example: # {\u0026#34;kind\u0026#34;:\u0026#34;Event\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;audit.k8s.io/v1\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;Metadata\u0026#34;,\u0026#34;auditID\u0026#34;:\u0026#34;xxxx\u0026#34;,\u0026#34;stage\u0026#34;:\u0026#34;ResponseComplete\u0026#34;,\u0026#34;requestURI\u0026#34;:\u0026#34;/apis/...?timeout=10s\u0026#34;,\u0026#34;verb\u0026#34;:\u0026#34;update\u0026#34;,\u0026#34;user\u0026#34;:{\u0026#34;username\u0026#34;:\u0026#34;system:kube-scheduler\u0026#34;,\u0026#34;groups\u0026#34;:[\u0026#34;system:authenticated\u0026#34;]},\u0026#34;sourceIPs\u0026#34;:[\u0026#34;xxx.xxx.xxx.xxx\u0026#34;],\u0026#34;userAgent\u0026#34;:\u0026#34;kube-scheduler/v1.19.3 (linux/amd64) kubernetes/1e11e4a/leader-election\u0026#34;,\u0026#34;objectRef\u0026#34;:{\u0026#34;resource\u0026#34;:\u0026#34;leases\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kube-system\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;kube-scheduler\u0026#34;,\u0026#34;uid\u0026#34;:\u0026#34;xxxx\u0026#34;,\u0026#34;apiGroup\u0026#34;:\u0026#34;coordination.k8s.io\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;resourceVersion\u0026#34;:\u0026#34;52124\u0026#34;},\u0026#34;responseStatus\u0026#34;:{\u0026#34;metadata\u0026#34;:{},\u0026#34;code\u0026#34;:200},\u0026#34;requestReceivedTimestamp\u0026#34;:\u0026#34;2020-10-29T16:26:44.967339Z\u0026#34;,\u0026#34;stageTimestamp\u0026#34;:\u0026#34;2020-10-29T16:26:44.968796Z\u0026#34;,\u0026#34;annotations\u0026#34;:{\u0026#34;authorization.k8s.io/decision\u0026#34;:\u0026#34;allow\u0026#34;,\u0026#34;authorization.k8s.io/reason\u0026#34;:\u0026#34;RBAC: allowed by ClusterRoleBinding \\\u0026#34;system:kube-scheduler\\\u0026#34; of ClusterRole \\\u0026#34;system:kube-scheduler\\\u0026#34; to User \\\u0026#34;system:kube-scheduler\\\u0026#34;\u0026#34;}} # {\u0026#34;kind\u0026#34;:\u0026#34;Event\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;audit.k8s.io/v1\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;Request\u0026#34;,\u0026#34;auditID\u0026#34;:\u0026#34;xxxx\u0026#34;,\u0026#34;stage\u0026#34;:\u0026#34;ResponseComplete\u0026#34;,\u0026#34;requestURI\u0026#34;:\u0026#34;/api/....?resourceVersion=0\\u0026timeout=10s\u0026#34;,\u0026#34;verb\u0026#34;:\u0026#34;get\u0026#34;,\u0026#34;user\u0026#34;:{\u0026#34;username\u0026#34;:\u0026#34;system:node:kube-slave-1\u0026#34;,\u0026#34;groups\u0026#34;:[\u0026#34;system:nodes\u0026#34;,\u0026#34;system:authenticated\u0026#34;]},\u0026#34;sourceIPs\u0026#34;:[\u0026#34;xxx.xxx.xxx.xxx\u0026#34;],\u0026#34;userAgent\u0026#34;:\u0026#34;kubelet/v1.19.3 (linux/amd64) kubernetes/1e11e4a\u0026#34;,\u0026#34;objectRef\u0026#34;:{\u0026#34;resource\u0026#34;:\u0026#34;nodes\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;kube-slave-1\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;},\u0026#34;responseStatus\u0026#34;:{\u0026#34;metadata\u0026#34;:{},\u0026#34;code\u0026#34;:200},\u0026#34;requestReceivedTimestamp\u0026#34;:\u0026#34;2020-10-29T16:26:45.099703Z\u0026#34;,\u0026#34;stageTimestamp\u0026#34;:\u0026#34;2020-10-29T16:26:45.100167Z\u0026#34;,\u0026#34;annotations\u0026#34;:{\u0026#34;authorization.k8s.io/decision\u0026#34;:\u0026#34;allow\u0026#34;,\u0026#34;authorization.k8s.io/reason\u0026#34;:\u0026#34;\u0026#34;}} \u0026lt;source\u0026gt; @type tail @id in_tail_kube_apiserver_audit multiline_flush_interval 5s path /var/log/kubernetes/kube-apiserver-audit.log pos_file /var/log/kube-apiserver-audit.log.pos tag audit \u0026lt;parse\u0026gt; @type json time_key requestReceivedTimestamp time_type string time_format %Y-%m-%dT%T.%L%Z \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;kind:ConfigMapmetadata:name:fluentd-config-conf-additionalnamespace:kube-system    1 2  kubectl apply -f ./kubernetes/kibana/31-Fluentd.yaml kubectl apply -f ./kubernetes/kibana/32-FluentdConfigMap.yaml   You should now be able to filter your logs by tag, and look at the audit logs.\nSo, in this setup, your audit logs are at 2 places: directly bare on your server, and in kibana. This redundancy is important IMO because whatever happens to your cluster (even a full flush of all your pods), you should be able to know who did what.\n2.3. Make things persistent Now that you have set up everything, you might have seen that everytime the ElasticSearch pod is restarted, the database is emptied. This is normal so far, because we don\u0026rsquo;t actually write any data on a persistent storage. For now ! But let\u0026rsquo;s solve that.\n3. Kube dashboard: Web UI to administrate the cluster   References   https://kubernetes.io/fr/docs/tasks/access-application-cluster/web-ui-dashboard/     1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318  # Copyright 2017 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;);# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.apiVersion:v1kind:Namespacemetadata:name:kubernetes-dashboard---apiVersion:v1kind:ServiceAccountmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboard---apiVersion:v1kind:Secretmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-certsnamespace:kubernetes-dashboardtype:Opaque---apiVersion:v1kind:Secretmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-csrfnamespace:kubernetes-dashboardtype:Opaquedata:csrf:\u0026#34;\u0026#34;---apiVersion:v1kind:Secretmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-key-holdernamespace:kubernetes-dashboardtype:Opaque---kind:ConfigMapapiVersion:v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboard-settingsnamespace:kubernetes-dashboard---kind:RoleapiVersion:rbac.authorization.k8s.io/v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardrules:# Allow Dashboard to get, update and delete Dashboard exclusive secrets.- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;secrets\u0026#34;]resourceNames:[\u0026#34;kubernetes-dashboard-key-holder\u0026#34;,\u0026#34;kubernetes-dashboard-certs\u0026#34;,\u0026#34;kubernetes-dashboard-csrf\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;delete\u0026#34;]# Allow Dashboard to get and update \u0026#39;kubernetes-dashboard-settings\u0026#39; config map.- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;configmaps\u0026#34;]resourceNames:[\u0026#34;kubernetes-dashboard-settings\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;update\u0026#34;]# Allow Dashboard to get metrics.- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;services\u0026#34;]resourceNames:[\u0026#34;heapster\u0026#34;,\u0026#34;dashboard-metrics-scraper\u0026#34;]verbs:[\u0026#34;proxy\u0026#34;]- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;services/proxy\u0026#34;]resourceNames:[\u0026#34;heapster\u0026#34;,\u0026#34;http:heapster:\u0026#34;,\u0026#34;https:heapster:\u0026#34;,\u0026#34;dashboard-metrics-scraper\u0026#34;,\u0026#34;http:dashboard-metrics-scraper\u0026#34;]verbs:[\u0026#34;get\u0026#34;]---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardrules:# Allow Metrics Scraper to get metrics from the Metrics server- apiGroups:[\u0026#34;metrics.k8s.io\u0026#34;]resources:[\u0026#34;pods\u0026#34;,\u0026#34;nodes\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;]---apiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:labels:k8s-app:kubernetes-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardroleRef:apiGroup:rbac.authorization.k8s.iokind:Rolename:kubernetes-dashboardsubjects:- kind:ServiceAccountname:kubernetes-dashboardnamespace:kubernetes-dashboard---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:kubernetes-dashboardroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:viewsubjects:- kind:ServiceAccountname:kubernetes-dashboardnamespace:kubernetes-dashboard---kind:ServiceapiVersion:v1metadata:labels:app:kube-dashboardcomponent:kube-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardspec:ports:- port:80targetPort:9090selector:app:kube-dashboardcomponent:kube-dashboard---kind:DeploymentapiVersion:apps/v1metadata:labels:app:kube-dashboardcomponent:kube-dashboardname:kubernetes-dashboardnamespace:kubernetes-dashboardspec:replicas:1revisionHistoryLimit:10selector:matchLabels:app:kube-dashboardcomponent:kube-dashboardtemplate:metadata:labels:app:kube-dashboardcomponent:kube-dashboardspec:containers:- name:kubernetes-dashboardimage:kubernetesui/dashboard:v2.0.1imagePullPolicy:Alwaysports:- containerPort:9090# containerPort: 8443protocol:TCPargs:- --insecure-port=9090# ADDED- --enable-insecure-login# ADDED# - --auto-generate-certificates- --namespace=kubernetes-dashboard- --authentication-mode=token# Uncomment the following line to manually specify Kubernetes API server Host# If not specified, Dashboard will attempt to auto discover the API server and connect# to it. Uncomment only if the default does not work.# - --apiserver-host=http://my-address:portvolumeMounts:- name:kubernetes-dashboard-certsmountPath:/certs# Create on-disk volume to store exec logs- mountPath:/tmpname:tmp-volumelivenessProbe:httpGet:# scheme: HTTPSpath:/# port: 8443port:9090# ADDEDinitialDelaySeconds:30timeoutSeconds:30securityContext:# allowPrivilegeEscalation: false allowPrivilegeEscalation:truereadOnlyRootFilesystem:truerunAsUser:1001runAsGroup:2001volumes:- name:kubernetes-dashboard-certssecret:secretName:kubernetes-dashboard-certs- name:tmp-volumeemptyDir:{}serviceAccountName:kubernetes-dashboardnodeSelector:\u0026#34;beta.kubernetes.io/os\u0026#34;: linux# Comment the following tolerations if Dashboard must not be deployed on mastertolerations:- key:node-role.kubernetes.io/mastereffect:NoSchedule---kind:ServiceapiVersion:v1metadata:labels:k8s-app:dashboard-metrics-scrapername:dashboard-metrics-scrapernamespace:kubernetes-dashboardspec:ports:- port:8000targetPort:8000selector:k8s-app:dashboard-metrics-scraper---kind:DeploymentapiVersion:apps/v1metadata:labels:k8s-app:dashboard-metrics-scrapername:dashboard-metrics-scrapernamespace:kubernetes-dashboardspec:replicas:1revisionHistoryLimit:10selector:matchLabels:k8s-app:dashboard-metrics-scrapertemplate:metadata:labels:k8s-app:dashboard-metrics-scraperannotations:seccomp.security.alpha.kubernetes.io/pod:\u0026#39;runtime/default\u0026#39;spec:containers:- name:dashboard-metrics-scraperimage:kubernetesui/metrics-scraper:v1.0.1ports:- containerPort:8000protocol:TCPlivenessProbe:httpGet:scheme:HTTPpath:/port:8000initialDelaySeconds:30timeoutSeconds:30volumeMounts:- mountPath:/tmpname:tmp-volumesecurityContext:allowPrivilegeEscalation:falsereadOnlyRootFilesystem:truerunAsUser:1001runAsGroup:2001serviceAccountName:kubernetes-dashboardnodeSelector:\u0026#34;beta.kubernetes.io/os\u0026#34;: linux# Comment the following tolerations if Dashboard must not be deployed on mastertolerations:- key:node-role.kubernetes.io/mastereffect:NoSchedulevolumes:- name:tmp-volumeemptyDir:{}    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingressroute-dashboardnamespace:kubernetes-dashboardspec:entryPoints:- websecureroutes:- match:Host(`kube-dashboard.bar.com`)kind:Ruleservices:- name:kubernetes-dashboardnamespace:kubernetes-dashboardkind:Serviceport:80tls:certResolver:myresolver    1 2  kubectl apply -f ./kubernetes/kube-dashboard/01-Dashboard.yaml kubectl apply -f ./kubernetes/kube-dashboard/02-IngressRoutes.yaml   Then, for debugging purpose, we\u0026rsquo;ll set up a test service account that can only view and list items in the dashboard. This service account will be named watchdog.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70  apiVersion:v1kind:ServiceAccountmetadata:labels:k8s-app:kubernetes-dashboardname:watchdognamespace:kubernetes-dashboard---kind:RoleapiVersion:rbac.authorization.k8s.io/v1metadata:labels:k8s-app:kubernetes-dashboardname:watchdognamespace:kubernetes-dashboardrules:- apiGroups:[\u0026#34;*\u0026#34;]resources:[\u0026#34;*\u0026#34;]resourceNames:[\u0026#34;*\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;]---kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1metadata:labels:k8s-app:kubernetes-dashboardname:watchdogrules:# Allow Metrics Scraper to get metrics from the Metrics server- apiGroups:[\u0026#34;*\u0026#34;]resources:[\u0026#34;*\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;]---apiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:labels:k8s-app:kubernetes-dashboardname:watchdognamespace:kubernetes-dashboardroleRef:apiGroup:rbac.authorization.k8s.iokind:Rolename:watchdogsubjects:- kind:ServiceAccountname:watchdognamespace:kubernetes-dashboard---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:watchdogroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:watchdogsubjects:- kind:ServiceAccountname:watchdognamespace:kubernetes-dashboard    1 2 3 4 5 6  # Create the role, cluster role and service account using them kubectl apply -f ./kubernetes/kube-dashboard/03-ServiceAccount.yaml # Get the secret\u0026#39;s name secret_name=\u0026#34;$(kubectl get serviceaccount watchdog -n kubernetes-dashboard -o json | jq \u0026#39;.secrets[0].name\u0026#39; -r)\u0026#34; # Get the secret\u0026#39;s token echo $(kubectl get secret $secret_name -n kubernetes-dashboard -o json | jq \u0026#39;.data.token\u0026#39; -r | base64 --decode)   Now, navigate to https://kube-dashboard.{{cluster.baseHostName}} and log in using the token you got above.\nYou should be able to see all resources in your cluster.\nYes, this is super unsafe. That\u0026rsquo;s why we are going to add authentication right now, and why I told you not to make this publicly exposed for now.\n Hey, we’ve done important things here ! Maybe it’s time to commit…\n1 2  git add . git commit -m \u0026#34;Monitoring: See what is going on\\n\\nFollowing guide @ https://gerkindev.github.io/devblog/walkthroughs/kubernetes/06-monitoring/\u0026#34;    ","description":"","id":6,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Monitoring"],"title":"Monitoring: See what is going on","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/06-monitoring/"},{"content":"Now that we have our authentication service up and running, we can protect our dashboards installed in the step  06 - Monitoring: See what is going on  using our Keycloak OpenID Connect provider. Here is a diagram on how authorization will be managed:\nTraefik dashboard Kibana Kube dashboard   References   https://itnext.io/protect-kubernetes-dashboard-with-openid-connect-104b9e75e39c    Again, we are going to set up a new instance of  louketo-proxy .\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74  apiVersion:apps/v1kind:Deploymentmetadata:name:gatekeepernamespace:kubernetes-dashboardlabels:app:kubernetes-dashboardcomponent:gatekeeperspec:replicas:1selector:matchLabels:app:kubernetes-dashboardcomponent:gatekeepertemplate:metadata:labels:app:kubernetes-dashboardcomponent:gatekeeperspec:containers:- name:keycloak-gatekeeperimage:\u0026#34;quay.io/keycloak/keycloak-gatekeeper:10.0.0\u0026#34;imagePullPolicy:IfNotPresentargs:- --listen=0.0.0.0:3000- --discovery-url=https://kube-keycloak.{{cluster.baseHostName}}/auth/realms/{{apiServer.realm}}- --client-id={{apiServer.clientId}}- --client-secret={{apiServer.clientSecret}}- --upstream-url=http://kubernetes-dashboard.kubernetes-dashboard.svc.cluster.local:80- --redirection-url=https://kube-dashboard.bar.com/- --skip-openid-provider-tls-verify=true- --enable-default-deny=true- --enable-logging=true- --enable-refresh-tokens=true- --enable-session-cookies=true- --encryption-key={{random32charsString}}- --secure-cookie=true- --resources=uri=/*ports:- name:httpcontainerPort:3000protocol:TCPlivenessProbe:httpGet:path:/oauth/healthport:3000initialDelaySeconds:3timeoutSeconds:2readinessProbe:httpGet:path:/oauth/healthport:3000initialDelaySeconds:3timeoutSeconds:2---apiVersion:v1kind:Servicemetadata:name:gatekeepernamespace:kubernetes-dashboardlabels:app:kubernetes-dashboardcomponent:gatekeeperspec:ports:- port:80targetPort:httpprotocol:TCPname:httpselector:app:kubernetes-dashboardcomponent:gatekeeper    Finally, modify your ingress route\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  apiVersion:traefik.containo.us/v1alpha1kind:IngressRoutemetadata:name:ingressroute-dashboardnamespace:kubernetes-dashboardspec:entryPoints:- websecureroutes:- match:Host(`kube-dashboard.{{cluster.baseHostName}}`)kind:Ruleservices:- name:gatekeepernamespace:kubernetes-dashboardkind:Serviceport:80tls:certResolver:myresolver     Hey, we’ve done important things here ! Maybe it’s time to commit…\n1 2  git add . git commit -m \u0026#34;Protect monitoring with authentication\\n\\nFollowing guide @ https://gerkindev.github.io/devblog/walkthroughs/kubernetes/09-safe-monitoring/\u0026#34;    ","description":"","id":7,"section":"walkthroughs","tags":["Kubernetes","Sysadmin","DevOps","Web service","Security","Authentication"],"title":"Protect monitoring with authentication","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/09-safe-monitoring/"},{"content":"Your setup is running, everything runs smoothly, and suddenly, ‼️ nothing is responding: your cluster is overloaded.\nWell, I hope you\u0026rsquo;ll expand your cluster capacity before it happens. It\u0026rsquo;s always really bad and stressful to do maintenance because of downtime.\nHopefully, here comes the real huge advantage of kubernetes: it is meant to scale, up, and down. So, assuming you have followed the full guide so far, let\u0026rsquo;s review together how to add some juice to our cluster ⚡.\nJoin the cluster\u0026rsquo;s VPN In the step  00 Setup the cluster\u0026rsquo;s VPN , we have set up a VPN so that each of our nodes can communicate safely with each others, on their own virtual network across the internet. This comes with the great power of being able to have servers spread all around the globe.\nFrom the OpenVPN server node So, log in to the OpenVPN master server, and run the following to generate a configuration for your brand new machine:\n1 2 3 4 5 6  # Generate a client docker run -v {{vpn.volumeName}}:/etc/openvpn --rm -it kylemanna/openvpn:2.3 easyrsa build-client-full {{newNode.name}} nopass # Set its static IP echo \u0026#34;ifconfig-push {{newNode.vpnIp}} {{vpn.serverIp}}\u0026#34; | docker run -v {{vpn.volumeName}}:/etc/openvpn -i --rm kylemanna/openvpn:2.3 tee /etc/openvpn/ccd/{{newNode.name}} # Get its config to your host docker run -v {{vpn.volumeName}}:/etc/openvpn --rm kylemanna/openvpn:2.3 ovpn_getclient {{newNode.name}} \u0026gt; {{newNode.name}}.ovpn   Then, move {{newNode.name}}.ovpn to your new node by a safe mean.\nFrom the new node Install OpenVPN:\n1 2  dnf install epel-release dnf install openvpn   Add the OpenVPN server to your /etc/hosts file (if not a real DNS name).\n1  echo \u0026#39;{{vpn.publicServerIp}}\tvpn.{{cluster.baseHostName}}\u0026#39; \u0026gt;\u0026gt; /etc/hosts    Install the OpenVPN configuration you just copied\n1 2 3 4  # Install the OpenVPN configuration install -o root -m 400 {{newNode.name}}.ovpn /etc/openvpn/client/{{newNode.name}}.conf # Enable the OpenVPN client systemctl enable --now openvpn-client@{{newNode.name}}   Finally, check if everything works as expected and you can reach both internet and your neighbor nodes\n1 2 3 4  # Check internet connection ping -c 4 8.8.8.8 # Check in-VPN connection ping -c 4 192.168.255.1   Join the cluster Since I assume you\u0026rsquo;ve initialized your cluster a while ago, and your previous cluster\u0026rsquo;s join token is expired, we are going to create a new one and use it.\nIf you\u0026rsquo;ve just created your cluster, you can check out  02 - Kickstart the cluster \nFrom any account or node with cluster admin capabilities Create a new cluster token:\n1  kubeadm token create --print-join-command     Sample output  kubeadm join 192.168.255.10:6443 --token gmedpt.veqzvuhcazac26gf --discovery-token-ca-cert-hash sha256:cb316693e48403ff18f840d47930f6737744d2ead362838695df3a1e1400cec1    Copy the kubeadm join ... command outputted by the command above.\nFrom the new node Run the command copied above:\n1  kubeadm join ...:6443 --token ... --discovery-token-ca-cert-hash sha256:....   If everything worked correctly, you should have an output like below:\n  Sample output  [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; [kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot; [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... ^[[B This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster.    ","description":"","id":8,"section":"walkthroughs","tags":["Kubernetes","Sysadmin"],"title":"Scaling up","uri":"https://gerkindev.github.io/devblog/walkthroughs/kubernetes/10-scaling-up/"},{"content":"Kubernetes is\u0026hellip;. Quite a thing, to say the least ! 😅 Even if their conceptors did a great job at making the kubectl cli as usable as possible, it can sometimes be a pain to be productive with it, read outputs, or do repetitive tasks. That\u0026rsquo;s why I wrote this small Quality of life improvements post: to regroup some install steps you might have missed, give you some useful 3rd party tools or maybe even give you tips a step ahead.\nkubectl auto-complete   References   📚 kubectl installation manual    Autocomplete is nice, and a real time saver. It avoids typos, and it\u0026rsquo;s quite satisfying to type a complete command in 4 keystrokes and a couple of tabs correctly placed. (even if I\u0026rsquo;m always unsure when relying on my browser\u0026rsquo;s autocomplete for https://analytics.google.com 😑).\nBut for this one, I can only say one thing, and you have no excuses:\n  \nSo, short stories short, and depending on your shell, type in:\nzsh bash  1 2 3 4  echo \u0026#39;autoload -Uz compinit compinit source \u0026lt;(kubectl completion zsh)\u0026#39; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc      All the (bad) flavours come from the natural world.\n 1 2 3 4 5 6 7 8 9 10 11  # Install the bash completion main script (assuming you\u0026#39;re on a RHEL/CentOS/Fedora) dnf install bash-completion # Reload env source ~/.bashrc # Check if bash_completion is properly imported, or add it to your bashrc if ! type _init_completion; then echo \u0026#39;source /usr/share/bash-completion/bash_completion\u0026#39; \u0026gt;\u0026gt; ~/.bashrc fi # Source the completion script echo \u0026#39;source \u0026lt;(kubectl completion bash)\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc       'use strict'; var containerId = JSON.parse(\"\\\"e5c5d0542d8caf10\\\"\"); var containerElem = document.getElementById(containerId); var tabLinks = null; var tabContents = null; var ids = []; if (containerElem) { tabLinks = containerElem.querySelectorAll('.tab__link'); tabContents = containerElem.querySelectorAll('.tab__content'); } for (var i = 0; i 0) { tabContents[0].style.display = 'block'; }  kubecolor: prettier kubectl commands outputs with colors   References   Add ANSI colors to kubectl describe and other outputs kubecolor    1 2 3  go get -u github.com/dty1er/kubecolor/cmd/kubecolor # Make sure kubecolor is found which kubecolor   If the command above did not worked, then you may have a problem with your $GOPATH or $GOHOME environment variables. If none are set, then the package was installed in ~/go/bin. Either fix your vars or add ~/go/bin to your $PATH.\nFinally, you could either use kubecolor instead of kubectl, or alias kubectl as kubecolor with the following code sample:\n1 2 3 4 5 6 7 8 9  # Backup original `kubectl` command path. Supports subsequent imports of the file. echo \u0026#39;export KUBECTL_ORIG_PATH=\u0026#34;${KUBECTL_ORIG_PATH:-\u0026#34;$(which kubectl)\u0026#34;}\u0026#34;\u0026#39; \u0026gt;\u0026gt; {{profileFile}} # Alias the real `kubectl` as `kubectll` echo \u0026#39;alias kubectll=\u0026#34;${KUBECTL_ORIG_PATH}\u0026#34;\u0026#39; \u0026gt;\u0026gt; {{profileFile}} # Alias kubectl to use colors by default echo \u0026#39;alias kubectl=\u0026#34;kubecolor\u0026#34;\u0026#39; \u0026gt;\u0026gt; {{profileFile}} # Enable the autocompletion for the alias too (see auto-complete install above) echo \u0026#34;complete -o default -F __start_kubectl kubecolor\u0026#34; \u0026gt;\u0026gt; {{profileFile}} source {{profileFile}}   I noticed some little things does not work well with kubecolor. That\u0026rsquo;s why the script above let you use the original kubectl command through kubectll. For instance, I noticed that some commands prompting user input (so using stdin), such as kubectl login we\u0026rsquo;ll see after don\u0026rsquo;t work.\nSo, if you try a command that seems to not work as expected, or stay stuck, fall back to kubectll.\n helm: a kubernetes stack template repository We are also going to use some  Helm charts . To install helm, run the following command:\n1 2  # See https://helm.sh/docs/intro/install/ curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash   krew: a kubectl plugins manager   krew  is a nice small plugin manager for your kubectl command. At the time of writing, it has  129 plugins available , including some pretty convinient to restart pods, login using OpenId, check the state of your cluster, and more.\nTo install krew, run the following: (taken from  the docs )\n Think about replacing {{profileFile}} with your actual zsh or bash profile\n 1 2 3 4 5 6 7 8 9 10 11 12 13  # Install krew ( set -x; cd \u0026#34;$(mktemp -d)\u0026#34; \u0026amp;\u0026amp; curl -fsSLO \u0026#34;https://github.com/kubernetes-sigs/krew/releases/latest/download/krew.tar.gz\u0026#34; \u0026amp;\u0026amp; tar zxvf krew.tar.gz \u0026amp;\u0026amp; KREW=./krew-\u0026#34;$(uname | tr \u0026#39;[:upper:]\u0026#39; \u0026#39;[:lower:]\u0026#39;)_$(uname -m | sed -e \u0026#39;s/x86_64/amd64/\u0026#39; -e \u0026#39;s/arm.*$/arm/\u0026#39;)\u0026#34; \u0026amp;\u0026amp; \u0026#34;$KREW\u0026#34; install krew ) # Add it to your $PATH and reload config echo \u0026#39;export PATH=\u0026#34;${KREW_ROOT:-$HOME/.krew}/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; {{profileFile}} source {{profileFile}} # Check krew works kubectl krew   One ring to rule them all For this one, I plead guilty of not using it enough, but it contains a lot of useful knowledge and possible solutions of most of your problems.\nYou guessed it, I\u0026rsquo;m talking about documentation. (because it would be an insult to tell you that StackOverflow is a thing.)\nRead it carefully. Take time to understand it and its underlying concepts. Don\u0026rsquo;t use tools you don\u0026rsquo;t know how they work. Because when things breaks, your knowledge of what and how it broke will help you to solve the problem quickly and without damages. So, read the documentation of your containers, your helm charts, your kubernetes network layer, and, of course, kubernetes and docker themselves.\n","description":"","id":9,"section":"blog","tags":["kubernetes"],"title":"Quality Of Life improvements to kubernetes","uri":"https://gerkindev.github.io/devblog/blog/kubernetes-qol/"},{"content":"Hello world ! ","description":"","id":10,"section":"","tags":null,"title":"About","uri":"https://gerkindev.github.io/devblog/about/"}]